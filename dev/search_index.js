var documenterSearchIndex = {"docs":
[{"location":"maths/hessians/#Hessians-and-symbolic-precomputation","page":"Hessians and symbolic precomputation","title":"Hessians and symbolic precomputation","text":"∇²Σ(::RAMSymbolic) -> pre-allocated array for vec(Σ)θᵀ\n∇²Σ_function(::RAMSymbolic) -> function to overwrite ∇²Σ in place","category":"section"},{"location":"developer/implied/#Custom-implied-types","page":"Custom implied types","title":"Custom implied types","text":"We recommend to first read the part Custom loss functions, as the overall implementation is the same and we will describe it here more briefly.\n\nImplied types are of subtype SemImplied. To implement your own implied type, you should define a struct\n\nstruct MyImplied <: SemImplied\n    ...\nend\n\nand a method to update!:\n\nimport StructuralEquationModels: objective!\n\nfunction update!(targets::EvaluationTargets, implied::MyImplied, model::AbstractSemSingle, params)\n\n    if is_objective_required(targets)\n        ...\n    end\n\n    if is_gradient_required(targets)\n        ...\n    end\n    if is_hessian_required(targets)\n        ...\n    end\n\nend\n\nAs you can see, update gets passed as a first argument targets, which is telling us whether the objective value, gradient, and/or hessian are needed. We can then use the functions is_..._required and conditional on what the optimizer needs, we can compute and store things we want to make available to the loss functions. For example, as we have seen in Second example - maximum likelihood, the RAM implied type computes the model-implied covariance matrix and makes it available via implied.Σ.\n\nJust as described in Custom loss functions, you may define a constructor. Typically, this will depend on the specification = ... argument that can be a ParameterTable or a RAMMatrices object.\n\nWe implement an ImpliedEmpty type in our package that does nothing but serving as an implied field in case you are using a loss function that does not need any implied type at all. You may use it as a template for defining your own implied type, as it also shows how to handle the specification objects:\n\n############################################################################################\n### Types\n############################################################################################\n\"\"\"\nEmpty placeholder for models that don't need an implied part.\n(For example, models that only regularize parameters.)\n\n# Constructor\n\n    ImpliedEmpty(;specification, kwargs...)\n\n# Arguments\n- `specification`: either a `RAMMatrices` or `ParameterTable` object\n\n# Examples\nA multigroup model with ridge regularization could be specified as a `SemEnsemble` with one\nmodel per group and an additional model with `ImpliedEmpty` and `SemRidge` for the regularization part.\n\n# Extended help\n\n## Interfaces\n- `param_labels(::ImpliedEmpty) `-> Vector of parameter labels\n- `nparams(::ImpliedEmpty)` -> Number of parameters\n\"\"\"\nstruct ImpliedEmpty{A, B, C} <: SemImplied\n    hessianeval::A\n    meanstruct::B\n    ram_matrices::C\nend\n\n############################################################################################\n### Constructors\n############################################################################################\n\nfunction ImpliedEmpty(;\n    specification,\n    meanstruct = NoMeanStruct(),\n    hessianeval = ExactHessian(),\n    kwargs...,\n)\n    return ImpliedEmpty(hessianeval, meanstruct, convert(RAMMatrices, specification))\nend\n\n############################################################################################\n### methods\n############################################################################################\n\nupdate!(targets::EvaluationTargets, implied::ImpliedEmpty, par, model) = nothing\n\n############################################################################################\n### Recommended methods\n############################################################################################\n\nupdate_observed(implied::ImpliedEmpty, observed::SemObserved; kwargs...) = implied\n\nAs you see, similar to Custom loss functions we implement a method for update_observed.","category":"section"},{"location":"performance/sorting/#Model-sorting","page":"Model sorting","title":"Model sorting","text":"In RAM notation, the model implied covariance matrix is computed as\n\nSigma = F(I-A)^-1S(I-A)^-TF^T\n\nIf the model is acyclic, the observed and latent variables can be reordered such that (I-A) is lower triangular. This has the computational benefit that the inversion of lower triangular matrices can be carried out by specialized algorithms.\n\nTo automatically reorder your variables in a way that makes this optimization possible, we provide a sort! method that can be applied to ParameterTable objects to sort the observed and latent variables from the most exogenous ones to the most endogenous.\n\nWe use it as\n\nsort_vars!(parameter_table)\n\nmodel = Sem(\n    specification = parameter_table,\n    ...\n)\n\nModels specified from sorted parameter tables will make use of the described optimizations.","category":"section"},{"location":"internals/internals/#Internals-and-Design","page":"Internals and design","title":"Internals and Design","text":"On the following pages, we document some technical information about the package. Those informations are no prerequisite for extending the package (as decribed in the developer documentation)!, but they may be useful.","category":"section"},{"location":"tutorials/meanstructure/#Models-with-mean-structures","page":"Mean Structures","title":"Models with mean structures","text":"To make use of mean structures in your model, you have to\n\nSpecify your model with a mean structure. The sections Graph interface and RAMMatrices interface both explain how this works.\nBuild your model with a meanstructure. We explain how that works in the following.\n\nLets say you specified A first model as a graph with a meanstructure:\n\nusing StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    dem60 ← ind60\n    dem65 ← dem60\n    dem65 ← ind60\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\n    # means\n    Symbol(1) → _(observed_vars)\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = latent_vars, \n    observed_vars = observed_vars)\n\nthat is, all observed variable means are estimated freely.\n\nTo build the model with a meanstructure, we proceed as usual, but pass the argument meanstructure = true. For our example,\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    meanstructure = true\n)\n\nfit(model)\n\nIf we build the model by parts, we have to pass the meanstructure = true argument to every part that requires it (when in doubt, simply consult the documentation for the respective part).\n\nFor our example,\n\nobserved = SemObservedData(specification = partable, data = data, meanstructure = true)\n\nimplied_ram = RAM(specification = partable, meanstructure = true)\n\nml = SemML(observed = observed, meanstructure = true)\n\nmodel = Sem(observed, implied_ram, SemLoss(ml))\n\nfit(model)","category":"section"},{"location":"developer/extending/#Extending-the-package","page":"Extending the package","title":"Extending the package","text":"As discussed in the section on Model Construction, every Structural Equation Model (Sem) consists of three (four with the optimizer) parts:\n\n(Image: SEM concept typed)\n\nOn the following pages, we will explain how you can define your own custom parts and \"plug them in\". There are certain things you have to do to define custom parts and some things you can do to have a more pleasent experience. In general, these requirements fall into the categories\n\nminimal (to use your custom part and fit a Sem with it)\nuse the outer constructor to build a model in a more convenient way\nuse additional functionality like standard errors, fit measures, etc.","category":"section"},{"location":"tutorials/first_model/#A-first-model","page":"A first model","title":"A first model","text":"In this tutorial, we will fit an example SEM with our package.  The example we are using is from the lavaan tutorial, so it may be familiar. It looks like this:\n\n(Image: Visualization of the Political Democracy model)\n\nWe assume the StructuralEquationModels package is already installed. To use it in the current session, we run\n\nusing StructuralEquationModels\n\nWe then first define the graph of our model in a syntax which is similar to the R-package lavaan:\n\nobs_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlat_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(obs_vars) ↔ _(obs_vars)\n    _(lat_vars) ↔ _(lat_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\nnote: Time to first model\nWhen executing the code from this tutorial the first time in a fresh julia session, you may wonder that it takes quite some time. This is not because the implementation is slow, but because the functions are compiled the first time you use them. Try rerunning the example a second time - you will see that all function executions after the first one are quite fast.\n\nWe then use this graph to define a ParameterTable object\n\npartable = ParameterTable(\n    graph,\n    latent_vars = lat_vars, \n    observed_vars = obs_vars)\n\nload the example data\n\ndata = example_data(\"political_democracy\")\n\nand specify our model as\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\nWe can now fit the model via\n\nmodel_fit = fit(model)\n\nand compute fit measures as\n\nfit_measures(model_fit)\n\nWe can also get a bit more information about the fitted model via the details() function:\n\ndetails(model_fit)\n\nTo investigate the parameter estimates, we can update our partable object to contain the new estimates:\n\nupdate_estimate!(partable, model_fit)\n\nand investigate the solution with\n\ndetails(partable)\n\nCongratulations, you fitted and inspected your very first model!  We recommend continuing with Our Concept of a Structural Equation Model.","category":"section"},{"location":"tutorials/constraints/constraints/#Constrained-optimization","page":"Constraints","title":"Constrained optimization","text":"","category":"section"},{"location":"tutorials/constraints/constraints/#Using-the-NLopt-backend","page":"Constraints","title":"Using the NLopt backend","text":"","category":"section"},{"location":"tutorials/constraints/constraints/#Define-an-example-model","page":"Constraints","title":"Define an example model","text":"Let's revisit our model from A first model:\n\nusing StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + label(:λ₂)*y2 + label(:λ₃)*y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → label(:λₗ)*dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ label(:y3y7)*y7\n    y8 ↔ label(:y8y4)*y4 + y6\n\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = latent_vars, \n    observed_vars = observed_vars)\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\nmodel_fit = fit(model)\n\nupdate_estimate!(partable, model_fit)\n\ndetails(partable)","category":"section"},{"location":"tutorials/constraints/constraints/#Define-the-constraints","page":"Constraints","title":"Define the constraints","text":"Let's introduce some constraints:\n\nEquality constraint: The covariances y3 ↔ y7 and y8 ↔ y4 should sum up to 1.\nInequality constraint: The difference between the loadings dem60 → y2 and dem60 → y3 should be smaller than 0.1\nBound constraint: The directed effect from  ind60 → dem65 should be smaller than 0.5\n\n(Of course those constaints only serve an illustratory purpose.)\n\nWe first need to get the indices of the respective parameters that are invoved in the constraints.  We can look up their labels in the output above, and retrieve their indices as\n\nparind = param_indices(model)\nparind[:y3y7] # 29\n\nThe bound constraint is easy to specify: Just give a vector of upper or lower bounds that contains the bound for each parameter. In our example, only the parameter labeled :λₗ has an upper bound, and the number of total parameters is n_par(model) = 31, so we define\n\nupper_bounds = fill(Inf, 31)\nupper_bounds[parind[:λₗ]] = 0.5\n\nThe equailty and inequality constraints have to be reformulated to be of the form x = 0 or x ≤ 0:\n\ny3 ↔ y7 + y8 ↔ y4 - 1 = 0\ndem60 → y2 - dem60 → y3 - 0.1 ≤ 0\n\nNow they can be defined as functions of the parameter vector:\n\nparind[:y3y7] # 29\nparind[:y8y4] # 30\n# θ[29] + θ[30] - 1 = 0.0\nfunction eq_constraint(θ, gradient)\n    if length(gradient) > 0\n        gradient .= 0.0\n        gradient[29] = 1.0\n        gradient[30] = 1.0\n    end\n    return θ[29] + θ[30] - 1\nend\n\nparind[:λ₂] # 3\nparind[:λ₃] # 4\n# θ[3] - θ[4] - 0.1 ≤ 0\nfunction ineq_constraint(θ, gradient)\n    if length(gradient) > 0\n        gradient .= 0.0\n        gradient[3] = 1.0\n        gradient[4] = -1.0\n    end\n    θ[3] - θ[4] - 0.1\nend\n\nIf the algorithm needs gradients at an iteration, it will pass the vector gradient that is of the same size as the parameters. With if length(gradient) > 0 we check if the algorithm needs gradients, and if it does, we fill the gradient vector with the gradients  of the constraint w.r.t. the parameters.\n\nIn NLopt, vector-valued constraints are also possible, but we refer to the documentation for that.","category":"section"},{"location":"tutorials/constraints/constraints/#Fit-the-model","page":"Constraints","title":"Fit the model","text":"We now have everything together to specify and fit our model. First, we specify our optimizer backend as\n\nusing NLopt\n\nconstrained_optimizer = SemOptimizerNLopt(\n    algorithm = :AUGLAG,\n    options = Dict(:upper_bounds => upper_bounds, :xtol_abs => 1e-4),\n    local_algorithm = :LD_LBFGS,\n    equality_constraints = NLoptConstraint(;f = eq_constraint, tol = 1e-8),\n    inequality_constraints = NLoptConstraint(;f = ineq_constraint, tol = 1e-8),\n)\n\nAs you see, the equality constraints and inequality constraints are passed as keyword arguments, and the bounds are passed as options for the (outer) optimization algorithm. Additionally, for equality and inequality constraints, a feasibility tolerance can be specified that controls if a solution can be accepted, even if it violates the constraints by a small amount.  Especially for equality constraints, it is recommended to allow for a small positive tolerance. In this example, we set both tolerances to 1e-8.\n\nwarning: Convergence criteria\nWe have often observed that the default convergence criteria in NLopt lead to non-convergence flags. Indeed, this example does not convergence with default criteria. As you see above, we used a realively liberal absolute tolerance in the optimization parameters of 1e-4. This should not be a problem in most cases, as the sampling variance in (almost all) structural equation models  should lead to uncertainty in the parameter estimates that are orders of magnitude larger. We nontheless recommend choosing a convergence criterion with care (i.e. w.r.t. the scale of your parameters), inspecting the solutions for plausibility, and comparing them to unconstrained solutions.\n\nmodel_constrained = Sem(\n    specification = partable,\n    data = data\n)\n\nmodel_fit_constrained = fit(constrained_optimizer, model_constrained)\n\nAs you can see, the optimizer converged (:XTOL_REACHED) and investigating the solution yields\n\nupdate_partable!(\n    partable,\n    :estimate_constr,\n    model_fit_constrained, \n    solution(model_fit_constrained), \n    )\n\ndetails(partable)\n\nAs we can see, the constrained solution is very close to the original solution (compare the columns estimate and estimate_constr), with the difference that the constrained parameters fulfill their constraints.  As all parameters are estimated simultaneously, it is expexted that some unconstrained parameters are also affected (e.g., the constraint on dem60 → y2 leads to a higher estimate of the residual variance y2 ↔ y2).","category":"section"},{"location":"tutorials/constraints/constraints/#Using-the-Optim.jl-backend","page":"Constraints","title":"Using the Optim.jl backend","text":"Information about constrained optimization using Optim.jl can be found in the packages documentation.","category":"section"},{"location":"performance/starting_values/#Starting-values","page":"Starting values","title":"Starting values","text":"The fit function has a keyword argument that takes either a vector of starting values or a function that takes a model as input to compute starting values. Current options are start_fabin3 for fabin 3 starting values [Hägglund82] or start_simple for simple starting values. Additional keyword arguments to fit are passed to the starting value function. For example,\n\n    fit(\n        model; \n        start_val = start_simple,\n        start_covariances_latent = 0.5\n    )\n\nuses simple starting values with 0.5 as a starting value for covariances between latent variables.\n\n[Hägglund82]: Hägglund, G. (1982). Factor analysis by instrumental variables methods. Psychometrika, 47(2), 209-222.","category":"section"},{"location":"tutorials/specification/parameter_table/#ParameterTable-interface","page":"ParameterTable interface","title":"ParameterTable interface","text":"Altough you can directly specify a parameter table, this is kind of tedious, so at the moment, we dont have a tutorial for this. As lavaan also uses parameter tables to store model specifications, we are working on a way to convert lavaan parameter tables to StructuralEquationModels.jl parameter tables, but this is still WIP.","category":"section"},{"location":"tutorials/specification/parameter_table/#Convert-from-and-to-RAMMatrices","page":"ParameterTable interface","title":"Convert from and to RAMMatrices","text":"To convert a RAMMatrices object to a ParameterTable, simply use partable = ParameterTable(rammatrices).  To convert an object of type ParameterTable to RAMMatrices, you can use ram_matrices = RAMMatrices(partable).","category":"section"},{"location":"performance/symbolic/#Symbolic-precomputation","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"In RAM notation, the model implied covariance matrix is computed as\n\nSigma = F(I-A)^-1S(I-A)^-TF^T\n\nIf the model is acyclic, we can compute\n\n(I-A)^-1 = sum_k = 0^n A^k\n\nfor some n  infty. Typically, the S and A matrices are sparse. In our package, we offer symbolic precomputation of Sigma, nablaSigma and even nabla^2Sigma for acyclic models to optimally exploit this sparsity. To use this feature, simply use the RAMSymbolic implied type for your model.\n\nThis can decrase model fitting time, but will also increase model building time (as we have to carry out the symbolic computations and compile specialised functions). As a result, this is probably not beneficial to use if you only fit a single model, but can lead to great improvements if you fit the same modle to multiple datasets (e.g. to compute bootstrap standard errors).","category":"section"},{"location":"tutorials/specification/ram_matrices/#RAMMatrices-interface","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Models can also be specified by an object of type RAMMatrices.  The RAM (reticular action model) specification corresponds to three matrices; the A matrix containing all directed parameters, the S matrix containing all undirected parameters, and the F matrix filtering out latent variables from the model implied covariance.\n\nThe model implied covariance matrix for the observed variables of a SEM is then computed as\n\nSigma = F(I-A)^-1S(I-A)^-TF^T\n\nFor A first model, the corresponding specification looks like this:\n\n\nS =[:θ1   0    0     0     0      0     0     0     0     0     0     0     0     0\n    0     :θ2  0     0     0      0     0     0     0     0     0     0     0     0\n    0     0     :θ3  0     0      0     0     0     0     0     0     0     0     0\n    0     0     0     :θ4  0      0     0     :θ15  0     0     0     0     0     0\n    0     0     0     0     :θ5   0     :θ16  0     :θ17  0     0     0     0     0\n    0     0     0     0     0     :θ6  0      0     0     :θ18  0     0     0     0\n    0     0     0     0     :θ16  0     :θ7   0     0     0     :θ19  0     0     0\n    0     0     0     :θ15 0      0     0     :θ8   0     0     0     0     0     0\n    0     0     0     0     :θ17  0     0     0     :θ9   0     :θ20  0     0     0\n    0     0     0     0     0     :θ18 0      0     0     :θ10  0     0     0     0\n    0     0     0     0     0     0     :θ19  0     :θ20  0     :θ11  0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     :θ12  0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     :θ13  0\n    0     0     0     0     0     0     0     0     0     0     0     0     0     :θ14]\n\nF =[1.0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    0 1 0 0 0 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0 0 0 0 0\n    0 0 0 0 0 1 0 0 0 0 0 0 0 0\n    0 0 0 0 0 0 1 0 0 0 0 0 0 0\n    0 0 0 0 0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 0 0 0 0 1 0 0 0 0 0\n    0 0 0 0 0 0 0 0 0 1 0 0 0 0\n    0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n\nA =[0  0  0  0  0  0  0  0  0  0  0     1.0   0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ21  0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ22  0     0\n    0  0  0  0  0  0  0  0  0  0  0     0     1.0   0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ23  0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ24  0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ25  0\n    0  0  0  0  0  0  0  0  0  0  0     0     0     1\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ26\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ27\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ28\n    0  0  0  0  0  0  0  0  0  0  0     0     0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ29  0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ30  :θ31  0]\n\nθ = Symbol.(:θ, 1:31)\n\nspec = RAMMatrices(;\n    A = A, \n    S = S, \n    F = F, \n    param_labels = θ,\n    vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8, :ind60, :dem60, :dem65]\n)\n\nmodel = Sem(\n    specification = spec,\n    data = example_data(\"political_democracy\")\n)\n\nLet's look at this step by step:\n\nFirst, we specify the A, S and F-Matrices.  For a free parameter, we write a Symbol like :θ1 (or any other symbol we like) to the corresponding place in the respective matrix, to constrain parameters to be equal we just use the same Symbol in the respective entries.  To fix a parameter (as in the A-Matrix above), we just write down the number we want to fix it to.  All other entries are 0.\n\nSecond, we specify a vector of symbols containing our parameters:\n\nθ = Symbol.(:θ, 1:31)\n\nThird, we construct an object of type RAMMatrices, passing our matrices and parameters, as well as the column names of our matrices.  Those are quite important, as they will be used to rearrange your data to match it to your RAMMatrices specification.\n\nspec = RAMMatrices(;\n    A = A, \n    S = S, \n    F = F, \n    param_labels = θ,\n    vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8, :ind60, :dem60, :dem65]\n)\n\nFinally, we construct a model, passing our RAMMatrices as the specification = ... argument.\n\nmodel = Sem(\n    specification = spec,\n    data = example_data(\"political_democracy\")\n)","category":"section"},{"location":"tutorials/specification/ram_matrices/#Meanstructure","page":"RAMMatrices interface","title":"Meanstructure","text":"According to the RAM, model implied mean values of the observed variables are computed as\n\nmu = F(I-A)^-1M\n\nwhere M is a vector of mean parameters. To estimate the means of the observed variables in our example (and set the latent means to 0), we would specify the model just as before but add \n\n...\n\nM = [:x32; :x33; :x34; :x35; :x36; :x37; :x38; :x39; :x40; :x41; :x42; 0; 0; 0]\n\nθ = Symbol.(:θ, 1:42)\n\nspec = RAMMatrices(;\n    ...,\n    M = M)\n\n...\n","category":"section"},{"location":"tutorials/specification/ram_matrices/#Convert-from-and-to-ParameterTables","page":"RAMMatrices interface","title":"Convert from and to ParameterTables","text":"To convert a RAMMatrices object to a ParameterTable, simply use partable = ParameterTable(ram_matrices).  To convert an object of type ParameterTable to RAMMatrices, you can use ram_matrices = RAMMatrices(partable).","category":"section"},{"location":"internals/files/#Files","page":"files","title":"Files","text":"We briefly describe the file and folder structure of the package.","category":"section"},{"location":"internals/files/#Source-code","page":"files","title":"Source code","text":"Source code is in the \"src\" folder:\n\n\"src\"\n\n\"StructuralEquationModels.jl\" defines the module and the exported objects\n\"types.jl\" defines all abstract types and the basic type hierarchy\n\"objective_gradient_hessian.jl\" contains methods for computing objective, gradient and hessian values for different model types as well as generic fallback methods\nThe folders \"observed\", \"implied\", and \"loss\" contain implementations of specific subtypes (for example, the \"loss\" folder contains a file \"ML.jl\" that implements the SemML loss function).\n\"optimizer\" contains connections to different optimization backends (aka methods for fit)\n\"optim.jl\": connection to the Optim.jl package\n\"frontend\" contains user-facing functions\n\"specification\" contains functionality for model specification\n\"fit\" contains functionality for model assessment, like fit measures and standard errors\n\"additional_functions\" contains helper functions for simulations, loading artifacts (example data) and various other things\n\nCode for the package extentions can be found in the \"ext\" folder:\n\n\"SEMNLOptExt\" for connection to NLopt.jl.\n\"SEMProximalOptExt\" for connection to ProximalAlgorithms.jl.","category":"section"},{"location":"internals/files/#Tests-and-Documentation","page":"files","title":"Tests and Documentation","text":"Tests are in the \"test\" folder, documentation in the \"docs\" folder.","category":"section"},{"location":"tutorials/specification/specification/#Model-specification","page":"Model specification","title":"Model specification","text":"Two things can be used to specify a model: a parameter table or ram matrices. You can convert them to each other, and to make your life easier, we also provide a way to get parameter tables from graphs.\n\nThis leads to the following chart:\n\n(Image: Specification flowchart)\n\nYou can enter model specification at each point, but in general (and especially if you come from lavaan), it is the easiest to follow the red arrows: specify a graph object, convert it to a prameter table, and use this parameter table to construct your models ( just like we did in A first model):\n\nobs_vars = ...\nlat_vars   = ...\n\ngraph = @StenoGraph begin\n    ...\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = lat_vars, \n    observed_vars = obs_vars)\n\nmodel = Sem(\n    specification = partable,\n    ...\n)\n\nOn the following pages, we explain how to enter the specification process at each step, i.e. how to specify models via the Graph interface, the ParameterTable interface, and the RAMMatrices interface.  If you have an OpenMx background, and are familiar with their way of specifying structural equation models via RAM matrices, the RAMMatrices interface may be of interest for you.","category":"section"},{"location":"performance/parametric/#Parametric-types","page":"Parametric Types","title":"Parametric types","text":"Recall that a new composite type in julia can be declared as\n\nstruct MyNewType\n    field1\n    field2\n    ...\nend\n\nOften we can speedup computations by declaring our type as a Parametric Type:\n\nstruct MyNewType{A, B}\n    field1::A\n    field2::B\n    ...\nend\n\ngiving each field a type and adding them as parameters to our type declaration.\n\nRecall our example from Custom loss functions:\n\nstruct Ridge <: SemLossFunction\n    α\n    I\nend\n\nWe could also declare it as a parametric type:\n\nstruct ParametricRidge{X, Y} <: SemLossFunction\n    α::X\n    I::Y\nend\n\nLet's see how this might affect performance:\n\nfunction add_α(ridge1, ridge2)\n    return ridge1.α + ridge2.α \nend\n\nmy_ridge_1 = Ridge(2.5, [2,3])\nmy_ridge_2 = Ridge(25.38, [2,3])\n\nmy_parametric_ridge_1 = ParametricRidge(2.1, [2,3])\nmy_parametric_ridge_2 = ParametricRidge(8.34, [2,3])\n\nusing BenchmarkTools\n\n@benchmark add_α($my_ridge_1, $my_ridge_2)\n\n# output\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  16.073 ns …  1.508 μs  ┊ GC (min … max): 0.00% … 98.35%\n Time  (median):     17.839 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   22.846 ns ± 23.564 ns  ┊ GC (mean ± σ):  1.64% ±  1.70%\n\n@benchmark add_α($my_parametric_ridge_1, $my_parametric_ridge_2)\n\n# output\n\nBenchmarkTools.Trial: 10000 samples with 1000 evaluations.\n Range (min … max):  1.371 ns … 20.250 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     2.097 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   2.169 ns ±  0.829 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n\nwhich is quite a difference. To learn more about parametric types, see the this section in the julia documentation.","category":"section"},{"location":"developer/loss/#Custom-loss-functions","page":"Custom loss functions","title":"Custom loss functions","text":"As an example, we will implement ridge regularization. Maximum likelihood estimation with ridge regularization consists of optimizing the objective\n\nF_ML(theta) + alpha lVert theta_I rVert^2_2\n\nSince we allow for the optimization of sums of loss functions, and the maximum likelihood loss function already exists, we only need to implement the ridge part (and additionally get ridge regularization for WLS and FIML estimation for free).","category":"section"},{"location":"developer/loss/#Minimal","page":"Custom loss functions","title":"Minimal","text":"To define a new loss function, you have to define a new type that is a subtype of SemLossFunction:\n\nstruct Ridge <: SemLossFunction\n    α\n    I\nend\n\nWe store the hyperparameter α and the indices I of the parameters we want to regularize.\n\nAdditionaly, we need to define a method of the function evaluate! to compute the objective:\n\nimport StructuralEquationModels: evaluate!\n\nevaluate!(objective::Number, gradient::Nothing, hessian::Nothing, ridge::Ridge, model::AbstractSem, par) = \n  ridge.α * sum(i -> par[i]^2, ridge.I)\n\nThe function evaluate! recognizes by the types of the arguments objective, gradient and hessian whether it should compute the objective value, gradient or hessian of the model w.r.t. the parameters. In this case, gradient and hessian are of type Nothing, signifying that they should not be computed, but only the objective value.\n\nThat's all we need to make it work! For example, we can now fit A first model with ridge regularization:\n\nWe first give some parameters labels to be able to identify them as targets for the regularization:\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → label(:a)*dem60\n    dem60 → label(:b)*dem65\n    ind60 → label(:c)*dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = latent_vars,\n    observed_vars = observed_vars\n)\n\nparameter_indices = getindex.([param_indices(partable)], [:a, :b, :c])\nmyridge = Ridge(0.01, parameter_indices)\n\nmodel = SemFiniteDiff(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = (SemML, myridge)\n)\n\nmodel_fit = fit(model)\n\nThis is one way of specifying the model - we now have one model with multiple loss functions. Because we did not provide a gradient for Ridge, we have to specify a SemFiniteDiff model that computes numerical gradients with finite difference approximation.\n\nNote that the last argument to the objective! method is the whole model. Therefore, we can access everything that is stored inside our model everytime we compute the objective value for our loss function. Since ridge regularization is a very easy case, we do not need to do this. But maximum likelihood estimation for example depends on both the observed and the model implied covariance matrix. See Second example - maximum likelihood for information on how to do that.","category":"section"},{"location":"developer/loss/#Improve-performance","page":"Custom loss functions","title":"Improve performance","text":"By far the biggest improvements in performance will result from specifying analytical gradients. We can do this for our example:\n\nfunction evaluate!(objective, gradient, hessian::Nothing, ridge::Ridge, model::AbstractSem, par)\n    # compute gradient\n    if !isnothing(gradient)\n        fill!(gradient, 0)\n        gradient[ridge.I] .= 2 * ridge.α * par[ridge.I]\n    end\n    # compute objective\n    if !isnothing(objective) \n        return ridge.α * sum(i -> par[i]^2, ridge.I)\n    end\nend\n\nAs you can see, in this method definition, both objective and gradient can be different from nothing. We then check whether to compute the objective value and/or the gradient with isnothing(objective)/isnothing(gradient). This syntax makes it possible to compute objective value and gradient at the same time, which is beneficial when the the objective and gradient share common computations.\n\nNow, instead of specifying a SemFiniteDiff, we can use the normal Sem constructor:\n\nmodel_new = Sem(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = (SemML, myridge)\n)\n\nmodel_fit = fit(model_new)\n\nThe results are the same, but we can verify that the computational costs are way lower (for this, the julia package BenchmarkTools has to be installed):\n\nusing BenchmarkTools\n\n@benchmark fit(model)\n\n@benchmark fit(model_new)\n\nThe exact results of those benchmarks are of course highly depended an your system (processor, RAM, etc.), but you should see that the median computation time with analytical gradients drops to about 5% of the computation without analytical gradients.\n\nAdditionally, you may provide analytic hessians by writing a respective method for evaluate!. However, this will only matter if you use an optimization algorithm that makes use of the hessians. Our default algorithmn LBFGS from the package Optim.jl does not use hessians (for example, the Newton algorithmn from the same package does).","category":"section"},{"location":"developer/loss/#Convenient","page":"Custom loss functions","title":"Convenient","text":"To be able to build the model with the Outer Constructor, you need to add a constructor for your loss function that only takes keyword arguments and allows for passing optional additional kewyword arguments. A constructor is just a function that creates a new instance of your type:\n\nfunction MyLoss(;arg1 = ..., arg2, kwargs...)\n    ...\n    return MyLoss(...)\nend\n\nAll keyword arguments that a user passes to the Sem constructor are passed to your loss function. In addition, all previously constructed parts of the model (implied and observed part) are passed as keyword arguments as well as the number of parameters n_par = ..., so your constructor may depend on those. For example, the constructor for SemML in our package depends on the additional argument meanstructure as well as the observed part of the model to pre-allocate arrays of the same size as the observed covariance matrix and the observed mean vector:\n\nfunction SemML(;observed, meanstructure = false, approx_H = false, kwargs...)\n\n    isnothing(obs_mean(observed)) ?\n        meandiff = nothing :\n        meandiff = copy(obs_mean(observed))\n\n    return SemML(\n        similar(obs_cov(observed)),\n        similar(obs_cov(observed)),\n        meandiff,\n        approx_H,\n        Val(meanstructure)\n        )\nend","category":"section"},{"location":"developer/loss/#Additional-functionality","page":"Custom loss functions","title":"Additional functionality","text":"","category":"section"},{"location":"developer/loss/#Update-observed-data","page":"Custom loss functions","title":"Update observed data","text":"If you are planing a simulation study where you have to fit the same model to many different datasets, it is computationally beneficial to not build the whole model completely new everytime you change your data. Therefore, we provide a function to update the data of your model, replace_observed(model(semfit); data = new_data). However, we can not know beforehand in what way your loss function depends on the specific datasets. The solution is to provide a method for update_observed. Since Ridge does not depend on the data at all, this is quite easy:\n\nimport StructuralEquationModels: update_observed\n\nupdate_observed(ridge::Ridge, observed::SemObserved; kwargs...) = ridge","category":"section"},{"location":"developer/loss/#Access-additional-information","page":"Custom loss functions","title":"Access additional information","text":"If you want to provide a way to query information about loss functions of your type, you can provide functions for that:\n\nhyperparameter(ridge::Ridge) = ridge.α\nregularization_indices(ridge::Ridge) = ridge.I","category":"section"},{"location":"developer/loss/#Second-example-maximum-likelihood","page":"Custom loss functions","title":"Second example - maximum likelihood","text":"Let's make a sligtly more complicated example: we will reimplement maximum likelihood estimation.\n\nTo keep it simple, we only cover models without a meanstructure. The maximum likelihood objective is defined as\n\nF_ML = log det Sigma_i + mathrmtrleft(Sigma_i^-1 Sigma_o right)\n\nwhere Sigma_i is the model implied covariance matrix and Sigma_o is the observed covariance matrix. We can query the model implied covariance matrix from the implied par of our model, and the observed covariance matrix from the observed path of our model.\n\nTo get information on what we can access from a certain implied or observed type, we can check it`s documentation an the pages API - model parts or via the help mode of the REPL:\n\njulia>?\n\nhelp?> RAM\n\nhelp?> SemObservedData\n\nWe see that the model implied covariance matrix can be assessed as Σ(implied) and the observed covariance matrix as obs_cov(observed).\n\nWith this information, we write can implement maximum likelihood optimization as\n\nstruct MaximumLikelihood <: SemLossFunction end\n\nusing LinearAlgebra\nimport StructuralEquationModels: obs_cov, evaluate!\n\nfunction evaluate!(objective::Number, gradient::Nothing, hessian::Nothing, semml::MaximumLikelihood, model::AbstractSem, par)\n    # access the model implied and observed covariance matrices\n    Σᵢ = implied(model).Σ\n    Σₒ = obs_cov(observed(model))\n    # compute the objective\n    if isposdef(Symmetric(Σᵢ)) # is the model implied covariance matrix positive definite?\n        return logdet(Σᵢ) + tr(inv(Σᵢ)*Σₒ)\n    else\n        return Inf\n    end\nend\n\nto deal with eventual non-positive definiteness of the model implied covariance matrix, we chose the pragmatic way of returning infinity whenever this is the case.\n\nLet's specify and fit a model:\n\nmodel_ml = SemFiniteDiff(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = MaximumLikelihood()\n)\n\nmodel_fit = fit(model_ml)\n\nIf you want to differentiate your own loss functions via automatic differentiation, check out the AutoDiffSEM package.","category":"section"},{"location":"developer/observed/#Custom-observed-types","page":"Custom observed types","title":"Custom observed types","text":"The implementation of new observed types is very similar to loss functions, so we will just go over it briefly (for additional information, revisit Custom loss functions).\n\nFirst, we need to define a new struct that is a subtype of SemObserved:\n\nstruct MyObserved <: SemObserved\n    ...\nend\n\nAdditionally, we can write an outer constructor that will typically depend on the keyword argument data = ...:\n\nfunction MyObserved(;data, kwargs...)\n    ...\n    return MyObserved(...)\nend\n\nTo compute some fit indices, you need to provide methods for\n\n# Number of samples (observations) in the dataset\nnsamples(observed::MyObserved) = ...\n# Number of observed variables\nnobserved_vars(observed::MyObserved) = ...\n\nAs always, you can add additional methods for properties that implied types and loss function want to access, for example (from the SemObservedData implementation):\n\nobs_cov(observed::SemObservedData) = observed.obs_cov","category":"section"},{"location":"tutorials/collection/collection/#Collections","page":"Collections","title":"Collections","text":"With StructuralEquationModels.jl, you can fit weighted sums of structural equation models.  The most common use case for this are Multigroup models.  Another use case may be optimizing the sum of loss functions for some of which you do know the analytic gradient, but not for others.  In this case, you can optimize the sum of a Sem and a SemFiniteDiff (or any other differentiation method).\n\nTo use this feature, you have to construct a SemEnsemble model, which is actually quite easy:\n\n# models\nmodel_1 = Sem(...)\n\nmodel_2 = SemFiniteDiff(...)\n\nmodel_3 = Sem(...)\n\nmodel_ensemble = SemEnsemble(model_1, model_2, model_3)\n\nSo you just construct the individual models (however you like) and pass them to SemEnsemble. You may also pass a vector of weigths to SemEnsemble. By default, those are set to N_modelN_total, i.e. each model is weighted by the number of observations in it's data (which matches the formula for multigroup models).\n\nMultigroup models can also be specified via the graph interface; for an example, see Multigroup models.","category":"section"},{"location":"tutorials/collection/collection/#API-collections","page":"Collections","title":"API - collections","text":"","category":"section"},{"location":"tutorials/collection/collection/#StructuralEquationModels.SemEnsemble","page":"Collections","title":"StructuralEquationModels.SemEnsemble","text":"(1) SemEnsemble(models...; weights = nothing, groups = nothing, kwargs...)\n\n(2) SemEnsemble(;specification, data, groups, column = :group, kwargs...)\n\nConstructor for ensemble models. (2) can be used to conveniently specify multigroup models.\n\nArguments\n\nmodels...: AbstractSems.\nweights::Vector:  Weights for each model. Defaults to the number of observed data points.\nspecification::EnsembleParameterTable: Model specification.\ndata::DataFrame: Observed data. Must contain a column of type Vector{Symbol} that contains the group.\ngroups::Vector{Symbol}: Group names.\ncolumn::Symbol: Name of the column in data that contains the group.\n\nAll additional kwargs are passed down to the model parts.\n\nReturns a SemEnsemble with fields\n\nn::Int: Number of models.\nsems::Tuple: AbstractSems.\nweights::Vector: Weights for each model.\nparam_labels::Vector: Stores parameter labels and their position.\n\nFor instructions on multigroup models, see the online documentation.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/collection/collection/#StructuralEquationModels.AbstractSemCollection","page":"Collections","title":"StructuralEquationModels.AbstractSemCollection","text":"Supertype for all collections of multiple SEMs\n\n\n\n\n\n","category":"type"},{"location":"tutorials/construction/construction/#Model-Construction","page":"Model Construction","title":"Model Construction","text":"There are two different ways of constructing a SEM in our package. You can use the Outer Constructor or Build by parts. The final models will be the same, the outer constructor just has some sensible defaults that make your life easier. All tutorials until now used the outer constructor Sem(specification = ..., data = ..., ...), which is normally the more convenient way. However, our package is build for extensibility, so there may be cases where user-defined parts of a model do not work with the outer constructor. Therefore, building the model by parts is always available as a fallback.","category":"section"},{"location":"tutorials/specification/graph_interface/#Graph-interface","page":"Graph interface","title":"Graph interface","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/#Workflow","page":"Graph interface","title":"Workflow","text":"As discussed before, when using the graph interface, you can specify your model as a graph\n\ngraph = @StenoGraph begin\n    ...\nend\n\nand convert it to a ParameterTable to construct your models:\n\nobs_vars = ...\nlat_vars   = ...\n\npartable = ParameterTable(\n    graph,\n    latent_vars = lat_vars, \n    observed_vars = obs_vars)\n\nmodel = Sem(\n    specification = partable,\n    ...\n)","category":"section"},{"location":"tutorials/specification/graph_interface/#Parameters","page":"Graph interface","title":"Parameters","text":"In general, there are two different types of parameters: directed and indirected parameters. A directed parameter from the variable x to y can be specified as x → y (or equivalently as y ← x); an undirected parameter as x ↔ y. We allow multiple variables on both sides of an arrow, for example x → [y z] or [a b] → [c d]. The later specifies element wise edges; that is its the same as a → c; b → d. If you want edges corresponding to the cross-product, we have the double lined arrow [a b] ⇒ [c d], corresponding to a → c; a → d; b → c; b → d. The undirected arrows ↔ (element-wise) and ⇔ (crossproduct) behave the same way.\n\nnote: Unicode symbols in julia\nThe → symbol is a unicode symbol allowed in julia (among many others; see this list). You can enter it in the julia REPL or the vscode IDE by typing \\to followed by hitting tab. Similarly, ← = \\leftarrow,\n↔ = \\leftrightarrow,\n⇒ = \\Rightarrow,\n⇐ = \\Leftarrow,\n⇔ = \\LeftrightarrowThis may seem cumbersome at first, but with some practice allows you to specify your models in a really elegant way: [x₁ x₂ x₃] ← ξ → η → [y₁ y₂ y₃].","category":"section"},{"location":"tutorials/specification/graph_interface/#Options","page":"Graph interface","title":"Options","text":"The graph syntax allows you to fix parameters to specific values, label them, and encode equality constraints by giving different parameters the same label. The following syntax example\n\ngraph = @StenoGraph begin\n\n    ξ₁ → fixed(1.0)*x1 + x2 + label(:a)*x3\n    ξ₂ → fixed(1.0)*x4 + x5 + label(:λ₁)*x6\n    ξ₃ → fixed(NaN)*x7 + x8 + label(:λ₁)*x9\n\n    ξ₃ ↔ fixed(1.0)*ξ₃\nend\n\nwould \n\nfix the directed effects from ξ₁ to x1 and from ξ₂ to x2 to 1\nleave the directed effect from ξ₃ to x7 free but instead restrict the variance of ξ₃ to 1\ngive the effect from ξ₁ to x3 the label :a (which can be convenient later if you want to retrieve information from your model about that specific parameter)\nconstrain the effect from ξ₂ to x6 and ξ₃ to x9 to be equal as they are both labeled the same.","category":"section"},{"location":"tutorials/specification/graph_interface/#Using-variables-inside-the-graph-specification","page":"Graph interface","title":"Using variables inside the graph specification","text":"As you saw above and in the A first model example, the graph object needs to be converted to a parameter table:\n\npartable = ParameterTable(\n    graph,\n    latent_vars = lat_vars, \n    observed_vars = obs_vars)\n\nThe ParameterTable constructor also needs you to specify a vector of observed and latent variables, in the example above this would correspond to\n\nobs_vars = [:x1 :x2 :x3 :x4 :x5 :x6 :x7 :x8 :x9]\nlat_vars   = [:ξ₁ :ξ₂ :ξ₃]\n\nThe variable names (:x1) have to be symbols, the syntax :something creates an object of type Symbol. But you can also use vectors of symbols inside the graph specification, escaping them with _(...). For example, this graph specification\n\n@StenoGraph begin\n    _(obs_vars) ↔ _(obs_vars)\n    _(lat_vars) ⇔ _(lat_vars)\nend\n\ncreates undirected effects coresponding to \n\nthe variances of all observed variables and\nthe variances plus covariances of all latent variables\n\nSo if you want to work with a subset of variables, simply specify a vector of symbols somevars = [...], and inside the graph specification, refer to them as _(somevars).","category":"section"},{"location":"tutorials/specification/graph_interface/#Meanstructure","page":"Graph interface","title":"Meanstructure","text":"Mean parameters are specified as a directed effect from 1 to the respective variable. In our example above, to estimate a mean parameter for all observed variables, we may write\n\n@StenoGraph begin\n    Symbol(1) → _(obs_vars)\nend","category":"section"},{"location":"tutorials/specification/graph_interface/#Further-Reading","page":"Graph interface","title":"Further Reading","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/#What's-this-strange-looking-@-thing?","page":"Graph interface","title":"What's this strange looking @-thing?","text":"The syntax to specify graphs (@StenoGraph) may seem a bit strange if you are not familiar with the julia language. It is called a macro, but explaining this concept in detail is beyond this documentation (and not necessary to understand to specify models). However, if you want to know more about it, you may have a look at the respective part of the manual.","category":"section"},{"location":"tutorials/specification/graph_interface/#The-StenoGraphs-Package","page":"Graph interface","title":"The StenoGraphs Package","text":"Behind the scenes, we are using the StenoGraphs package to specify our graphs. It makes a domain specific language available that allows you to specify graphs with arbitrary information attached to its edges and nodes (for structural equation models, this may be the name or the value of a parameter). Is also allows you to specify your own types to \"attach\" to the graph, called a Modifier. So if you contemplate about writing your own modifier (e.g., to mark a variable as ordinal, an effect as quadratic, ...), please refer to the StenoGraphs documentation.","category":"section"},{"location":"tutorials/backends/nlopt/#Using-NLopt.jl","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"SemOptimizerNLopt implements the connection to NLopt.jl. It is only available if the NLopt package is loaded alongside StructuralEquationModels.jl in the running Julia session. It takes a bunch of arguments:\n\n    •  algorithm: optimization algorithm\n\n    •  options::Dict{Symbol, Any}: options for the optimization algorithm\n\n    •  local_algorithm: local optimization algorithm\n\n    •  local_options::Dict{Symbol, Any}: options for the local optimization algorithm\n\n    •  equality_constraints::Vector{NLoptConstraint}: vector of equality constraints\n\n    •  inequality_constraints::Vector{NLoptConstraint}: vector of inequality constraints\n\nConstraints are explained in the section on Constrained optimization.\n\nThe defaults are LBFGS as the optimization algorithm and the standard options from NLopt.jl. We can choose something different:\n\nusing NLopt\n\nmy_optimizer = SemOptimizerNLopt(;\n    algorithm = :AUGLAG,\n    options = Dict(:maxeval => 200),\n    local_algorithm = :LD_LBFGS,\n    local_options = Dict(:ftol_rel => 1e-6)\n)\n\nThis uses an augmented lagrangian method with LBFGS as the local optimization algorithm, stops at a maximum of 200 evaluations and uses a relative tolerance of the objective value of 1e-6 as the stopping criterion for the local algorithm.\n\nTo see how to use the optimizer to actually fit a model now, check out the Model fitting section.\n\nIn the NLopt docs, you can find explanations about the different algorithms and a tutorial that also explains the different options.\n\nTo choose an algorithm, just pass its name without the 'NLOPT_' prefix (for example, 'NLOPT_LD_SLSQP' can be used by passing algorithm = :LD_SLSQP).\n\nThe README of the julia package may also be helpful, and provides a list of options:\n\nalgorithm\nstopval\nftol_rel\nftol_abs\nxtol_rel\nxtol_abs\nconstrtol_abs\nmaxeval\nmaxtime\ninitial_step\npopulation\nseed\nvector_storage","category":"section"},{"location":"performance/simulation/#Simulation-studies","page":"Simulation studies","title":"Simulation studies","text":"","category":"section"},{"location":"performance/simulation/#Replace-observed-data","page":"Simulation studies","title":"Replace observed data","text":"In simulation studies, a common task is fitting the same model to many different datasets. It would be a waste of resources to reconstruct the complete model for each dataset. We therefore provide the function replace_observed to change the observed part of a model, without necessarily reconstructing the other parts.\n\nFor the A first model, you would use it as\n\ndata = example_data(\"political_democracy\")\n\ndata_1 = data[1:30, :]\n\ndata_2 = data[31:75, :]\n\nmodel = Sem(\n    specification = partable,\n    data = data_1\n)\n\nmodel_updated = replace_observed(model; data = data_2, specification = partable)\n\nIf you are building your models by parts, you can also update each part seperately with the function update_observed. For example,\n\n\nnew_observed = SemObservedData(;data = data_2, specification = partable)\n\nmy_optimizer = SemOptimizerOptim()\n\nnew_optimizer = update_observed(my_optimizer, new_observed)","category":"section"},{"location":"performance/simulation/#Multithreading","page":"Simulation studies","title":"Multithreading","text":"danger: Thread safety\nThis is only relevant when you are planning to fit updated models in parallelModels generated by replace_observed may share the same objects in memory (e.g. some parts of  model and model_updated are the same objects in memory.) Therefore, fitting both of these models in parallel will lead to race conditions,  possibly crashing your computer. To avoid these problems, you should copy model before updating it.\n\nTaking into account the warning above, fitting multiple models in parallel becomes as easy as:\n\nmodel1 = Sem(\n    specification = partable,\n    data = data_1\n)\n\nmodel2 = deepcopy(replace_observed(model; data = data_2, specification = partable))\n\nmodels = [model1, model2]\nfits = Vector{SemFit}(undef, 2)\n\nThreads.@threads for i in 1:2\n    fits[i] = fit(models[i])\nend","category":"section"},{"location":"performance/simulation/#API","page":"Simulation studies","title":"API","text":"","category":"section"},{"location":"performance/simulation/#StructuralEquationModels.replace_observed","page":"Simulation studies","title":"StructuralEquationModels.replace_observed","text":"(1) replace_observed(model::AbstractSemSingle; kwargs...)\n\n(2) replace_observed(model::AbstractSemSingle, observed; kwargs...)\n\n(3) replace_observed(model::SemEnsemble; column = :group, weights = nothing, kwargs...)\n\nReturn a new model with swaped observed part.\n\nArguments\n\nmodel::AbstractSemSingle: model to swap the observed part of.\nkwargs: additional keyword arguments; typically includes data and specification\nobserved: Either an object of subtype of SemObserved or a subtype of SemObserved\n\nFor SemEnsemble models:\n\ncolumn: if a DataFrame is passed as data = ..., which column signifies the group?\nweights: how to weight the different sub-models,   defaults to number of samples per group in the new data\nkwargs: has to be a dict with keys equal to the group names.   For data can also be a DataFrame with column containing the group information,   and for specification can also be an EnsembleParameterTable.\n\nExamples\n\nSee the online documentation on Replace observed data.\n\n\n\n\n\n","category":"function"},{"location":"performance/simulation/#StructuralEquationModels.update_observed","page":"Simulation studies","title":"StructuralEquationModels.update_observed","text":"update_observed(to_update, observed::SemObserved; kwargs...)\n\nUpdate a SemImplied, SemLossFunction or SemOptimizer object to use a SemObserved object.\n\nExamples\n\nSee the online documentation on Replace observed data.\n\nImplementation\n\nYou can provide a method for this function when defining a new type, for more information on this see the online developer documentation on Update observed data.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/construction/build_by_parts/#Build-by-parts","page":"Build by parts","title":"Build by parts","text":"You can always build a model by parts - that is, you construct the observed, implied, loss and optimizer part seperately.\n\nAs an example on how this works, we will build A first model in parts.\n\nFirst, we specify the model just as usual:\n\nusing StructuralEquationModels\n\ndata = example_data(\"political_democracy\")\n\nobs_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlat_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(obs_vars) ↔ _(obs_vars)\n    _(lat_vars) ↔ _(lat_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = lat_vars, \n    observed_vars = obs_vars)\n\nNow, we construct the different parts:\n\n# observed -----------------------------------------------------------------------------\nobserved = SemObservedData(specification = partable, data = data)\n\n# implied ------------------------------------------------------------------------------\nimplied_ram = RAM(specification = partable)\n\n# loss ---------------------------------------------------------------------------------\nml = SemML(observed = observed)\n\nloss_ml = SemLoss(ml)\n\n# optimizer ----------------------------------------------------------------------------\noptimizer = SemOptimizerOptim()\n\n# model --------------------------------------------------------------------------------\n\nmodel_ml = Sem(observed, implied_ram, loss_ml)\n\nfit(optimizer, model_ml)","category":"section"},{"location":"tutorials/construction/outer_constructor/#Outer-Constructor","page":"Outer Constructor","title":"Outer Constructor","text":"We already have seen the outer constructor in action in A first model:\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\n# output\n\nStructural Equation Model\n- Loss Functions\n   SemML\n- Fields\n   observed:  SemObservedData\n   implied:   RAM\n\nThe output of this call tells you exactly what model you just constructed (i.e. what the loss functions, observed, implied and optimizer parts are).\n\nAs you can see, by default, we use maximum likelihood estimation abd the RAM implied type. To choose something different, you can provide it as a keyword argument:\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    observed = ...,\n    implied = ...,\n    loss = ...,\n)\n\nFor example, to construct a model for weighted least squares estimation that uses symbolic precomputation, write\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    implied = RAMSymbolic,\n    loss = SemWLS,\n    optimizer = SemOptimizerOptim\n)\n\nIn the section on Our Concept of a Structural Equation Model, we go over the different options you have for each part of the model, and in API - model parts we explain each option in detail. Let's make another example: to use full information maximum likelihood information (FIML), we use\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    loss = SemFIML,\n    observed = SemObservedMissing,\n    meanstructure = true\n)\n\nYou may also provide addition arguments for specific parts of the model. For example, WLS estimation uses per default\n\nW = frac12 D^T(S^-1otimes S^-1)D\n\nas the weight matrix, where D is the so-called duplication matrix and S is the observed covariance matrix. However, you can pass any other weight matrix you want (e.g., UWL, DWLS, ADF estimation) as a keyword argument:\n\nW = ...\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    implied = RAMSymbolic,\n    loss = SemWLS,\n    wls_weight_matrix = W\n)\n\n\nTo see what additional keyword arguments are supported, you can consult the documentation of the specific part of the model (either in the REPL by typing ? to enter the help mode and then typing the name of the thing you want to know something about, or in the online section API - model parts):\n\njulia>?\n\nhelp>SemObservedMissing\n\n# output\n\n  For observed data with missing values.\n\n  Constructor\n  ≡≡≡≡≡≡≡≡≡≡≡\n\n  SemObservedMissing(;\n      data,\n      observed_vars = nothing,\n      specification = nothing,\n      kwargs...)\n\n  Arguments\n  ≡≡≡≡≡≡≡≡≡\n\n    •  specification: optional SEM model specification\n       (SemSpecification)\n\n    •  data: observed data\n\n    •  observed_vars::Vector{Symbol}: column names of the data (if\n       the object passed as data does not have column names, i.e. is\n       not a data frame)\n\n  ────────────────────────────────────────────────────────────────────────\n\nExtended help is available with `??SemObservedMissing`","category":"section"},{"location":"tutorials/construction/outer_constructor/#Optimize-loss-functions-without-analytic-gradient","page":"Outer Constructor","title":"Optimize loss functions without analytic gradient","text":"For loss functions without analytic gradients, it is possible to use finite difference approximation or automatic differentiation. All loss functions provided in the package do have analytic gradients (and some even hessians or approximations thereof), so there is no need do use this feature if you are only working with them. However, if you implement your own loss function, you do not have to provide analytic gradients.\n\nTo use finite difference approximation, you may construct your model just as before, but swap the Sem constructor for SemFiniteDiff. For example\n\nmodel = SemFiniteDiff(\n    specification = partable,\n    data = data\n)\n\nconstructs a model that will use finite difference approximation if you estimate the parameters via fit(model).","category":"section"},{"location":"performance/mixed_differentiation/#Mixed-analytical-and-automatic-differentiation","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"This way of specifying our model is not ideal, however, because now also the maximum likelihood loss function lives inside a SemFiniteDiff model, and this means even though we have defined analytical gradients for it, we do not make use of them.\n\nA more efficient way is therefore to specify our model as an ensemble model: \n\nmodel_ml = Sem(\n    specification = partable,\n    data = data,\n    loss = SemML\n)\n\nmodel_ridge = SemFiniteDiff(\n    specification = partable,\n    data = data,\n    loss = myridge\n)\n\nmodel_ml_ridge = SemEnsemble(model_ml, model_ridge)\n\nmodel_ml_ridge_fit = fit(model_ml_ridge)\n\nThe results of both methods will be the same, but we can verify that the computation costs differ (the package BenchmarkTools has to be installed for this):\n\nusing BenchmarkTools\n\n@benchmark fit(model)\n\n@benchmark fit(model_ml_ridge)","category":"section"},{"location":"tutorials/inspection/inspection/#Model-inspection","page":"Model Inspection","title":"Model inspection","text":"After you fitted a model,\n\nmodel_fit = fit(model)\n\nyou end up with an object of type SemFit.\n\nYou can get some more information about it by using the details function:\n\ndetails(model_fit)\n\nTo compute fit measures, we use\n\nfit_measures(model_fit)\n\nor compute them individually:\n\nAIC(model_fit)\n\nA list of available Fit measures is at the end of this page.\n\nTo inspect the parameter estimates, we can update a ParameterTable object and call details on it:\n\nupdate_estimate!(partable, model_fit)\n\ndetails(partable)\n\nWe can also update the ParameterTable object with other information via update_partable!. For example, if we want to compare hessian-based and bootstrap-based standard errors, we may write\n\nse_bs = se_bootstrap(model_fit; n_boot = 20)\nse_he = se_hessian(model_fit)\n\nupdate_partable!(partable, :se_hessian, model_fit, se_he)\nupdate_partable!(partable, :se_bootstrap, model_fit, se_bs)\n\ndetails(partable)","category":"section"},{"location":"tutorials/inspection/inspection/#Export-results","page":"Model Inspection","title":"Export results","text":"You may convert a ParameterTable to a DataFrame and use the DataFrames package for further analysis (or to save it to your hard drive).\n\nusing DataFrames\n\nparameters_df = DataFrame(partable)","category":"section"},{"location":"tutorials/inspection/inspection/#API-model-inspection","page":"Model Inspection","title":"API - model inspection","text":"","category":"section"},{"location":"tutorials/inspection/inspection/#Additional-functions","page":"Model Inspection","title":"Additional functions","text":"Additional functions that can be used to extract information from a SemFit object:","category":"section"},{"location":"tutorials/inspection/inspection/#Fit-measures","page":"Model Inspection","title":"Fit measures","text":"","category":"section"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.details","page":"Model Inspection","title":"StructuralEquationModels.details","text":"(1) details(sem_fit::SemFit; show_fitmeasures = false)\n\n(2) details(partable::AbstractParameterTable; ...)\n\nPrint information about (1) a fitted SEM or (2) a parameter table to stdout.\n\nExtended help\n\nAddition keyword arguments\n\ndigits = 2: controls precision of printed estimates, standard errors, etc.\ncolor = :light_cyan: color of some parts of the printed output. Can be adjusted for readability.\nsecondary_color = :light_yellow\nshow_variables = true\nshow_columns = nothing: columns names to include in the output e.g.[:from, :to, :estimate])\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.update_estimate!","page":"Model Inspection","title":"StructuralEquationModels.update_estimate!","text":"update_estimate!(\n    partable::AbstractParameterTable,\n    fit::SemFit)\n\nWrite parameter estimates from fit to the :estimate column of partable\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.update_partable!","page":"Model Inspection","title":"StructuralEquationModels.update_partable!","text":"(1) update_partable!(partable::AbstractParameterTable, column, fitted:SemFit, params, default = nothing)\n\n(2) update_partable!(partable::AbstractParameterTable, column, param_labels::Vector{Symbol}, params, default = nothing)\n\nAdd a new column to a parameter table.  column is the name of the column, params contains the values of the new column, and fitted or param_labels is used to match the values to the correct parameter labels. The default value is used if a parameter in partable does not occur in param_labels.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.SemFit","page":"Model Inspection","title":"StructuralEquationModels.SemFit","text":"SemFit\n\nFitted structural equation model.\n\nInterfaces\n\nminimum(::SemFit) -> minimum objective value\nsolution(::SemFit) -> parameter estimates\nstart_val(::SemFit) -> starting values\nmodel(::SemFit)\noptimization_result(::SemFit)\noptimizer(::SemFit) -> optimization algorithm\nn_iterations(::SemFit) -> number of iterations\nconvergence(::SemFit) -> convergence properties\n\n\n\n\n\n","category":"type"},{"location":"tutorials/inspection/inspection/#StatsAPI.params","page":"Model Inspection","title":"StatsAPI.params","text":"params(model)\n\nReturn all parameters of a model.\n\n\n\n\n\nparams(partable::ParameterTable) -> Vector{Symbol}\n\nReturn the vector of SEM model parameter identifiers.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.param_labels","page":"Model Inspection","title":"StructuralEquationModels.param_labels","text":"param_labels(semobj) -> Vector{Symbol}\n\nReturn the vector of parameter labels (in the same order as params).\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.nparams","page":"Model Inspection","title":"StructuralEquationModels.nparams","text":"nparams(semobj)\n\nReturn the number of parameters in a SEM model associated with semobj.\n\nSee also params.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.nsamples","page":"Model Inspection","title":"StructuralEquationModels.nsamples","text":"nsamples(semobj)\n\nReturn the number of samples (observed data points).\n\nFor ensemble models, return the sum over all submodels.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.nobserved_vars","page":"Model Inspection","title":"StructuralEquationModels.nobserved_vars","text":"nobserved_vars(semobj)\n\nReturn the number of observed variables in a SEM model associated with semobj.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.fit_measures","page":"Model Inspection","title":"StructuralEquationModels.fit_measures","text":"fit_measures(sem_fit, args...)\n\nReturn a default set of fit measures or the fit measures passed as args....\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.AIC","page":"Model Inspection","title":"StructuralEquationModels.AIC","text":"AIC(sem_fit::SemFit)\n\nReturn the akaike information criterion.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.BIC","page":"Model Inspection","title":"StructuralEquationModels.BIC","text":"BIC(sem_fit::SemFit)\n\nReturn the bayesian information criterion.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.χ²","page":"Model Inspection","title":"StructuralEquationModels.χ²","text":"χ²(sem_fit::SemFit)\n\nReturn the χ² value.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StatsAPI.dof","page":"Model Inspection","title":"StatsAPI.dof","text":"dof(model::StatisticalModel)\n\nReturn the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.\n\n\n\n\n\ndof(sem_fit::SemFit)\ndof(model::AbstractSem)\n\nReturn the degrees of freedom.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.minus2ll","page":"Model Inspection","title":"StructuralEquationModels.minus2ll","text":"minus2ll(sem_fit::SemFit)\n\nReturn the negative 2* log likelihood.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.p_value","page":"Model Inspection","title":"StructuralEquationModels.p_value","text":"p(sem_fit::SemFit)\n\nReturn the p value computed from the χ² test statistic.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.RMSEA","page":"Model Inspection","title":"StructuralEquationModels.RMSEA","text":"RMSEA(sem_fit::SemFit)\n\nReturn the RMSEA.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/regularization/regularization/#Regularization","page":"Regularization","title":"Regularization","text":"","category":"section"},{"location":"tutorials/regularization/regularization/#Setup","page":"Regularization","title":"Setup","text":"For ridge regularization, you can simply use SemRidge as an additional loss function  (for example, a model with the loss functions SemML and SemRidge corresponds to ridge-regularized maximum likelihood estimation).\n\nFor lasso, elastic net and (far) beyond, you can load the ProximalAlgorithms.jl and ProximalOperators.jl packages alongside StructuralEquationModels:\n\nusing Pkg\nPkg.add(\"ProximalAlgorithms\")\nPkg.add(\"ProximalOperators\")\n\nusing StructuralEquationModels, ProximalAlgorithms, ProximalOperators","category":"section"},{"location":"tutorials/regularization/regularization/#SemOptimizerProximal","page":"Regularization","title":"SemOptimizerProximal","text":"To estimate regularized models, we provide a \"building block\" for the optimizer part, called SemOptimizerProximal. It connects our package to the ProximalAlgorithms.jl optimization backend, providing so-called proximal optimization algorithms.  Those can handle, amongst other things, various forms of regularization.\n\nIt can be used as\n\nSemOptimizerProximal(\n    algorithm = ProximalAlgorithms.PANOC(),\n    operator_g,\n    operator_h = nothing\n    )\n\nThe proximal operator (aka the regularization function) can be passed as operator_g. The available Algorithms are listed here.","category":"section"},{"location":"tutorials/regularization/regularization/#First-example-lasso","page":"Regularization","title":"First example - lasso","text":"To show how it works, let's revisit A first model:\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    dem60 ← ind60\n    dem65 ← dem60\n    dem65 ← ind60\n\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    y1 ↔ label(:cov_15)*y5\n    y2 ↔ label(:cov_24)*y4 + label(:cov_26)*y6\n    y3 ↔ label(:cov_37)*y7\n    y4 ↔ label(:cov_48)*y8\n    y6 ↔ label(:cov_68)*y8\n\nend\n\npartable = ParameterTable(\n    graph,\n    latent_vars = latent_vars, \n    observed_vars = observed_vars\n)\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\nWe labeled the covariances between the items because we want to regularize those:\n\nind = getindex.(\n    [param_indices(model)], \n    [:cov_15, :cov_24, :cov_26, :cov_37, :cov_48, :cov_68])\n\nIn the following, we fit the same model with lasso regularization of those covariances. The lasso penalty is defined as\n\nsum lambda_i lvert theta_i rvert\n\nFrom the previously linked documentation, we find that lasso regularization is named NormL1 in the ProximalOperators package, and that we can pass an array of hyperparameters (λ) to control the amount of regularization for each parameter. To regularize only the observed item covariances, we define λ as\n\nλ = zeros(31); λ[ind] .= 0.02\n\nand use SemOptimizerProximal.\n\noptimizer_lasso = SemOptimizerProximal(\n    operator_g = NormL1(λ)\n    )\n\nmodel_lasso = Sem(\n    specification = partable,\n    data = data\n)\n\nLet's fit the regularized model\n\n\nfit_lasso = fit(optimizer_lasso, model_lasso)\n\nand compare the solution to unregularizted estimates:\n\nsem_fit = fit(model)\n\nupdate_estimate!(partable, sem_fit)\n\nupdate_partable!(partable, :estimate_lasso, fit_lasso, solution(fit_lasso))\n\ndetails(partable)\n\nInstead of explicitely defining a SemOptimizerProximal object, you can also pass engine = :Proximal and additional keyword arguments to fit:\n\nsem_fit = fit(model; engine = :Proximal, operator_g = NormL1(λ))","category":"section"},{"location":"tutorials/regularization/regularization/#Second-example-mixed-l1-and-l0-regularization","page":"Regularization","title":"Second example - mixed l1 and l0 regularization","text":"You can choose to penalize different parameters with different types of regularization functions. Let's use the lasso again on the covariances, but additionally penalyze the error variances of the observed items via l0 regularization.\n\nThe l0 penalty is defined as\n\nlambda mathrmnnz(theta)\n\nTo define a sup of separable proximal operators (i.e. no parameter is penalized twice), we can use SlicedSeparableSum from the ProximalOperators package:\n\nprox_operator = SlicedSeparableSum((NormL0(20.0), NormL1(0.02), NormL0(0.0)), ([ind], [9:11], [vcat(1:8, 12:25)]))\n\nmodel_mixed = Sem(\n    specification = partable,\n    data = data,    \n)\n\nfit_mixed = fit(model_mixed; engine = :Proximal, operator_g = prox_operator)\n\nLet's again compare the different results:\n\nupdate_partable!(partable, :estimate_mixed, fit_mixed, solution(fit_mixed))\n\ndetails(partable)","category":"section"},{"location":"performance/mkl/#Use-MKL","page":"MKL","title":"Use MKL","text":"Depending on the machine and the specific models, we sometimes observed large performance benefits from using MKL as a backend for matrix operations.  Fortunately, this is very simple to do in julia, so you can just try it out and check if turns out to be beneficial in your use case.\n\nWe install the MKL.jl package:\n\nusing Pkg; Pkg.add(\"MKL\")\n\nWhenever we execute using MKL in a julia session, from now on MKL will be used as a backend. To check the installation:\n\nusing LinearAlgebra\n\nBLAS.get_config()\n\nusing MKL\n\nBLAS.get_config()\n\nTo check the performance implications for fitting a SEM, you can use the BenchmarkTools package:\n\nusing BenchmarkTools\n\n@benchmark fit($your_model)\n\nusing MKL\n\n@benchmark fit($your_model)","category":"section"},{"location":"tutorials/fitting/fitting/#Model-fitting","page":"Model Fitting","title":"Model fitting","text":"As we saw in A first model, after you have build a model, you can fit it via\n\nmodel_fit = fit(model)\n\n# output\n\nFitted Structural Equation Model \n=============================================== \n--------------------- Model ------------------- \n\nStructural Equation Model \n- Loss Functions \n   SemML\n- Fields \n   observed:  SemObservedData \n   implied:   RAM \n   optimizer: SemOptimizerOptim \n\n------------- Optimization result ------------- \n\n * Status: success\n\n * Candidate solution\n    Final objective value:     2.120543e+01\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 6.13e-05 ≰ 1.5e-08\n    |x - x'|/|x'|          = 8.21e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.05e-09 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 4.94e-11 ≤ 1.0e-10\n    |g(x)|                 = 2.48e-05 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    175\n    f(x) calls:    524\n    ∇f(x) calls:   524","category":"section"},{"location":"tutorials/fitting/fitting/#Choosing-an-optimizer","page":"Model Fitting","title":"Choosing an optimizer","text":"To choose a different optimizer, you can call fit with the keyword argument engine = ..., and pass additional keyword arguments:\n\nusing Optim\n\nmodel_fit = fit(model; engine = :Optim, algorithm = BFGS())\n\nAvailable options for engine are :Optim, :NLopt and :Proximal, where :NLopt and :Proximal are only available if the NLopt.jl and ProximalAlgorithms.jl packages are loaded respectively.\n\nThe available keyword arguments are listed in the sections Using Optim.jl, Using NLopt.jl and Regularization.\n\nAlternative, you can also explicitely define a SemOptimizer and pass it as the first argument to fit:\n\nmy_optimizer = SemOptimizerOptim(algorithm = BFGS())\n\nfit(my_optimizer, model)\n\nYou may also optionally specify Starting values.","category":"section"},{"location":"tutorials/fitting/fitting/#API-model-fitting","page":"Model Fitting","title":"API - model fitting","text":"","category":"section"},{"location":"tutorials/fitting/fitting/#StatsAPI.fit","page":"Model Fitting","title":"StatsAPI.fit","text":"Fit a statistical model.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/concept/#Our-Concept-of-a-Structural-Equation-Model","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"In our package, every Structural Equation Model (Sem) consists of three parts (four, if you count the optimizer):\n\n(Image: SEM concept)\n\nThose parts are interchangable building blocks (like 'Legos'), i.e. there are different pieces available you can choose as the observed slot of the model, and stick them together with other pieces that can serve as the implied part.\n\nThe observed part is for observed data, the implied part is what the model implies about your data (e.g. the model implied covariance matrix), and the loss part compares the observed data and implied properties (e.g. weighted least squares difference between the observed and implied covariance matrix). The optimizer part is not part of the model itself, but it is needed to fit the model as it connects to the optimization backend (e.g. the type of optimization algorithm used).\n\nFor example, to build a model for maximum likelihood estimation with the NLopt optimization suite as a backend you would choose SemML as a loss function and SemOptimizerNLopt as the optimizer.\n\nAs you can see, a model can have as many loss functions as you want it to have. We always optimize over their (weighted) sum. So to build a model for ridge regularized full information maximum likelihood estimation, you would choose two loss functions, SemFIML and SemRidge.\n\nIn julia, everything has a type. To make more precise which objects can be used as the different building blocks, we require them to have a certain type:\n\n(Image: SEM concept typed)\n\nSo everything that can be used as the 'observed' part has to be of type SemObserved.\n\nHere is an overview on the available building blocks:\n\nSemObserved SemImplied SemLossFunction SemOptimizer\nSemObservedData RAM SemML SemOptimizerOptim\nSemObservedCovariance RAMSymbolic SemWLS SemOptimizerNLopt\nSemObservedMissing ImpliedEmpty SemFIML \n  SemRidge \n  SemConstant \n\nThe rest of this page explains the building blocks for each part. First, we explain every part and give an overview on the different options that are available. After that, the API - model parts section serves as a reference for detailed explanations about the different options. (How to stick them together to a final model is explained in the section on Model Construction.)","category":"section"},{"location":"tutorials/concept/#The-observed-part-aka-[SemObserved](@ref)","page":"Our Concept of a Structural Equation Model","title":"The observed part aka SemObserved","text":"The observed part contains all necessary information about the observed data. Currently, we have three options: SemObservedData for fully observed datasets, SemObservedCovariance for observed covariances (and means) and SemObservedMissing for data that contains missing values.","category":"section"},{"location":"tutorials/concept/#The-implied-part-aka-[SemImplied](@ref)","page":"Our Concept of a Structural Equation Model","title":"The implied part aka SemImplied","text":"The implied part is what your model implies about the data, for example, the model-implied covariance matrix. There are two options at the moment: RAM, which uses the reticular action model to compute the model implied covariance matrix, and RAMSymbolic which does the same but symbolically pre-computes part of the model, which increases subsequent performance in model fitting (see Symbolic precomputation). There is also a third option, ImpliedEmpty that can serve as a 'placeholder' for models that do not need an implied part.","category":"section"},{"location":"tutorials/concept/#The-loss-part-aka-SemLoss","page":"Our Concept of a Structural Equation Model","title":"The loss part aka SemLoss","text":"The loss part specifies the objective that is optimized to find the parameter estimates. If it contains more then one loss function (aka SemLossFunction)), we find the parameters by minimizing the sum of loss functions (for example in maximum likelihood estimation + ridge regularization). Available loss functions are\n\nSemML: maximum likelihood estimation\nSemWLS: weighted least squares estimation\nSemFIML: full-information maximum likelihood estimation\nSemRidge: ridge regularization","category":"section"},{"location":"tutorials/concept/#The-optimizer-part-aka-SemOptimizer","page":"Our Concept of a Structural Equation Model","title":"The optimizer part aka SemOptimizer","text":"The optimizer part of a model connects to the numerical optimization backend used to fit the model.  It can be used to control options like the optimization algorithm, linesearch, stopping criteria, etc.  There are currently three available backends, SemOptimizerOptim connecting to the Optim.jl backend, SemOptimizerNLopt connecting to the NLopt.jl backend and SemOptimizerProximal connecting to ProximalAlgorithms.jl. For more information about the available options see also the tutorials about Using Optim.jl and Using NLopt.jl, as well as Constrained optimization and Regularization .","category":"section"},{"location":"tutorials/concept/#What-to-do-next","page":"Our Concept of a Structural Equation Model","title":"What to do next","text":"You now have an understanding of our representation of structural equation models.\n\nTo learn more about how to use the package, you may visit the remaining tutorials.\n\nIf you want to learn how to extend the package (e.g., add a new loss function), you may visit Extending the package.","category":"section"},{"location":"tutorials/concept/#API-model-parts","page":"Our Concept of a Structural Equation Model","title":"API - model parts","text":"","category":"section"},{"location":"tutorials/concept/#observed","page":"Our Concept of a Structural Equation Model","title":"observed","text":"","category":"section"},{"location":"tutorials/concept/#implied","page":"Our Concept of a Structural Equation Model","title":"implied","text":"","category":"section"},{"location":"tutorials/concept/#loss-functions","page":"Our Concept of a Structural Equation Model","title":"loss functions","text":"","category":"section"},{"location":"tutorials/concept/#optimizer","page":"Our Concept of a Structural Equation Model","title":"optimizer","text":"","category":"section"},{"location":"tutorials/concept/#StructuralEquationModels.SemObserved","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObserved","text":"Supertype of all objects that can serve as the observed field of a SEM. Pre-processes data and computes sufficient statistics for example. If you have a special kind of data, e.g. ordinal data, you should implement a subtype of SemObserved.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedData","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedData","text":"For observed data without missings.\n\nConstructor\n\nSemObservedData(;\n    data,\n    observed_vars = nothing,\n    specification = nothing,\n    kwargs...)\n\nArguments\n\ndata: observed data – DataFrame or Matrix\nobserved_vars::Vector{Symbol}: column names of the data (if the object passed as data does not have column names, i.e. is not a data frame)\nspecification: optional SEM specification (SemSpecification)\n\nExtended help\n\nInterfaces\n\nnsamples(::SemObservedData) -> number of observed data points\nnobserved_vars(::SemObservedData) -> number of observed (manifested) variables\nsamples(::SemObservedData) -> observed data\nobs_cov(::SemObservedData) -> observed covariance matrix\nobs_mean(::SemObservedData) -> observed mean vector\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedCovariance","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedCovariance","text":"Type alias for SemObservedData that has mean and covariance, but no actual data.\n\nFor instances of SemObservedCovariance samples returns nothing.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedMissing","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedMissing","text":"For observed data with missing values.\n\nConstructor\n\nSemObservedMissing(;\n    data,\n    observed_vars = nothing,\n    specification = nothing,\n    kwargs...)\n\nArguments\n\ndata: observed data\nobserved_vars::Vector{Symbol}: column names of the data (if the object passed as data does not have column names, i.e. is not a data frame)\nspecification: optional SEM model specification (SemSpecification)\n\nExtended help\n\nInterfaces\n\nnsamples(::SemObservedMissing) -> number of samples (data points)\nnobserved_vars(::SemObservedMissing) -> number of observed variables\nsamples(::SemObservedMissing) -> data matrix (contains both measured and missing values)\n\nExpectation maximization\n\nem_mvn!(::SemObservedMissing) can be called to fit a covariance matrix and mean vector to the data using an expectation maximization (EM) algorithm under the assumption of multivariate normality. After, the following methods are available:\n\nem_model(::SemObservedMissing) -> EmMVNModel that contains the covariance matrix and mean vector found via EM\nobs_cov(::SemObservedData) -> EM covariance matrix\nobs_mean(::SemObservedData) -> EM mean vector\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.samples","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.samples","text":"samples(observed::SemObservedData)\n\nGets the matrix of observed data samples. Rows are samples, columns are observed variables.\n\nSee Also\n\nnsamples, observed_vars.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/concept/#StructuralEquationModels.observed_vars","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.observed_vars","text":"observed_vars(semobj) -> Vector{Symbol}\n\nReturn the vector of SEM model observed variable in the order specified by the model, which also should match the order of variables in SemObserved.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/concept/#StructuralEquationModels.SemSpecification","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemSpecification","text":"Base type for all SEM specifications.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemImplied","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemImplied","text":"Supertype of all objects that can serve as the implied field of a SEM. Computes model-implied values that should be compared with the observed data to find parameter estimates, e. g. the model implied covariance or mean. If you would like to implement a different notation, e.g. LISREL, you should implement a subtype of SemImplied.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.RAM","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.RAM","text":"Model implied covariance and means via RAM notation.\n\nConstructor\n\nRAM(;specification,\n    meanstructure = false,\n    gradient = true,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\nmeanstructure::Bool: does the model have a meanstructure?\ngradient::Bool: is gradient-based optimization used\n\nExtended help\n\nRAM notation\n\nThe model implied covariance matrix is computed as\n\n    Sigma = F(I-A)^-1S(I-A)^-TF^T\n\nand for models with a meanstructure, the model implied means are computed as\n\n    mu = F(I-A)^-1M\n\nInterfaces\n\nparam_labels(::RAM)-> vector of parameter labels\nnparams(::RAM) -> number of parameters\nram.Σ -> model implied covariance matrix\nram.μ -> model implied mean vector\n\nRAM matrices for the current parameter values:\n\nram.A\nram.S\nram.F\nram.M\n\nJacobians of RAM matrices w.r.t to the parameter vector θ\n\nram.∇A -> vec(A)θᵀ\nram.∇S -> vec(S)θᵀ\nram.∇M = Mθᵀ\n\nVector of indices of each parameter in the respective RAM matrix:\n\nram.A_indices\nram.S_indices\nram.M_indices\n\nAdditional interfaces\n\nram.F⨉I_A⁻¹ -> F(I-A)^-1\nram.F⨉I_A⁻¹S -> F(I-A)^-1S\nram.I_A -> I-A\n\nOnly available in gradient! calls:\n\nram.I_A⁻¹ -> (I-A)^-1\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.RAMSymbolic","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.RAMSymbolic","text":"Subtype of SemImplied that implements the RAM notation with symbolic precomputation.\n\nConstructor\n\nRAMSymbolic(;\n    specification,\n    vech = false,\n    gradient = true,\n    hessian = false,\n    approximate_hessian = false,\n    meanstructure = false,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\nmeanstructure::Bool: does the model have a meanstructure?\ngradient::Bool: is gradient-based optimization used\nhessian::Bool: is hessian-based optimization used\napproximate_hessian::Bool: for hessian based optimization: should the hessian be approximated\nvech::Bool: should the half-vectorization of Σ be computed (instead of the full matrix)   (automatically set to true if any of the loss functions is SemWLS)\n\nExtended help\n\nInterfaces\n\nparam_labels(::RAMSymbolic)-> vector of parameter ids\nnparams(::RAMSymbolic) -> number of parameters\nram.Σ -> model implied covariance matrix\nram.μ -> model implied mean vector\n\nJacobians (only available in gradient! calls)\n\nram.∇Σ -> vec(Σ)θᵀ\nram.∇μ -> μθᵀ\nram.∇Σ_function -> function to overwrite ∇Σ in place,   i.e. ∇Σ_function(∇Σ, θ). Typically, you do not want to use this but simply   query ram.∇Σ.\n\nHessians The computation of hessians is more involved. Therefore, we desribe it in the online documentation,  and the respective interfaces are omitted here.\n\nRAM notation\n\nThe model implied covariance matrix is computed as\n\n    Sigma = F(I-A)^-1S(I-A)^-TF^T\n\nand for models with a meanstructure, the model implied means are computed as\n\n    mu = F(I-A)^-1M\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.ImpliedEmpty","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.ImpliedEmpty","text":"Empty placeholder for models that don't need an implied part. (For example, models that only regularize parameters.)\n\nConstructor\n\nImpliedEmpty(;specification, kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\n\nExamples\n\nA multigroup model with ridge regularization could be specified as a SemEnsemble with one model per group and an additional model with ImpliedEmpty and SemRidge for the regularization part.\n\nExtended help\n\nInterfaces\n\nparam_labels(::ImpliedEmpty)-> Vector of parameter labels\nnparams(::ImpliedEmpty) -> Number of parameters\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemLoss","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemLoss","text":"SemLoss(args...; loss_weights = nothing, ...)\n\nConstructs the loss field of a SEM. Can contain multiple SemLossFunctions, the model is optimized over their sum. See also SemLossFunction.\n\nArguments\n\nargs...: Multiple SemLossFunctions.\nloss_weights::Vector: Weights for each loss function. Defaults to unweighted optimization.\n\nExamples\n\nmy_ml_loss = SemML(...)\nmy_ridge_loss = SemRidge(...)\nmy_loss = SemLoss(SemML, SemRidge; loss_weights = [1.0, 2.0])\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemLossFunction","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemLossFunction","text":"Supertype for all loss functions of SEMs. If you want to implement a custom loss function, it should be a subtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemML","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemML","text":"Maximum likelihood estimation.\n\nConstructor\n\nSemML(;observed, meanstructure = false, approximate_hessian = false, kwargs...)\n\nArguments\n\nobserved::SemObserved: the observed part of the model\nmeanstructure::Bool: does the model have a meanstructure?\napproximate_hessian::Bool: if hessian-based optimization is used, should the hessian be swapped for an approximation\n\nExamples\n\nmy_ml = SemML(observed = my_observed)\n\nInterfaces\n\nAnalytic gradients are available, and for models without a meanstructure and RAMSymbolic implied type, also analytic hessians.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemFIML","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemFIML","text":"Full information maximum likelihood estimation. Can handle observed data with missings.\n\nConstructor\n\nSemFIML(;observed, specification, kwargs...)\n\nArguments\n\nobserved::SemObservedMissing: the observed part of the model\nspecification: either a RAMMatrices or ParameterTable object\n\nExamples\n\nmy_fiml = SemFIML(observed = my_observed, specification = my_parameter_table)\n\nInterfaces\n\nAnalytic gradients are available.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemWLS","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemWLS","text":"Weighted least squares estimation. At the moment only available with the RAMSymbolic implied type.\n\nConstructor\n\nSemWLS(;\n    observed,\n    meanstructure = false,\n    wls_weight_matrix = nothing,\n    wls_weight_matrix_mean = nothing,\n    approximate_hessian = false,\n    kwargs...)\n\nArguments\n\nobserved: the SemObserved part of the model\nmeanstructure::Bool: does the model have a meanstructure?\napproximate_hessian::Bool: should the hessian be swapped for an approximation\nwls_weight_matrix: the weight matrix for weighted least squares.   Defaults to GLS estimation (05*(D^T*kron(SS)*D) where D is the duplication matrix   and S is the inverse of the observed covariance matrix)\nwls_weight_matrix_mean: the weight matrix for the mean part of weighted least squares.   Defaults to GLS estimation (the inverse of the observed covariance matrix)\n\nExamples\n\nmy_wls = SemWLS(observed = my_observed)\n\nInterfaces\n\nAnalytic gradients are available, and for models without a meanstructure also analytic hessians.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemRidge","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemRidge","text":"Ridge regularization.\n\nConstructor\n\nSemRidge(;α_ridge, which_ridge, nparams, parameter_type = Float64, implied = nothing, kwargs...)\n\nArguments\n\nα_ridge: hyperparameter for penalty term\nwhich_ridge::Vector: Vector of parameter labels (Symbols) or indices that indicate which parameters should be regularized.\nnparams::Int: number of parameters of the model\nimplied::SemImplied: implied part of the model\nparameter_type: type of the parameters\n\nExamples\n\nmy_ridge = SemRidge(;α_ridge = 0.02, which_ridge = [:λ₁, :λ₂, :ω₂₃], nparams = 30, implied = my_implied)\n\nInterfaces\n\nAnalytic gradients and hessians are available.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemConstant","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemConstant","text":"Constant loss term. Can be used for comparability to other packages.\n\nConstructor\n\nSemConstant(;constant_loss, kwargs...)\n\nArguments\n\nconstant_loss::Number: constant to add to the objective\n\nExamples\n\n    my_constant = SemConstant(constant_loss = 42.0)\n\nInterfaces\n\nAnalytic gradients and hessians are available.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizer","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizer","text":"Supertype of all objects that can serve as the optimizer field of a SEM. Connects the SEM to its optimization backend and controls options like the optimization algorithm. If you want to connect the SEM package to a new optimization backend, you should implement a subtype of SemOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizerOptim","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizerOptim","text":"SemOptimizerOptim{A, B} <: SemOptimizer{:Optim}\n\nConnects to Optim.jl as the optimization backend.\n\nConstructor\n\nSemOptimizerOptim(;\n    algorithm = LBFGS(),\n    options = Optim.Options(;f_reltol = 1e-10, x_abstol = 1.5e-8),\n    kwargs...)\n\nArguments\n\nalgorithm: optimization algorithm from Optim.jl\noptions::Optim.Options: options for the optimization algorithm\n\nUsage\n\nAll algorithms and options from the Optim.jl library are available, for more information see the Optim.jl online documentation.\n\nExamples\n\nmy_optimizer = SemOptimizerOptim()\n\n# hessian based optimization with backtracking linesearch and modified initial step size\nusing Optim, LineSearches\n\nmy_newton_optimizer = SemOptimizerOptim(\n    algorithm = Newton(\n        ;linesearch = BackTracking(order=3),\n        alphaguess = InitialHagerZhang()\n    )\n)\n\nExtended help\n\nConstrained optimization\n\nWhen using the Fminbox or SAMIN constrained optimization algorithms, the vector or dictionary of lower and upper bounds for each model parameter can be specified via lower_bounds and upper_bounds keyword arguments. Alternatively, the lower_bound and upper_bound keyword arguments can be used to specify the default bound for all non-variance model parameters, and the variance_lower_bound and variance_upper_bound keyword – for the variance parameters (the diagonal of the S matrix).\n\nInterfaces\n\nalgorithm(::SemOptimizerOptim)\noptions(::SemOptimizerOptim)\n\nImplementation\n\nSubtype of SemOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizerNLopt","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizerNLopt","text":"Connects to NLopt.jl as the optimization backend. Only usable if NLopt.jl is loaded in the current Julia session!\n\nConstructor\n\nSemOptimizerNLopt(;\n    algorithm = :LD_LBFGS,\n    options = Dict{Symbol, Any}(),\n    local_algorithm = nothing,\n    local_options = Dict{Symbol, Any}(),\n    equality_constraints = Vector{NLoptConstraint}(),\n    inequality_constraints = Vector{NLoptConstraint}(),\n    kwargs...)\n\nArguments\n\nalgorithm: optimization algorithm.\noptions::Dict{Symbol, Any}: options for the optimization algorithm\nlocal_algorithm: local optimization algorithm\nlocal_options::Dict{Symbol, Any}: options for the local optimization algorithm\nequality_constraints::Vector{NLoptConstraint}: vector of equality constraints\ninequality_constraints::Vector{NLoptConstraint}: vector of inequality constraints\n\nExample\n\nmy_optimizer = SemOptimizerNLopt()\n\n# constrained optimization with augmented lagrangian\nmy_constrained_optimizer = SemOptimizerNLopt(;\n    algorithm = :AUGLAG,\n    local_algorithm = :LD_LBFGS,\n    local_options = Dict(:ftol_rel => 1e-6),\n    inequality_constraints = NLoptConstraint(;f = my_constraint, tol = 0.0),\n)\n\nUsage\n\nAll algorithms and options from the NLopt library are available, for more information see the NLopt.jl package and the NLopt online documentation. For information on how to use inequality and equality constraints, see Constrained optimization in our online documentation.\n\nExtended help\n\nInterfaces\n\nalgorithm(::SemOptimizerNLopt)\nlocal_algorithm(::SemOptimizerNLopt)\noptions(::SemOptimizerNLopt)\nlocal_options(::SemOptimizerNLopt)\nequality_constraints(::SemOptimizerNLopt)\ninequality_constraints(::SemOptimizerNLopt)\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizerProximal","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizerProximal","text":"Connects to ProximalAlgorithms.jl as the optimization backend. Can be used for regularized SEM, for a tutorial see the online docs on Regularization.\n\nConstructor\n\nSemOptimizerProximal(;\n    algorithm = ProximalAlgorithms.PANOC(),\n    operator_g,\n    operator_h = nothing,\n    kwargs...,\n\nArguments\n\nalgorithm: optimization algorithm.\noperator_g: proximal operator (e.g., regularization penalty)\noperator_h: optional second proximal operator\n\nUsage\n\nAll algorithms and operators from ProximalAlgorithms.jl are available, for more information see the online docs on Regularization and the documentation of ProximalAlgorithms.jl / ProximalOperators.jl.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/backends/optim/#Using-Optim.jl","page":"Using Optim.jl","title":"Using Optim.jl","text":"SemOptimizerOptim implements the connection to Optim.jl. It takes two arguments, algorithm and options. The defaults are LBFGS as the optimization algorithm and the standard options from Optim.jl. We can load the Optim and LineSearches packages to choose something different:\n\nusing Optim, LineSearches\n\nmy_optimizer = SemOptimizerOptim(\n    algorithm = BFGS(\n        linesearch = BackTracking(order=3), \n        alphaguess = InitialHagerZhang()\n        ),\n    options = Optim.Options(show_trace = true) \n    )\n\nThis optimizer will use BFGS (!not L-BFGS) with a back tracking linesearch and a certain initial step length guess. Also, the trace of the optimization will be printed to the console.\n\nTo see how to use the optimizer to actually fit a model now, check out the Model fitting section.\n\nFor a list of all available algorithms and options, we refer to this page of the Optim.jl manual.","category":"section"},{"location":"developer/sem/#Custom-model-types","page":"Custom model types","title":"Custom model types","text":"The abstract supertype for all models is AbstractSem, which has two subtypes, AbstractSemSingle{O, I, L} and AbstractSemCollection. Currently, there are 2 subtypes of AbstractSemSingle: Sem, SemFiniteDiff. All subtypes of AbstractSemSingle should have at least observed, implied, loss and optimizer fields, and share their types ({O, I, L}) with the parametric abstract supertype. For example, the SemFiniteDiff type is implemented as\n\nstruct SemFiniteDiff{O <: SemObserved, I <: SemImplied, L <: SemLoss} <:\n       AbstractSemSingle{O, I, L}\n    observed::O\n    implied::I\n    loss::L\nend\n\nAdditionally, you can change how objective/gradient/hessian values are computed by providing methods for evaluate!, e.g. from SemFiniteDiff's implementation:\n\nevaluate!(objective, gradient, hessian, model::SemFiniteDiff, params) = ...\n\nAdditionally, we can define constructors like the one in \"src/frontend/specification/Sem.jl\".\n\nIt is also possible to add new subtypes for AbstractSemCollection.","category":"section"},{"location":"developer/optimizer/#Custom-optimizer-types","page":"Custom optimizer types","title":"Custom optimizer types","text":"The optimizer part of a model connects it to the optimization backend.  Let's say we want to implement a new optimizer as SemOptimizerName. The first part of the implementation is very similar to loss functions, so we just show the implementation of SemOptimizerOptim here as a reference:\n\n############################################################################################\n### Types and Constructor\n############################################################################################\nmutable struct SemOptimizerName{A, B} <: SemOptimizer{:Name}\n    algorithm::A\n    options::B\nend\n\nSemOptimizer{:Name}(args...; kwargs...) = SemOptimizerName(args...; kwargs...)\n\nSemOptimizerName(;\n    algorithm = LBFGS(),\n    options = Optim.Options(; f_reltol = 1e-10, x_abstol = 1.5e-8),\n    kwargs...,\n) = SemOptimizerName(algorithm, options)\n\n############################################################################################\n### Recommended methods\n############################################################################################\n\nupdate_observed(optimizer::SemOptimizerName, observed::SemObserved; kwargs...) = optimizer\n\n############################################################################################\n### additional methods\n############################################################################################\n\nalgorithm(optimizer::SemOptimizerName) = optimizer.algorithm\noptions(optimizer::SemOptimizerName) = optimizer.options\n\nNote that your optimizer is a subtype of SemOptimizer{:Name}, where you can choose a :Name that can later be used as a keyword argument to fit(engine = :Name). Similarly, SemOptimizer{:Name}(args...; kwargs...) = SemOptimizerName(args...; kwargs...) should be defined as well as a constructor that uses only keyword arguments:\n\nSemOptimizerName(;\n    algorithm = LBFGS(),\n    options = Optim.Options(; f_reltol = 1e-10, x_abstol = 1.5e-8),\n    kwargs...,\n) = SemOptimizerName(algorithm, options)\n\nA method for update_observed and additional methods might be usefull, but are not necessary.\n\nNow comes the substantive part: We need to provide a method for fit:\n\nfunction fit(\n    optim::SemOptimizerName,\n    model::AbstractSem,\n    start_params::AbstractVector;\n    kwargs...,\n)\n    optimization_result = ...\n\n    ...\n\n    return SemFit(minimum, minimizer, start_params, model, optimization_result)\nend\n\nThe method has to return a SemFit object that consists of the minimum of the objective at the solution, the minimizer (aka parameter estimates), the starting values, the model and the optimization result (which may be anything you desire for your specific backend).\n\nIn addition, you might want to provide methods to access properties of your optimization result:\n\noptimizer(res::MyOptimizationResult) = ...\nn_iterations(res::MyOptimizationResult) = ...\nconvergence(res::MyOptimizationResult) = ...","category":"section"},{"location":"tutorials/collection/multigroup/#Multigroup-models","page":"Multigroup models","title":"Multigroup models","text":"As an example, we will fit the model from the lavaan tutorial with loadings constrained to equality across groups.\n\nWe first load the example data.  We have to make sure that the column indicating the group (here called school) is a vector of Symbols, not strings - so we convert it.\n\ndat = example_data(\"holzinger_swineford\")\ndat.school = ifelse.(dat.school .== \"Pasteur\", :Pasteur, :Grant_White)\n\nWe then specify our model via the graph interface:\n\nlatent_vars = [:visual, :textual, :speed]\nobserved_vars = Symbol.(:x, 1:9)\n\ngraph = @StenoGraph begin\n    # measurement model\n    visual  → fixed(1, 1)*x1 + label(:λ₂, :λ₂)*x2 + label(:λ₃, :λ₃)*x3\n    textual → fixed(1, 1)*x4 + label(:λ₅, :λ₅)*x5 + label(:λ₆, :λ₆)*x6\n    speed   → fixed(1, 1)*x7 + label(:λ₈, :λ₈)*x8 + label(:λ₉, :λ₉)*x9\n    # variances and covariances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars)   ⇔ _(latent_vars)\nend\n\nYou can pass multiple arguments to fix() and label() for each group. Parameters with the same label (within and across groups) are constrained to be equal. To fix a parameter in one group, but estimate it freely in the other, you may write fix(NaN, 4.3).\n\nYou can then use the resulting graph to specify an EnsembleParameterTable\n\ngroups = [:Pasteur, :Grant_White]\n\npartable = EnsembleParameterTable(\n    graph, \n    observed_vars = observed_vars,\n    latent_vars = latent_vars,\n    groups = groups)\n\nThe parameter table can be used to create a SemEnsemble model:\n\nmodel_ml_multigroup = SemEnsemble(\n    specification = partable,\n    data = dat,\n    column = :school,\n    groups = groups)\n\nnote: A different way to specify\nInstead of choosing the workflow \"Graph -> EnsembleParameterTable -> model\", you may also directly specify RAMMatrices for each group (for an example see this test).\n\nWe now fit the model and inspect the parameter estimates:\n\nsem_fit = fit(model_ml_multigroup)\nupdate_estimate!(partable, sem_fit)\ndetails(partable)\n\nOther things you can query about your fitted model (fit measures, standard errors, etc.) are described in the section Model inspection and work the same way for multigroup models.","category":"section"},{"location":"#A-fast-and-flexible-SEM-framework","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"StructuralEquationModels.jl is a package for Structural Equation Modeling (SEM) still under active development. It is written for one purpose: Facilitating methodological innovations for SEM. This purpose implies two subgoals for the package: Easy extensibility and speed. You can easily define custom objective functions and other parts of the model. At the same time, it is (very) fast. These properties enable SEM researchers (such as you!) to play around with ideas (extensibility) and run extensive simulations (speed) to evaluate these ideas and users to profit from the resulting innovation.","category":"section"},{"location":"#Get-Started","page":"A fast and flexible SEM framework","title":"Get Started","text":"To get started, we recommend the following order:\n\ninstall the package (Installation),\nread A first model, and\nget familiar with Our Concept of a Structural Equation Model.\n\nAfter that, if you are interested in specifying your own loss function (or other parts), you can proceed with Extending the package.","category":"section"},{"location":"#Target-Group","page":"A fast and flexible SEM framework","title":"Target Group","text":"You may consider using this package if you need extensibility and/or speed, e.g.\n\nyou want to extend SEM (e.g. add a new objective function)\nyou want to extend SEM, and your implementation needs to be fast\nyou want to fit the same model(s) to many datasets (bootstrapping, simulation studies)\nyou are planning a study and would like to do power simulations\n\nFor examples of how to use the package, see the Tutorials.","category":"section"},{"location":"#Batteries-Included","page":"A fast and flexible SEM framework","title":"Batteries Included","text":"Models you can fit out of the box include\n\nLinear SEM that can be specified in RAM notation\nML, GLS and FIML estimation\nRidge/Lasso/... Regularization\nMultigroup SEM\nSums of arbitrary loss functions (everything the optimizer can handle)\n\nWe provide fast objective functions, gradients, and for some cases hessians as well as approximations thereof. As a user, you can easily define custom loss functions. For those, you can decide to provide analytical gradients or use finite difference approximation / automatic differentiation. You can choose to mix loss functions natively found in this package and those you provide. In such cases, you optimize over a sum of different objectives (e.g. ML + Ridge). This strategy also applies to gradients, where you may supply analytic gradients or opt for automatic differentiation or mixed analytical and automatic differentiation.","category":"section"},{"location":"#Installation","page":"A fast and flexible SEM framework","title":"Installation","text":"You must have julia installed (and we strongly recommend using an IDE of your choice; we like VS Code with the Julia extension).\n\nTo install the latest version of our package, use the following commands:\n\njulia> ]\npkg> add StructuralEquationModels","category":"section"},{"location":"#Citing-the-package","page":"A fast and flexible SEM framework","title":"Citing the package","text":"To cite our package, go to the GitHub repostory and click on \"Cite this repostiory\" on the right side or see the CSL file.","category":"section"},{"location":"internals/types/#Type-hierarchy","page":"types","title":"Type hierarchy","text":"The type hierarchy is implemented in \"src/types.jl\".\n\nAbstractSem: the most abstract type in our package\n\nAbstractSemSingle{O, I, L} <: AbstractSem is an abstract parametric type that is a supertype of all single models\nSem: models that do not need automatic differentiation or finite difference approximation\nSemFiniteDiff: models whose gradients and/or hessians should be computed via finite difference approximation\nAbstractSemCollection <: AbstractSem is an abstract supertype of all models that contain multiple AbstractSem submodels\n\nEvery AbstractSemSingle has to have SemObserved, SemImplied, and SemLoss fields (and can have additional fields).\n\nSemLoss is a container for multiple SemLossFunctions.","category":"section"},{"location":"complementary/maths/","page":"-","title":"-","text":"This page is still empty.","category":"section"}]
}
