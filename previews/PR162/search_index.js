var documenterSearchIndex = {"docs":
[{"location":"maths/hessians/#Hessians-and-symbolic-precomputation","page":"Hessians and symbolic precomputation","title":"Hessians and symbolic precomputation","text":"","category":"section"},{"location":"maths/hessians/","page":"Hessians and symbolic precomputation","title":"Hessians and symbolic precomputation","text":"∇²Σ(::RAMSymbolic) -> pre-allocated array for vec(Σ)θᵀ\n∇²Σ_function(::RAMSymbolic) -> function to overwrite ∇²Σ in place","category":"page"},{"location":"performance/sorting/#Model-sorting","page":"Model sorting","title":"Model sorting","text":"","category":"section"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"In RAM notation, the model implied covariance matrix is computed as","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"Sigma = F(I-A)^-1S(I-A)^-TF^T","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"If the model is acyclic, the observed and latent variables can be reordered such that (I-A) is lower triangular. This has the computational benefit that the inversion of lower triangular matrices can be carried out by specialized algorithms.","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"To automatically reorder your variables in a way that makes this optimization possible, we provide a sort! method that can be applied to ParameterTable objects to sort the observed and latent variables from the most exogenous ones to the most endogenous.","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"We use it as","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"sort!(parameter_table)\n\nmodel = Sem(\n    specification = parameter_table,\n    ...\n)","category":"page"},{"location":"performance/sorting/","page":"Model sorting","title":"Model sorting","text":"Models specified from sorted parameter tables will make use of the described optimizations.","category":"page"},{"location":"internals/internals/#Internals-and-Design","page":"Internals and design","title":"Internals and Design","text":"","category":"section"},{"location":"internals/internals/","page":"Internals and design","title":"Internals and design","text":"On the following pages, we document the internals and design of the package. Those informations are no prerequisite for extending the package (as decribed in the developer documentation)!, but they may be useful and hopefully interesting.","category":"page"},{"location":"tutorials/meanstructure/#Models-with-mean-structures","page":"Mean Structures","title":"Models with mean structures","text":"","category":"section"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"To make use of mean structures in your model, you have to","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"Specify your model with a mean structure. The sections Graph interface and RAMMatrices interface both explain how this works.\nBuild your model with a meanstructure. We explain how that works in the following.","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"Lets say you specified A first model as a graph with a meanstructure:","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"using StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    dem60 ← ind60\n    dem65 ← dem60\n    dem65 ← ind60\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\n    # means\n    Symbol(\"1\") → _(observed_vars)\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"using StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    dem60 ← ind60\n    dem65 ← dem60\n    dem65 ← ind60\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\n    # means\n    Symbol(\"1\") → _(observed_vars)\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"that is, all observed variable means are estimated freely.","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"To build the model with a meanstructure, we proceed as usual, but pass the argument meanstructure = true. For our example,","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"data = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    meanstructure = true\n)\n\nsem_fit(model)","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"If we build the model by parts, we have to pass the meanstructure = true argument to every part that requires it (when in doubt, simply comsult the documentation for the respective part).","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"For our example,","category":"page"},{"location":"tutorials/meanstructure/","page":"Mean Structures","title":"Mean Structures","text":"observed = SemObservedData(specification = partable, data = data, meanstructure = true)\n\nimply_ram = RAM(specification = partable, meanstructure = true)\n\nml = SemML(observed = observed, meanstructure = true)\n\nmodel = Sem(observed, imply_ram, SemLoss(ml), SemOptimizerOptim())\n\nsem_fit(model)","category":"page"},{"location":"developer/extending/#Extending-the-package","page":"Extending the package","title":"Extending the package","text":"","category":"section"},{"location":"developer/extending/","page":"Extending the package","title":"Extending the package","text":"As discussed in the section on Model Construction, every Structural Equation Model (Sem) consists of four parts:","category":"page"},{"location":"developer/extending/","page":"Extending the package","title":"Extending the package","text":"(Image: SEM concept typed)","category":"page"},{"location":"developer/extending/","page":"Extending the package","title":"Extending the package","text":"On the following pages, we will explain how you can define your own custom parts and \"plug them in\". There are certain things you have to do to define custom parts and some things you can do to have a more pleasent experience. In general, these requirements fall into the categories","category":"page"},{"location":"developer/extending/","page":"Extending the package","title":"Extending the package","text":"minimal (to use your custom part and fit a Sem with it)\nuse the outer constructor to build a model in a more convenient way\nuse additional functionality like standard errors, fit measures, etc.","category":"page"},{"location":"tutorials/first_model/#A-first-model","page":"A first model","title":"A first model","text":"","category":"section"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"In this tutorial, we will fit an example SEM with our package.  The example we are using is from the lavaan tutorial, so it may be familiar. It looks like this:","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"(Image: Visualization of the Political Democracy model)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"We assume the StructuralEquationModels package is already installed. To use it in the current session, we run","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"using StructuralEquationModels","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"We then first define the graph of our model in a syntax which is similar to the R-package lavaan:","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"observed_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"observed_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"note: Time to first model\nWhen executing the code from this tutorial the first time in a fresh julia session, you may wonder that it takes quite some time. This is not because the implementation is slow, but because the functions are compiled the first time you use them. Try rerunning the example a second time - you will see that all function executions after the first one are quite fast.","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"We then use this graph to define a ParameterTable object","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"partable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"load the example data","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"data = example_data(\"political_democracy\")","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"data = example_data(\"political_democracy\")","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"and specify our model as","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"model = Sem(\n    specification = partable,\n    data = data\n)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"We can now fit the model via","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"model_fit = sem_fit(model)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"and compute fit measures as","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"fit_measures(model_fit)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"We can also get a bit more information about the fitted model via the sem_summary() function:","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"sem_summary(model_fit)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"To investigate the parameter estimates, we can update our partable object to contain the new estimates:","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"update_estimate!(partable, model_fit)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"and investigate the solution with","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"sem_summary(partable)","category":"page"},{"location":"tutorials/first_model/","page":"A first model","title":"A first model","text":"Congratulations, you fitted and inspected your very first model!  We recommend continuing with Our Concept of a Structural Equation Model.","category":"page"},{"location":"tutorials/constraints/constraints/#Constrained-optimization","page":"Constraints","title":"Constrained optimization","text":"","category":"section"},{"location":"tutorials/constraints/constraints/#Using-the-NLopt-backend","page":"Constraints","title":"Using the NLopt backend","text":"","category":"section"},{"location":"tutorials/constraints/constraints/#Define-an-example-model","page":"Constraints","title":"Define an example model","text":"","category":"section"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"Let's revisit our model from A first model:","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"using StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\nmodel_fit = sem_fit(model)\n\nupdate_estimate!(partable, model_fit)\n\nsem_summary(partable)","category":"page"},{"location":"tutorials/constraints/constraints/#Define-the-constraints","page":"Constraints","title":"Define the constraints","text":"","category":"section"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"Let's introduce some constraints:","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"Equality constraint: The covariances y3 ↔ y7 and y8 ↔ y4 should sum up to 1.\nInequality constraint: The difference between the loadings dem60 → y2 and dem60 → y3 should be smaller than 0.1\nBound constraint: The directed effect from  ind60 → dem65 should be smaller than 0.5","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"(Of course those constaints only serve an illustratory purpose.)","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"We first need to get the indices of the respective parameters that are invoved in the constraints. We can look up their labels in the output above, and retrieve their indices as","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"parameter_indices = get_identifier_indices([:θ_29, :θ_30, :θ_3, :θ_4, :θ_11], model)","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"The bound constraint is easy to specify: Just give a vector of upper or lower bounds that contains the bound for each parameter. In our example, only parameter number 11 has an upper bound, and the number of total parameters is n_par(model) = 31, so we define","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"upper_bounds = fill(Inf, 31)\nupper_bounds[11] = 0.5","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"The equailty and inequality constraints have to be reformulated to be of the form x = 0 or x ≤ 0:","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"y3 ↔ y7 + y8 ↔ y4 - 1 = 0\ndem60 → y2 - dem60 → y3 - 0.1 ≤ 0","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"Now they can be defined as functions of the parameter vector:","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"# θ[29] + θ[30] - 1 = 0.0\nfunction eq_constraint(θ, gradient)\n    if length(gradient) > 0\n        gradient .= 0.0\n        gradient[29] = 1.0\n        gradient[30] = 1.0\n    end\n    return θ[29] + θ[30] - 1\nend\n\n# θ[3] - θ[4] - 0.1 ≤ 0\nfunction ineq_constraint(θ, gradient)\n    if length(gradient) > 0\n        gradient .= 0.0\n        gradient[3] = 1.0\n        gradient[4] = -1.0\n    end\n    θ[3] - θ[4] - 0.1\nend","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"If the algorithm needs gradients at an iteration, it will pass the vector gradient that is of the same size as the parameters. With if length(gradient) > 0 we check if the algorithm needs gradients, and if it does, we fill the gradient vector with the gradients  of the constraint w.r.t. the parameters.","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"In NLopt, vector-valued constraints are also possible, but we refer to the documentation fot that.","category":"page"},{"location":"tutorials/constraints/constraints/#Fit-the-model","page":"Constraints","title":"Fit the model","text":"","category":"section"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"We now have everything together to specify and fit our model. First, we specify our optimizer backend as","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"constrained_optimizer = SemOptimizerNLopt(\n    algorithm = :AUGLAG,\n    options = Dict(:upper_bounds => upper_bounds, :xtol_abs => 1e-4),\n    local_algorithm = :LD_LBFGS,\n    equality_constraints = NLoptConstraint(;f = eq_constraint, tol = 1e-8),\n    inequality_constraints = NLoptConstraint(;f = ineq_constraint, tol = 1e-8),\n)","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"As you see, the equality constraints and inequality constraints are passed as keyword arguments, and the bounds are passed as options for the (outer) optimization algorithm. Additionally, for equality and inequality constraints, a feasibility tolerance can be specified that controls if a solution can be accepted, even if it violates the constraints by a small amount.  Especially for equality constraints, it is recommended to allow for a small positive tolerance. In this example, we set both tolerances to 1e-8.","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"warning: Convergence criteria\nWe have often observed that the default convergence criteria in NLopt lead to non-convergence flags. Indeed, this example does not convergence with default criteria. As you see above, we used a realively liberal absolute tolerance in the optimization parameters of 1e-4. This should not be a problem in most cases, as the sampling variance in (almost all) structural equation models  should lead to uncertainty in the parameter estimates that are orders of magnitude larger. We nontheless recommend choosing a convergence criterion with care (i.e. w.r.t. the scale of your parameters), inspecting the solutions for plausibility, and comparing them to unconstrained solutions.","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"model_constrained = Sem(\n    specification = partable,\n    data = data,\n    optimizer = constrained_optimizer\n)\n\nmodel_fit_constrained = sem_fit(model_constrained)","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"As you can see, the optimizer converged (:XTOL_REACHED) and investigating the solution yields","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"update_partable!(\n    partable, \n    model_fit_constrained, \n    solution(model_fit_constrained), \n    :estimate_constr)\n\nsem_summary(partable)","category":"page"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"As we can see, the constrained solution is very close to the original solution (compare the columns estimate and estimate_constr), with the difference that the constrained parameters fulfill their constraints.  As all parameters are estimated simultaneously, it is expexted that some unconstrained parameters are also affected (e.g., the constraint on dem60 → y2 leads to a higher estimate of the residual variance y2 ↔ y2).","category":"page"},{"location":"tutorials/constraints/constraints/#Using-the-Optim.jl-backend","page":"Constraints","title":"Using the Optim.jl backend","text":"","category":"section"},{"location":"tutorials/constraints/constraints/","page":"Constraints","title":"Constraints","text":"Information about constrained optimization using Optim.jl can be found in the packages documentation.","category":"page"},{"location":"performance/starting_values/#Starting-values","page":"Starting values","title":"Starting values","text":"","category":"section"},{"location":"performance/starting_values/","page":"Starting values","title":"Starting values","text":"The sem_fit function has a keyword argument that takes either a vector of starting values or a function that takes a model as input to compute starting values. Current options are start_fabin3 for fabin 3 starting values [Hägglund82] or start_simple for simple starting values. Additional keyword arguments to sem_fit are passed to the starting value function. For example,","category":"page"},{"location":"performance/starting_values/","page":"Starting values","title":"Starting values","text":"    sem_fit(\n        model; \n        start_val = start_simple,\n        start_covariances_latent = 0.5\n    )","category":"page"},{"location":"performance/starting_values/","page":"Starting values","title":"Starting values","text":"uses simple starting values with 0.5 as a starting value for covariances between latent variables.","category":"page"},{"location":"performance/starting_values/","page":"Starting values","title":"Starting values","text":"[Hägglund82]: Hägglund, G. (1982). Factor analysis by instrumental variables methods. Psychometrika, 47(2), 209-222.","category":"page"},{"location":"tutorials/specification/parameter_table/#ParameterTable-interface","page":"ParameterTable interface","title":"ParameterTable interface","text":"","category":"section"},{"location":"tutorials/specification/parameter_table/","page":"ParameterTable interface","title":"ParameterTable interface","text":"Altough you can directly specify a parameter table, this is kind of tedious, so at the moment, we dont have a tutorial for this. As lavaan also uses parameter tables to store model specifications, we are working on a way to convert lavaan parameter tables to StructuralEquationModels.jl parameter tables, but this is still WIP.","category":"page"},{"location":"tutorials/specification/parameter_table/#Convert-from-and-to-RAMMatrices","page":"ParameterTable interface","title":"Convert from and to RAMMatrices","text":"","category":"section"},{"location":"tutorials/specification/parameter_table/","page":"ParameterTable interface","title":"ParameterTable interface","text":"To convert a RAMMatrices object to a ParameterTable, simply use partable = ParameterTable(rammatrices).  To convert an object of type ParameterTable to RAMMatrices, you can use ram_matrices = RAMMatrices(partable).","category":"page"},{"location":"performance/symbolic/#Symbolic-precomputation","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"","category":"section"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"In RAM notation, the model implied covariance matrix is computed as","category":"page"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"Sigma = F(I-A)^-1S(I-A)^-TF^T","category":"page"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"If the model is acyclic, we can compute","category":"page"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"(I-A)^-1 = sum_k = 0^n A^k","category":"page"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"for some n  infty. Typically, the S and A matrices are sparse. In our package, we offer symbolic precomputation of Sigma, nablaSigma and even nabla^2Sigma for acyclic models to optimally exploit this sparsity. To use this feature, simply use the RAMSymbolic imply type for your model.","category":"page"},{"location":"performance/symbolic/","page":"Symbolic precomputation","title":"Symbolic precomputation","text":"This can decrase model fitting time, but will also increase model building time (as we have to carry out the symbolic computations and compile specialised functions). As a result, this is probably not beneficial to use if you only fit a single model, but can lead to great improvements if you fit the same modle to multiple datasets (e.g. to compute bootstrap standard errors).","category":"page"},{"location":"tutorials/specification/ram_matrices/#RAMMatrices-interface","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"","category":"section"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Models can also be specified by an object of type RAMMatrices.  The RAM (reticular action model) specification corresponds to three matrices; the A matrix containing all directed parameters, the S matrix containing all undirected parameters, and the F matrix filtering out latent variables from the model implied covariance.","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"The model implied covariance matrix for the observed variables of a SEM is then computed as","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Sigma = F(I-A)^-1S(I-A)^-TF^T","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"For A first model, the corresponding specification looks like this:","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"\nS =[:θ1   0    0     0     0      0     0     0     0     0     0     0     0     0\n    0     :θ2  0     0     0      0     0     0     0     0     0     0     0     0\n    0     0     :θ3  0     0      0     0     0     0     0     0     0     0     0\n    0     0     0     :θ4  0      0     0     :θ15  0     0     0     0     0     0\n    0     0     0     0     :θ5   0     :θ16  0     :θ17  0     0     0     0     0\n    0     0     0     0     0     :θ6  0      0     0     :θ18  0     0     0     0\n    0     0     0     0     :θ16  0     :θ7   0     0     0     :θ19  0     0     0\n    0     0     0     :θ15 0      0     0     :θ8   0     0     0     0     0     0\n    0     0     0     0     :θ17  0     0     0     :θ9   0     :θ20  0     0     0\n    0     0     0     0     0     :θ18 0      0     0     :θ10  0     0     0     0\n    0     0     0     0     0     0     :θ19  0     :θ20  0     :θ11  0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     :θ12  0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     :θ13  0\n    0     0     0     0     0     0     0     0     0     0     0     0     0     :θ14]\n\nF =[1.0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    0 1 0 0 0 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0 0 0 0 0\n    0 0 0 0 0 1 0 0 0 0 0 0 0 0\n    0 0 0 0 0 0 1 0 0 0 0 0 0 0\n    0 0 0 0 0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 0 0 0 0 1 0 0 0 0 0\n    0 0 0 0 0 0 0 0 0 1 0 0 0 0\n    0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n\nA =[0  0  0  0  0  0  0  0  0  0  0     1.0   0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ21  0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ22  0     0\n    0  0  0  0  0  0  0  0  0  0  0     0     1.0   0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ23  0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ24  0\n    0  0  0  0  0  0  0  0  0  0  0     0     :θ25  0\n    0  0  0  0  0  0  0  0  0  0  0     0     0     1\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ26\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ27\n    0  0  0  0  0  0  0  0  0  0  0     0     0     :θ28\n    0  0  0  0  0  0  0  0  0  0  0     0     0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ29  0     0\n    0  0  0  0  0  0  0  0  0  0  0     :θ30  :θ31  0]\n\nθ = Symbol.(:θ, 1:31)\n\nspec = RAMMatrices(;\n    A = A, \n    S = S, \n    F = F, \n    parameters = θ,\n    colnames = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8, :ind60, :dem60, :dem65]\n)\n\nmodel = Sem(\n    specification = spec,\n    data = example_data(\"political_democracy\")\n)","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Let's look at this step by step:","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"First, we specify the A, S and F-Matrices.  For a free parameter, we write a Symbol like :θ1 (or any other symbol we like) to the corresponding place in the respective matrix, to constrain parameters to be equal we just use the same Symbol in the respective entries.  To fix a parameter (as in the A-Matrix above), we just write down the number we want to fix it to.  All other entries are 0.","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Second, we specify a vector of symbols containing our parameters:","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"θ = Symbol.(:θ, 1:31)","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Third, we construct an object of type RAMMatrices, passing our matrices and parameters, as well as the column names of our matrices.  Those are quite important, as they will be used to rearrange your data to match it to your RAMMatrices specification.","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"spec = RAMMatrices(;\n    A = A, \n    S = S, \n    F = F, \n    parameters = θ,\n    colnames = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8, :ind60, :dem60, :dem65]\n)","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"Finally, we construct a model, passing our RAMMatrices as the specification = ... argument.","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"model = Sem(\n    specification = spec,\n    data = example_data(\"political_democracy\")\n)","category":"page"},{"location":"tutorials/specification/ram_matrices/#Meanstructure","page":"RAMMatrices interface","title":"Meanstructure","text":"","category":"section"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"According to the RAM, model implied mean values of the observed variables are computed as","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"mu = F(I-A)^-1M","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"where M is a vector of mean parameters. To estimate the means of the observed variables in our example (and set the latent means to 0), we would specify the model just as before but add ","category":"page"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"...\n\nM = [:x32; :x33; :x34; :x35; :x36; :x37; :x38; :x39; :x40; :x41; :x42; 0; 0; 0]\n\nθ = Symbol.(:θ, 1:42)\n\nspec = RAMMatrices(;\n    ...,\n    M = M)\n\n...\n","category":"page"},{"location":"tutorials/specification/ram_matrices/#Convert-from-and-to-ParameterTables","page":"RAMMatrices interface","title":"Convert from and to ParameterTables","text":"","category":"section"},{"location":"tutorials/specification/ram_matrices/","page":"RAMMatrices interface","title":"RAMMatrices interface","text":"To convert a RAMMatrices object to a ParameterTable, simply use partable = ParameterTable(ram_matrices).  To convert an object of type ParameterTable to RAMMatrices, you can use ram_matrices = RAMMatrices(partable).","category":"page"},{"location":"internals/files/#Files","page":"files","title":"Files","text":"","category":"section"},{"location":"internals/files/","page":"files","title":"files","text":"We briefly describe the file and folder structure of the package.","category":"page"},{"location":"internals/files/#Source-code","page":"files","title":"Source code","text":"","category":"section"},{"location":"internals/files/","page":"files","title":"files","text":"All source code is in the \"src\" folder:","category":"page"},{"location":"internals/files/","page":"files","title":"files","text":"\"src\"","category":"page"},{"location":"internals/files/","page":"files","title":"files","text":"\"StructuralEquationModels.jl\" defines the module and the exported objects\n\"types.jl\" defines all abstract types and the basic type hierarchy\n\"objective_gradient_hessian.jl\" contains methods for computing objective, gradient and hessian values for different model types as well as generic fallback methods\nThe four folders \"observed\", \"imply\", \"loss\" and \"diff\" contain implementations of specific subtypes (for example, the \"loss\" folder contains a file \"ML.jl\" that implements the SemML loss function).\n\"optimizer\" contains connections to different optimization backends (aka methods for sem_fit)\n\"optim.jl\": connection to the Optim.jl package\n\"NLopt.jl\": connection to the NLopt.jl package\n\"frontend\" contains user-facing functions\n\"specification\" contains functionality for model specification\n\"fit\" contains functionality for model assessment, like fit measures and standard errors\n\"additional_functions\" contains helper functions for simulations, loading artifacts (example data) and various other things","category":"page"},{"location":"internals/files/#Tests-and-Documentation","page":"files","title":"Tests and Documentation","text":"","category":"section"},{"location":"internals/files/","page":"files","title":"files","text":"Tests are in the \"test\" folder, documentation in the \"docs\" folder.","category":"page"},{"location":"tutorials/specification/specification/#Model-specification","page":"Model specification","title":"Model specification","text":"","category":"section"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"Two things can be used to specify a model: a parameter table or ram matrices. You can convert them to each other, and to make your life easier, we also provide a way to get parameter tables from graphs.","category":"page"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"This leads to the following chart:","category":"page"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"(Image: Specification flowchart)","category":"page"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"You can enter model specification at each point, but in general (and especially if you come from lavaan), it is the easiest to follow the red arrows: specify a graph object, convert it to a prameter table, and use this parameter table to construct your models ( just like we did in A first model):","category":"page"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"observed_vars = ...\nlatent_vars   = ...\n\ngraph = @StenoGraph begin\n    ...\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)\n\nmodel = Sem(\n    specification = partable,\n    ...\n)","category":"page"},{"location":"tutorials/specification/specification/","page":"Model specification","title":"Model specification","text":"On the following pages, we explain how to enter the specification process at each step, i.e. how to specify models via the Graph interface, the ParameterTable interface, and the RAMMatrices interface.  If you have an OpenMx background, and are familiar with their way of specifying structural equation models via RAM matrices, the RAMMatrices interface may be of interest for you.","category":"page"},{"location":"performance/parametric/#Parametric-types","page":"Parametric Types","title":"Parametric types","text":"","category":"section"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"Recall that a new composite type in julia can be declared as","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"struct MyNewType\n    field1\n    field2\n    ...\nend","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"Often we can speedup computations by declaring our type as a Parametric Type:","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"struct MyNewType{A, B}\n    field1::A\n    field2::B\n    ...\nend","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"giving each field a type and adding them as parameters to our type declaration.","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"Recall our example from Custom loss functions:","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"struct Ridge <: SemLossFunction\n    α\n    I\nend","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"We could also declare it as a parametric type:","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"struct ParametricRidge{X, Y} <: SemLossFunction\n    α::X\n    I::Y\nend","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"Let's see how this might affect performance:","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"function add_α(ridge1, ridge2)\n    return ridge1.α + ridge2.α \nend\n\nmy_ridge_1 = Ridge(2.5, [2,3])\nmy_ridge_2 = Ridge(25.38, [2,3])\n\nmy_parametric_ridge_1 = ParametricRidge(2.1, [2,3])\nmy_parametric_ridge_2 = ParametricRidge(8.34, [2,3])\n\nusing BenchmarkTools\n\n@benchmark add_α($my_ridge_1, $my_ridge_2)\n\n# output\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  16.073 ns …  1.508 μs  ┊ GC (min … max): 0.00% … 98.35%\n Time  (median):     17.839 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   22.846 ns ± 23.564 ns  ┊ GC (mean ± σ):  1.64% ±  1.70%\n\n@benchmark add_α($my_parametric_ridge_1, $my_parametric_ridge_2)\n\n# output\n\nBenchmarkTools.Trial: 10000 samples with 1000 evaluations.\n Range (min … max):  1.371 ns … 20.250 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     2.097 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   2.169 ns ±  0.829 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n","category":"page"},{"location":"performance/parametric/","page":"Parametric Types","title":"Parametric Types","text":"which is quite a difference. To learn more about parametric types, see the this section in the julia documentation.","category":"page"},{"location":"developer/loss/#Custom-loss-functions","page":"Custom loss functions","title":"Custom loss functions","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"As an example, we will implement ridge regularization. Maximum likelihood estimation with ridge regularization consists of optimizing the objective","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"F_ML(theta) + alpha lVert theta_I rVert^2_2","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Since we allow for the optimization of sums of loss functions, and the maximum likelihood loss function already exists, we only need to implement the ridge part (and additionally get ridge regularization for WLS and FIML estimation for free).","category":"page"},{"location":"developer/loss/#Minimal","page":"Custom loss functions","title":"Minimal","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"using StructuralEquationModels","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"To define a new loss function, you have to define a new type that is a subtype of SemLossFunction:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"struct Ridge <: SemLossFunction\n    α\n    I\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"We store the hyperparameter α and the indices I of the parameters we want to regularize.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Additionaly, we need to define a method to compute the objective:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"import StructuralEquationModels: objective!\n\nobjective!(ridge::Ridge, par, model::AbstractSemSingle) = ridge.α*sum(par[ridge.I].^2)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"That's all we need to make it work! For example, we can now fit A first model with ridge regularization:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"We first give some parameters labels to be able to identify them as targets for the regularization:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"observed_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → label(:a)*dem60\n    dem60 → label(:b)*dem65\n    ind60 → label(:c)*dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars,\n    observed_vars = observed_vars,\n    graph = graph)\n\nparameter_indices  = get_identifier_indices([:a, :b, :c], partable)\nmyridge = Ridge(0.01, parameter_indices)\n\nmodel = SemFiniteDiff(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = (SemML, myridge)\n)\n\nmodel_fit = sem_fit(model)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"This is one way of specifying the model - we now have one model with multiple loss functions. Because we did not provide a gradient for Ridge, we have to specify a SemFiniteDiff model that computes numerical gradients with finite difference approximation.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Note that the last argument to the objective! method is the whole model. Therefore, we can access everything that is stored inside our model everytime we compute the objective value for our loss function. Since ridge regularization is a very easy case, we do not need to do this. But maximum likelihood estimation for example depends on both the observed and the model implied covariance matrix. See Second example - maximum likelihood for information on how to do that.","category":"page"},{"location":"developer/loss/#Improve-performance","page":"Custom loss functions","title":"Improve performance","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"By far the biggest improvements in performance will result from specifying analytical gradients. We can do this for our example:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"import StructuralEquationModels: gradient!\n\nfunction gradient!(ridge::Ridge, par, model::AbstractSemSingle)\n    gradient = zero(par)\n    gradient[ridge.I] .= 2*ridge.α*par[ridge.I]\n    return gradient\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Now, instead of specifying a SemFiniteDiff, we can use the normal Sem constructor:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"model_new = Sem(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = (SemML, myridge)\n)\n\nmodel_fit = sem_fit(model_new)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"The results are the same, but we can verify that the computational costs are way lower (for this, the julia package BenchmarkTools has to be installed):","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"using BenchmarkTools\n\n@benchmark sem_fit(model)\n\n@benchmark sem_fit(model_new)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"The exact results of those benchmarks are of course highly depended an your system (processor, RAM, etc.), but you should see that the median computation time with analytical gradients drops to about 5% of the computation without analytical gradients.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Additionally, you may provide analytic hessians by writing a method of the form","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"function hessian!(ridge::Ridge, par, model::AbstractSemSingle)\n    ...\n    return hessian\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"however, this will only matter if you use an optimization algorithm that makes use of the hessians. Our default algorithmn LBFGS from the package Optim.jl does not use hessians (for example, the Newton algorithmn from the same package does).","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"To improve performance even more, you can write a method of the form","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"function objective_gradient!(ridge::Ridge, par, model::AbstractSemSingle)\n    ...\n    return objective, gradient\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"This is beneficial when the computation of the objective and gradient share common computations. For example, in maximum likelihood estimation, the model implied covariance matrix has to be inverted to both compute the objective and gradient. Whenever the optimization algorithmn asks for the objective value and gradient at the same point, we call objective_gradient! and only have to do the shared computations - in this case the matrix inversion - once.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"If you want to do hessian-based optimization, there are also the following methods:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"function objective_hessian!(ridge::Ridge, par, model::AbstractSemSingle)\n    ...\n    return objective, hessian\nend\n\nfunction gradient_hessian!(ridge::Ridge, par, model::AbstractSemSingle)\n    ...\n    return gradient, hessian\nend\n\nfunction objective_gradient_hessian!(ridge::Ridge, par, model::AbstractSemSingle)\n    ...\n    return objective, gradient, hessian\nend","category":"page"},{"location":"developer/loss/#Convenient","page":"Custom loss functions","title":"Convenient","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"To be able to build the model with the Outer Constructor, you need to add a constructor for your loss function that only takes keyword arguments and allows for passing optional additional kewyword arguments. A constructor is just a function that creates a new instance of your type:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"function MyLoss(;arg1 = ..., arg2, kwargs...)\n    ...\n    return MyLoss(...)\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"All keyword arguments that a user passes to the Sem constructor are passed to your loss function. In addition, all previously constructed parts of the model (imply and observed part) are passed as keyword arguments as well as the number of parameters n_par = ..., so your constructor may depend on those. For example, the constructor for SemML in our package depends on the additional argument meanstructure as well as the observed part of the model to pre-allocate arrays of the same size as the observed covariance matrix and the observed mean vector: ","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"function SemML(;observed, meanstructure = false, approx_H = false, kwargs...)\n\n    isnothing(obs_mean(observed)) ?\n        meandiff = nothing :\n        meandiff = copy(obs_mean(observed))\n\n    return SemML(\n        similar(obs_cov(observed)),\n        similar(obs_cov(observed)),\n        meandiff,\n        approx_H,\n        Val(meanstructure)\n        )\nend","category":"page"},{"location":"developer/loss/#Additional-functionality","page":"Custom loss functions","title":"Additional functionality","text":"","category":"section"},{"location":"developer/loss/#Update-observed-data","page":"Custom loss functions","title":"Update observed data","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"If you are planing a simulation study where you have to fit the same model to many different datasets, it is computationally beneficial to not build the whole model completely new everytime you change your data. Therefore, we provide a function to update the data of your model, swap_observed(model(semfit); data = new_data). However, we can not know beforehand in what way your loss function depends on the specific datasets. The solution is to provide a method for update_observed. Since Ridge does not depend on the data at all, this is quite easy:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"import StructuralEquationModels: update_observed\n\nupdate_observed(ridge::Ridge, observed::SemObserved; kwargs...) = ridge","category":"page"},{"location":"developer/loss/#Access-additional-information","page":"Custom loss functions","title":"Access additional information","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"If you want to provide a way to query information about loss functions of your type, you can provide functions for that:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"hyperparameter(ridge::Ridge) = ridge.α\nregularization_indices(ridge::Ridge) = ridge.I","category":"page"},{"location":"developer/loss/#Second-example-maximum-likelihood","page":"Custom loss functions","title":"Second example - maximum likelihood","text":"","category":"section"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Let's make a sligtly more complicated example: we will reimplement maximum likelihood estimation.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"To keep it simple, we only cover models without a meanstructure. The maximum likelihood objective is defined as","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"F_ML = log det Sigma_i + mathrmtrleft(Sigma_i^-1 Sigma_o right)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"where Sigma_i is the model implied covariance matrix and Sigma_o is the observed covariance matrix. We can query the model implied covariance matrix from the imply par of our model, and the observed covariance matrix from the observed path of our model.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"To get information on what we can access from a certain imply or observed type, we can check it`s documentation an the pages API - model parts or via the help mode of the REPL:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"julia>?\n\nhelp?> RAM\n\nhelp?> SemObservedCommon","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"We see that the model implied covariance matrix can be assessed as Σ(imply) and the observed covariance matrix as obs_cov(observed).","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"With this information, we write can implement maximum likelihood optimization as","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"struct MaximumLikelihood <: SemLossFunction end\n\nusing LinearAlgebra\nimport StructuralEquationModels: Σ, obs_cov, objective!\n\nfunction objective!(semml::MaximumLikelihood, parameters, model::AbstractSem)\n    # access the model implied and observed covariance matrices\n    Σᵢ = Σ(imply(model))\n    Σₒ = obs_cov(observed(model))\n    # compute the objective\n    if isposdef(Symmetric(Σᵢ)) # is the model implied covariance matrix positive definite?\n        return logdet(Σᵢ) + tr(inv(Σᵢ)*Σₒ)\n    else\n        return Inf\n    end\nend","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"to deal with eventual non-positive definiteness of the model implied covariance matrix, we chose the pragmatic way of returning infinity whenever this is the case.","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"Let's specify and fit a model:","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"model_ml = SemFiniteDiff(\n    specification = partable,\n    data = example_data(\"political_democracy\"),\n    loss = MaximumLikelihood()\n)\n\nmodel_fit = sem_fit(model_ml)","category":"page"},{"location":"developer/loss/","page":"Custom loss functions","title":"Custom loss functions","text":"If you want to differentiate your own loss functions via automatic differentiation, check out the AutoDiffSEM package (spoiler allert: it's really easy).","category":"page"},{"location":"developer/observed/#Custom-observed-types","page":"Custom observed types","title":"Custom observed types","text":"","category":"section"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"The implementation of new observed types is very similar to loss functions, so we will just go over it briefly (for additional information, revisit Custom loss functions).","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"First, we need to define a new struct that is a subtype of SemObserved:","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"struct MyObserved <: SemObserved\n    ...\nend","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"Additionally, we can write an outer constructor that will typically depend on the keyword argument data = ...:","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"function MyObserved(;data, kwargs...)\n    ...\n    return MyObserved(...)\nend","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"To compute some fit indices, you need to provide methods for","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"# Number of observed datapoints\nn_obs(observed::MyObserved) = ...\n# Number of manifest variables\nn_man(observed::MyObserved) = ...","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"As always, you can add additional methods for properties that imply types and loss function want to access, for example (from the SemObservedCommon implementation):","category":"page"},{"location":"developer/observed/","page":"Custom observed types","title":"Custom observed types","text":"obs_cov(observed::SemObservedCommon) = observed.obs_cov","category":"page"},{"location":"tutorials/collection/collection/#Collections","page":"Collections","title":"Collections","text":"","category":"section"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"With StructuralEquationModels.jl, you can fit weighted sums of structural equation models.  The most common use case for this are Multigroup models.  Another use case may be optimizing the sum of loss functions for some of which you do know the analytic gradient, but not for others.  In this case, you can optimize the sum of a Sem and a SemFiniteDiff (or any other differentiation method).","category":"page"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"To use this feature, you have to construct a SemEnsemble model, which is actually quite easy:","category":"page"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"# models\nmodel_1 = Sem(...)\n\nmodel_2 = SemFiniteDiff(...)\n\nmodel_3 = Sem(...)\n\nmodel_ensemble = SemEnsemble(model_1, model_2, model_3; optimizer = ...)","category":"page"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"So you just construct the individual models (however you like) and pass them to SemEnsemble. One important thing to note is that the individual optimizer entries of each model do not matter (as you can optimize your ensemble model only with one algorithmn from one optimization suite). Instead, SemEnsemble has its own optimizer part that specifies the backend for the whole ensemble model. You may also pass a vector of weigths to SemEnsemble. By default, those are set to N_modelN_total, i.e. each model is weighted by the number of observations in it's data (which matches the formula for multigroup models).","category":"page"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"Multigroup models can also be specified via the graph interface; for an example, see Multigroup models.","category":"page"},{"location":"tutorials/collection/collection/#API-collections","page":"Collections","title":"API - collections","text":"","category":"section"},{"location":"tutorials/collection/collection/","page":"Collections","title":"Collections","text":"SemEnsemble\nAbstractSemCollection","category":"page"},{"location":"tutorials/collection/collection/#StructuralEquationModels.SemEnsemble","page":"Collections","title":"StructuralEquationModels.SemEnsemble","text":"SemEnsemble(models..., optimizer = SemOptimizerOptim, weights = nothing, kwargs...)\n\nConstructor for ensemble models.\n\nArguments\n\nmodels...: AbstractSems.\noptimizer: object of subtype SemOptimizer or a constructor.\nweights::Vector:  Weights for each model. Defaults to the number of observed data points.\n\nAll additional kwargs are passed down to the constructor for the optimizer field.\n\nReturns a SemEnsemble with fields\n\nn::Int: Number of models.\nsems::Tuple: AbstractSems.\nweights::Vector: Weights for each model.\noptimizer::SemOptimizer: Connects the model to the optimizer. See also SemOptimizer.\nidentifier::Dict: Stores parameter labels and their position.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/collection/collection/#StructuralEquationModels.AbstractSemCollection","page":"Collections","title":"StructuralEquationModels.AbstractSemCollection","text":"Supertype for all collections of multiple SEMs\n\n\n\n\n\n","category":"type"},{"location":"developer/imply/#Custom-imply-types","page":"Custom imply types","title":"Custom imply types","text":"","category":"section"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"We recommend to first read the part Custom loss functions, as the overall implementation is the same and we will describe it here more briefly.","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"Imply types are of subtype SemImply. To implement your own imply type, you should define a struct","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"struct MyImply <: SemImply\n    ...\nend","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"and at least a method to compute the objective","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"import StructuralEquationModels: objective!\n\nfunction objective!(imply::MyImply, par, model::AbstractSemSingle)\n    ...\n    return nothing\nend","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"This method should compute and store things you want to make available to the loss functions, and returns nothing. For example, as we have seen in Second example - maximum likelihood, the RAM imply type computes the model-implied covariance matrix and makes it available via Σ(imply). To make stored computations available to loss functions, simply write a function - for example, for the RAM imply type we defined","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"Σ(imply::RAM) = imply.Σ","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"Additionally, you can specify methods for gradient and hessian as well as the combinations described in Custom loss functions.","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"The last thing nedded to make it work is a method for n_par that takes your imply type and returns the number of parameters of the model:","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"n_par(imply::MyImply) = ...","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"Just as described in Custom loss functions, you may define a constructor. Typically, this will depend on the specification = ... argument that can be a ParameterTable or a RAMMatrices object.","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"We implement an ImplyEmpty type in our package that does nothing but serving as an imply field in case you are using a loss function that does not need any imply type at all. You may use it as a template for defining your own imply type, as it also shows how to handle the specification objects:","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"############################################################################\n### Types\n############################################################################\n\nstruct ImplyEmpty{V, V2} <: SemImply\n    identifier::V2\n    n_par::V\nend\n\n############################################################################\n### Constructors\n############################################################################\n\nfunction ImplyEmpty(;\n        specification,\n        kwargs...)\n\n        ram_matrices = RAMMatrices(specification)\n        identifier = StructuralEquationModels.identifier(ram_matrices)\n\n        n_par = length(ram_matrices.parameters)\n\n        return ImplyEmpty(identifier, n_par)\nend\n\n############################################################################\n### methods\n############################################################################\n\nobjective!(imply::ImplyEmpty, par, model) = nothing\ngradient!(imply::ImplyEmpty, par, model) = nothing\nhessian!(imply::ImplyEmpty, par, model) = nothing\n\n############################################################################\n### Recommended methods\n############################################################################\n\nidentifier(imply::ImplyEmpty) = imply.identifier\nn_par(imply::ImplyEmpty) = imply.n_par\n\nupdate_observed(imply::ImplyEmpty, observed::SemObserved; kwargs...) = imply","category":"page"},{"location":"developer/imply/","page":"Custom imply types","title":"Custom imply types","text":"As you see, similar to Custom loss functions we implement a method for update_observed. Additionally, you should store the identifier from the specification object and write a method for identifier, as this will make it possible to access parameter indices by label.","category":"page"},{"location":"tutorials/construction/construction/#Model-Construction","page":"Model Construction","title":"Model Construction","text":"","category":"section"},{"location":"tutorials/construction/construction/","page":"Model Construction","title":"Model Construction","text":"There are two different ways of constructing a SEM in our package. You can use the Outer Constructor or Build by parts. The final models will be the same, the outer constructor just has some sensible defaults that make your life easier. All tutorials until now used the outer constructor Sem(specification = ..., data = ..., ...), which is normally the more convenient way. However, our package is build for extensibility, so there may be cases where user-defined parts of a model do not work with the outer constructor. Therefore, building the model by parts is always available as a fallback.","category":"page"},{"location":"tutorials/specification/graph_interface/#Graph-interface","page":"Graph interface","title":"Graph interface","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/#Workflow","page":"Graph interface","title":"Workflow","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"As discussed before, when using the graph interface, you can specify your model as a graph","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"graph = @StenoGraph begin\n    ...\nend","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"and convert it to a ParameterTable to construct your models:","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"observed_vars = ...\nlatent_vars   = ...\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)\n\nmodel = Sem(\n    specification = partable,\n    ...\n)","category":"page"},{"location":"tutorials/specification/graph_interface/#Parameters","page":"Graph interface","title":"Parameters","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"In general, there are two different types of parameters: directed and indirected parameters. A directed parameter from the variable x to y can be specified as x → y (or equivalently as y ← x); an undirected parameter as x ↔ y. We allow multiple variables on both sides of an arrow, for example x → [y z] or [a b] → [c d]. The later specifies element wise edges; that is its the same as a → c; b → d. If you want edges corresponding to the cross-product, we have the double lined arrow [a b] ⇒ [c d], corresponding to a → c; a → d; b → c; b → d. The undirected arrows ↔ (element-wise) and ⇔ (crossproduct) behave the same way.","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"note: Unicode symbols in julia\nThe → symbol is a unicode symbol allowed in julia (among many others; see this list). You can enter it in the julia REPL or the vscode IDE by typing \\to followed by hitting tab. Similarly, ← = \\leftarrow,\n↔ = \\leftrightarrow,\n⇒ = \\Rightarrow,\n⇐ = \\Leftarrow,\n⇔ = \\LeftrightarrowThis may seem cumbersome at first, but with some practice allows you to specify your models in a really elegant way: [x₁ x₂ x₃] ← ξ → η → [y₁ y₂ y₃].","category":"page"},{"location":"tutorials/specification/graph_interface/#Options","page":"Graph interface","title":"Options","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"The graph syntax allows you to fix parameters to specific values, label them, and encode equality constraints by giving different parameters the same label. The following syntax example","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"graph = @StenoGraph begin\n\n    ξ₁ → fixed(1.0)*x1 + x2 + label(:a)*x3\n    ξ₂ → fixed(1.0)*x4 + x5 + label(:λ₁)*x6\n    ξ₃ → fixed(NaN)*x7 + x8 + label(:λ₁)*x9\n\n    ξ₃ ↔ fixed(1.0)*ξ₃\nend","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"would ","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"fix the directed effects from ξ₁ to x1 and from ξ₂ to x2 to 1\nleave the directed effect from ξ₃ to x7 free but instead restrict the variance of ξ₃ to 1\ngive the effect from ξ₁ to x3 the label :a (which can be convenient later if you want to retrieve information from your model about that specific parameter)\nconstrain the effect from ξ₂ to x6 and ξ₃ to x9 to be equal as they are both labeled the same.","category":"page"},{"location":"tutorials/specification/graph_interface/#Using-variables-inside-the-graph-specification","page":"Graph interface","title":"Using variables inside the graph specification","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"As you saw above and in the A first model example, the graph object needs to be converted to a parameter table:","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"partable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"The ParameterTable constructor also needs you to specify a vector of observed and latent variables, in the example above this would correspond to","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"observed_vars = [:x1 :x2 :x3 :x4 :x5 :x6 :x7 :x8 :x9]\nlatent_vars   = [:ξ₁ :ξ₂ :ξ₃]","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"The variable names (:x1) have to be symbols, the syntax :something creates an object of type Symbol. But you can also use vectors of symbols inside the graph specification, escaping them with _(...). For example, this graph specification","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"@StenoGraph begin\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ⇔ _(latent_vars)\nend","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"creates undirected effects coresponding to ","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"the variances of all observed variables and\nthe variances plus covariances of all latent variables","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"So if you want to work with a subset of variables, simply specify a vector of symbols somevars = [...], and inside the graph specification, refer to them as _(somevars).","category":"page"},{"location":"tutorials/specification/graph_interface/#Meanstructure","page":"Graph interface","title":"Meanstructure","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"Mean parameters are specified as a directed effect from 1 to the respective variable. In our example above, to estimate a mean parameter for all observed variables, we may write","category":"page"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"@StenoGraph begin\n    Symbol(\"1\") → _(observed_vars)\nend","category":"page"},{"location":"tutorials/specification/graph_interface/#Further-Reading","page":"Graph interface","title":"Further Reading","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/#What's-this-strange-looking-@-thing?","page":"Graph interface","title":"What's this strange looking @-thing?","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"The syntax to specify graphs (@StenoGraph) may seem a bit strange if you are not familiar with the julia language. It is called a macro, but explaining this concept in detail is beyond this documentation (and not necessary to understand to specify models). However, if you want to know more about it, you may have a look at the respective part of the manual.","category":"page"},{"location":"tutorials/specification/graph_interface/#The-StenoGraphs-Package","page":"Graph interface","title":"The StenoGraphs Package","text":"","category":"section"},{"location":"tutorials/specification/graph_interface/","page":"Graph interface","title":"Graph interface","text":"Behind the scenes, we are using the StenoGraphs package to specify our graphs. It makes a domain specific language available that allows you to specify graphs with arbitrary information attached to its edges and nodes (for structural equation models, this may be the name or the value of a parameter). Is also allows you to specify your own types to \"attach\" to the graph, called a Modifier. So if you contemplate about writing your own modifier (e.g., to mark a variable as ordinal, an effect as quadratic, ...), please refer to the StenoGraphs documentation.","category":"page"},{"location":"tutorials/backends/nlopt/#Using-NLopt.jl","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"","category":"section"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"SemOptimizerNLopt implements the connection to NLopt.jl. It takes a bunch of arguments:","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"    •  algorithm: optimization algorithm\n\n    •  options::Dict{Symbol, Any}: options for the optimization algorithm\n\n    •  local_algorithm: local optimization algorithm\n\n    •  local_options::Dict{Symbol, Any}: options for the local optimization algorithm\n\n    •  equality_constraints::Vector{NLoptConstraint}: vector of equality constraints\n\n    •  inequality_constraints::Vector{NLoptConstraint}: vector of inequality constraints","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"Constraints are explained in the section on Constrained optimization.","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"The defaults are LBFGS as the optimization algorithm and the standard options from NLopt.jl. We can choose something different:","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"my_optimizer = SemOptimizerNLopt(;\n    algorithm = :AUGLAG,\n    options = Dict(:maxeval => 200),\n    local_algorithm = :LD_LBFGS,\n    local_options = Dict(:ftol_rel => 1e-6)\n)","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"This uses an augmented lagrangian method with LBFGS as the local optimization algorithm, stops at a maximum of 200 evaluations and uses a relative tolerance of the objective value of 1e-6 as the stopping criterion for the local algorithm.","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"In the NLopt docs, you can find explanations about the different algorithms and a tutorial that also explains the different options.","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"To choose an algorithm, just pass its name without the 'NLOPT_' prefix (for example, 'NLOPT_LD_SLSQP' can be used by passing algorithm = :LD_SLSQP).","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"The README of the julia package may also be helpful, and provides a list of options:","category":"page"},{"location":"tutorials/backends/nlopt/","page":"Using NLopt.jl","title":"Using NLopt.jl","text":"algorithm\nstopval\nftol_rel\nftol_abs\nxtol_rel\nxtol_abs\nconstrtol_abs\nmaxeval\nmaxtime\ninitial_step\npopulation\nseed\nvector_storage","category":"page"},{"location":"performance/simulation/#Simulation-studies","page":"Simulation studies","title":"Simulation studies","text":"","category":"section"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"note: Simulation study interface\nWe are currently working on an interface for simulation studies. Until we are finished with this, this page is just a collection of tips.","category":"page"},{"location":"performance/simulation/#Swap-observed-data","page":"Simulation studies","title":"Swap observed data","text":"","category":"section"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"In simulation studies, a common task is fitting the same model to many different datasets. It would be a waste of resources to reconstruct the complete model for each dataset. We therefore provide the function swap_observed to change the observed part of a model, without necessarily reconstructing the other parts.","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"For the A first model, you would use it as","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"using StructuralEquationModels\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"data = example_data(\"political_democracy\")\n\ndata_1 = data[1:30, :]\n\ndata_2 = data[31:75, :]\n\nmodel = Sem(\n    specification = partable,\n    data = data_1\n)\n\nmodel_updated = swap_observed(model; data = data_2, specification = partable)","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"danger: Thread safety\nThis is only relevant when you are planning to fit updated models in parallelModels generated this way may share the same objects in memory (e.g. some parts of  model and model_updated are the same objects in memory.) Therefore, fitting both of these models in parallel will lead to race conditions,  possibly crashing your computer. To avoid these problems, you should copy model before updating it.","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"If you are building your models by parts, you can also update each part seperately with the function update_observed. For example,","category":"page"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"\nnew_observed = SemObservedData(;data = data_2, specification = partable)\n\nmy_optimizer = SemOptimizerOptim()\n\nnew_optimizer = update_observed(my_optimizer, new_observed)","category":"page"},{"location":"performance/simulation/#API","page":"Simulation studies","title":"API","text":"","category":"section"},{"location":"performance/simulation/","page":"Simulation studies","title":"Simulation studies","text":"swap_observed\nupdate_observed","category":"page"},{"location":"performance/simulation/#StructuralEquationModels.swap_observed","page":"Simulation studies","title":"StructuralEquationModels.swap_observed","text":"(1) swap_observed(model::AbstractSemSingle; kwargs...)\n\n(2) swap_observed(model::AbstractSemSingle, observed; kwargs...)\n\nReturn a new model with swaped observed part.\n\nArguments\n\nmodel::AbstractSemSingle: optimization algorithm.\nkwargs: additional keyword arguments; typically includes data = ...\nobserved: Either an object of subtype of SemObserved or a subtype of SemObserved\n\nExamples\n\nSee the online documentation on Swap observed data.\n\n\n\n\n\n","category":"function"},{"location":"performance/simulation/#StructuralEquationModels.update_observed","page":"Simulation studies","title":"StructuralEquationModels.update_observed","text":"update_observed(to_update, observed::SemObserved; kwargs...)\n\nUpdate a SemImply, SemLossFunction or SemOptimizer object to use a SemObserved object.\n\nExamples\n\nSee the online documentation on Swap observed data.\n\nImplementation\n\nYou can provide a method for this function when defining a new type, for more information on this see the online developer documentation on Update observed data.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/construction/build_by_parts/#Build-by-parts","page":"Build by parts","title":"Build by parts","text":"","category":"section"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"You can always build a model by parts - that is, you construct the observed, imply, loss and optimizer part seperately.","category":"page"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"As an example on how this works, we will build A first model in parts.","category":"page"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"First, we specify the model just as usual:","category":"page"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"using StructuralEquationModels\n\ndata = example_data(\"political_democracy\")\n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)","category":"page"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"Now, we construct the different parts:","category":"page"},{"location":"tutorials/construction/build_by_parts/","page":"Build by parts","title":"Build by parts","text":"# observed ---------------------------------------------------------------------------------\nobserved = SemObservedData(specification = partable, data = data)\n\n# imply ------------------------------------------------------------------------------------\nimply_ram = RAM(specification = partable)\n\n# loss -------------------------------------------------------------------------------------\nml = SemML(observed = observed)\n\nloss_ml = SemLoss(ml)\n\n# optimizer -------------------------------------------------------------------------------------\noptimizer = SemOptimizerOptim()\n\n# model ------------------------------------------------------------------------------------\n\nmodel_ml = Sem(observed, imply_ram, loss_ml, optimizer)","category":"page"},{"location":"tutorials/construction/outer_constructor/#Outer-Constructor","page":"Outer Constructor","title":"Outer Constructor","text":"","category":"section"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"We already have seen the outer constructor in action in A first model:","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"model = Sem(\n    specification = partable,\n    data = data\n)\n\n# output\n\nStructural Equation Model\n- Loss Functions\n   SemML\n- Fields\n   observed:  SemObservedCommon\n   imply:     RAM\n   optimizer: SemOptimizerOptim","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"The output of this call tells you exactly what model you just constructed (i.e. what the loss functions, observed, imply and optimizer parts are).","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"As you can see, by default, we use maximum likelihood estimation, the RAM imply type and the Optim.jl optimization backend.  To choose something different, you can provide it as a keyword argument:","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"model = Sem(\n    specification = partable,\n    data = data,\n    observed = ...,\n    imply = ...,\n    loss = ...,\n    optimizer = ...\n)","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"For example, to construct a model for weighted least squares estimation that uses symbolic precomputation and the NLopt backend, write","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"model = Sem(\n    specification = partable,\n    data = data,\n    imply = RAMSymbolic,\n    loss = SemWLS,\n    optimizer = SemOptimizerNLopt\n)","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"In the section on Our Concept of a Structural Equation Model, we go over the different options you have for each part of the model, and in API - model parts we explain each option in detail. Let's make another example: to use full information maximum likelihood information (FIML), we use","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"model = Sem(\n    specification = partable,\n    data = data,\n    loss = SemFIML,\n    observed = SemObservedMissing,\n    meanstructure = true\n)","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"You may also provide addition arguments for specific parts of the model. For example, WLS estimation uses per default","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"W = frac12 D^T(S^-1otimes S^-1)D","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"as the weight matrix, where D is the so-called duplication matrix and S is the observed covariance matrix. However, you can pass any other weight matrix you want (e.g., UWL, DWLS, ADF estimation) as a keyword argument:","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"W = ...\n\nmodel = Sem(\n    specification = partable,\n    data = data,\n    imply = RAMSymbolic,\n    loss = SemWLS\n    wls_weight_matrix = W\n)\n","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"To see what additional keyword arguments are supported, you can consult the documentation of the specific part of the model (either in the REPL by typing ? to enter the help mode and then typing the name of the thing you want to know something about, or in the online section API - model parts):","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"julia>?\n\nhelp>SemObservedMissing\n\n# output\n\n  For observed data with missing values.\n\n  Constructor\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  SemObservedMissing(;\n      specification,\n      data,\n      obs_colnames = nothing,\n      kwargs...)\n\n  Arguments\n  ≡≡≡≡≡≡≡≡≡≡≡\n\n    •  specification: either a RAMMatrices or ParameterTable object (1)\n\n    •  data: observed data\n\n    •  obs_colnames::Vector{Symbol}: column names of the data (if the object passed as data does not have column names, i.e. is not a data frame)\n\n  ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nExtended help is available with `??`","category":"page"},{"location":"tutorials/construction/outer_constructor/#Optimize-loss-functions-without-analytic-gradient","page":"Outer Constructor","title":"Optimize loss functions without analytic gradient","text":"","category":"section"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"For loss functions without analytic gradients, it is possible to use finite difference approximation or automatic differentiation. All loss functions provided in the package do have analytic gradients (and some even hessians or approximations thereof), so there is no need do use this feature if you are only working with them. However, if you implement your own loss function, you do not have to provide analytic gradients. This page is a about finite difference approximation. For information about how to use automatic differentiation, see the documentation of the AutoDiffSEM package.","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"To use finite difference approximation, you may construct your model just as before, but swap the Sem constructor for SemFiniteDiff. For example","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"model = SemFiniteDiff(\n    specification = partable,\n    data = data\n)","category":"page"},{"location":"tutorials/construction/outer_constructor/","page":"Outer Constructor","title":"Outer Constructor","text":"constructs a model that will use finite difference approximation if you estimate the parameters via sem_fit(model).","category":"page"},{"location":"performance/mixed_differentiation/#Mixed-analytical-and-automatic-differentiation","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"","category":"section"},{"location":"performance/mixed_differentiation/","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"This way of specifying our model is not ideal, however, because now also the maximum likelihood loss function lives inside a SemFiniteDiff model, and this means even though we have defined analytical gradients for it, we do not make use of them.","category":"page"},{"location":"performance/mixed_differentiation/","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"A more efficient way is therefore to specify our model as an ensemble model: ","category":"page"},{"location":"performance/mixed_differentiation/","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"model_ml = Sem(\n    specification = partable,\n    data = data,\n    loss = SemML\n)\n\nmodel_ridge = SemFiniteDiff(\n    specification = partable,\n    data = data,\n    loss = myridge\n)\n\nmodel_ml_ridge = SemEnsemble(model_ml, model_ridge)\n\nmodel_ml_ridge_fit = sem_fit(model_ml_ridge)","category":"page"},{"location":"performance/mixed_differentiation/","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"The results of both methods will be the same, but we can verify that the computation costs differ (the package BenchmarkTools has to be installed for this):","category":"page"},{"location":"performance/mixed_differentiation/","page":"Mixed analytical and automatic differentiation","title":"Mixed analytical and automatic differentiation","text":"using BenchmarkTools\n\n@benchmark sem_fit(model)\n\n@benchmark sem_fit(model_ml_ridge)","category":"page"},{"location":"tutorials/inspection/inspection/#Model-inspection","page":"Model Inspection","title":"Model inspection","text":"","category":"section"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"using StructuralEquationModels \n\nobserved_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    # loadings\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    # latent regressions\n    ind60 → dem60\n    dem60 → dem65\n    ind60 → dem65\n\n    # variances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    # covariances\n    y1 ↔ y5\n    y2 ↔ y4 + y6\n    y3 ↔ y7\n    y8 ↔ y4 + y6\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)\n\nmodel_fit = sem_fit(model)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"After you fitted a model,","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"model_fit = sem_fit(model)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"you end up with an object of type SemFit.","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"You can get some more information about it by using the sem_summary function:","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"sem_summary(model_fit)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"To compute fit measures, we use","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"fit_measures(model_fit)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"or compute them individually:","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"AIC(model_fit)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"A list of available Fit measures is at the end of this page.","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"To inspect the parameter estimates, we can update a ParameterTable object and call sem_summary on it:","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"update_estimate!(partable, model_fit)\n\nsem_summary(partable)","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"We can also update the ParameterTable object with other information via update_partable!. For example, if we want to compare hessian-based and bootstrap-based standard errors, we may write","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"se_bs = se_bootstrap(model_fit; n_boot = 20)\nse_he = se_hessian(model_fit)\n\nupdate_partable!(partable, model_fit, se_he, :se_hessian)\nupdate_partable!(partable, model_fit, se_bs, :se_bootstrap)\n\nsem_summary(partable)","category":"page"},{"location":"tutorials/inspection/inspection/#Export-results","page":"Model Inspection","title":"Export results","text":"","category":"section"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"You may convert a ParameterTable to a DataFrame and use the DataFrames package for further analysis (or to save it to your hard drive).","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"using DataFrames\n\nparameters_df = DataFrame(partable)","category":"page"},{"location":"tutorials/inspection/inspection/#API-model-inspection","page":"Model Inspection","title":"API - model inspection","text":"","category":"section"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"sem_summary\nupdate_estimate!\nupdate_partable!","category":"page"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.sem_summary","page":"Model Inspection","title":"StructuralEquationModels.sem_summary","text":"(1) sem_summary(sem_fit::SemFit; show_fitmeasures = false)\n\n(2) sem_summary(partable::AbstractParameterTable)\n\nPrint information about (1) a fitted SEM or (2) a parameter table to stdout.\n\nExtended help\n\nAddition keyword arguments\n\ndigits = 2: controls precision of printed estimates, standard errors, etc.\ncolor = :light_cyan: color of some parts of the printed output. Can be adjusted for readability.\nsecondary_color = :light_yellow\nshow_variables = true\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.update_estimate!","page":"Model Inspection","title":"StructuralEquationModels.update_estimate!","text":"update_estimate!(\n    partable::AbstractParameterTable, \n    sem_fit::SemFit)\n\nWrite parameter estimates from sem_fit to the :estimate column of partable\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.update_partable!","page":"Model Inspection","title":"StructuralEquationModels.update_partable!","text":"update_partable!(partable::AbstractParameterTable, sem_fit::SemFit, vec, column)\n\nWrite vec to column of partable.\n\nArguments\n\nvec::Vector: has to be in the same order as the model parameters\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#Additional-functions","page":"Model Inspection","title":"Additional functions","text":"","category":"section"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"Additional functions that can be used to extract information from a SemFit object:","category":"page"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"SemFit","category":"page"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.SemFit","page":"Model Inspection","title":"StructuralEquationModels.SemFit","text":"SemFit\n\nFitted structural equation model.\n\nInterfaces\n\nminimum(::SemFit) -> minimum objective value\nsolution(::SemFit) -> parameter estimates\nstart_val(::SemFit) -> starting values\nmodel(::SemFit)\noptimization_result(::SemFit)\noptimizer(::SemFit) -> optimization algorithm\nn_iterations(::SemFit) -> number of iterations\nconvergence(::SemFit) -> convergence properties\n\n\n\n\n\n","category":"type"},{"location":"tutorials/inspection/inspection/#Fit-measures","page":"Model Inspection","title":"Fit measures","text":"","category":"section"},{"location":"tutorials/inspection/inspection/","page":"Model Inspection","title":"Model Inspection","text":"fit_measures\nAIC\nBIC\nχ²\ndf\nminus2ll\nn_man\nn_obs\nn_par\np_value\nRMSEA","category":"page"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.fit_measures","page":"Model Inspection","title":"StructuralEquationModels.fit_measures","text":"fit_measures(sem_fit, args...)\n\nReturn a default set of fit measures or the fit measures passed as arg....\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.AIC","page":"Model Inspection","title":"StructuralEquationModels.AIC","text":"AIC(sem_fit::SemFit)\n\nReturn the akaike information criterion.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.BIC","page":"Model Inspection","title":"StructuralEquationModels.BIC","text":"BIC(sem_fit::SemFit)\n\nReturn the bayesian information criterion.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.χ²","page":"Model Inspection","title":"StructuralEquationModels.χ²","text":"χ²(sem_fit::SemFit)\n\nReturn the χ² value.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.df","page":"Model Inspection","title":"StructuralEquationModels.df","text":"df(sem_fit::SemFit)\ndf(model::AbstractSem)\n\nReturn the degrees of freedom.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.minus2ll","page":"Model Inspection","title":"StructuralEquationModels.minus2ll","text":"minus2ll(sem_fit::SemFit)\n\nReturn the negative 2* log likelihood.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.n_man","page":"Model Inspection","title":"StructuralEquationModels.n_man","text":"n_man(sem_fit::SemFit)\nn_man(model::AbstractSemSingle)\n\nReturn the number of manifest variables.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.n_obs","page":"Model Inspection","title":"StructuralEquationModels.n_obs","text":"n_obs(sem_fit::SemFit)\nn_obs(model::AbstractSemSingle)\nn_obs(model::SemEnsemble)\n\nReturn the number of observed data points.\n\nFor ensemble models, return the sum over all submodels.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.n_par","page":"Model Inspection","title":"StructuralEquationModels.n_par","text":"n_par(sem_fit::SemFit)\nn_par(model::AbstractSemSingle)\nn_par(model::SemEnsemble)\nn_par(identifier::Dict)\n\nReturn the number of parameters.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.p_value","page":"Model Inspection","title":"StructuralEquationModels.p_value","text":"p(sem_fit::SemFit)\n\nReturn the p value computed from the χ² test statistic.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/inspection/inspection/#StructuralEquationModels.RMSEA","page":"Model Inspection","title":"StructuralEquationModels.RMSEA","text":"RMSEA(sem_fit::SemFit)\n\nReturn the RMSEA.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/regularization/regularization/#Regularization","page":"Regularization","title":"Regularization","text":"","category":"section"},{"location":"tutorials/regularization/regularization/#Setup","page":"Regularization","title":"Setup","text":"","category":"section"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"For ridge regularization, you can simply use SemRidge as an additional loss function  (for example, a model with the loss functions SemML and SemRidge corresponds to ridge-regularized maximum likelihood estimation).","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"For lasso, elastic net and (far) beyond, we provide the ProximalSEM package. You can install it and load it alongside StructuralEquationModels:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"import Pkg\nPkg.add(url = \"https://github.com/StructuralEquationModels/ProximalSEM.jl\")\n\nusing StructuralEquationModels, ProximalSEM","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"import Pkg\nPkg.add(url = \"https://github.com/StructuralEquationModels/ProximalSEM.jl\")\n\nusing StructuralEquationModels, ProximalSEM","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"warning: ProximalSEM is still WIP\nThe ProximalSEM package does not have any releases yet, and is not well tested - until the first release, use at your own risk and expect interfaces to change without prior notice.","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"Additionally, you need to install and load ProximalOperators.jl:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"using ProximalOperators","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"Pkg.add(\"ProximalOperators\")\n\nusing ProximalOperators","category":"page"},{"location":"tutorials/regularization/regularization/#SemOptimizerProximal","page":"Regularization","title":"SemOptimizerProximal","text":"","category":"section"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"ProximalSEM provides a new \"building block\" for the optimizer part of a model, called SemOptimizerProximal. It connects our package to the ProximalAlgorithms.jl optimization backend, providing so-called proximal optimization algorithms.  Those can handle, amongst other things, various forms of regularization.","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"It can be used as","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"SemOptimizerProximal(\n    algorithm = ProximalAlgorithms.PANOC(),\n    options = Dict{Symbol, Any}(),\n    operator_g,\n    operator_h = nothing\n    )","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"The proximal operator (aka the regularization function) can be passed as operator_g, available options are listed here. The available Algorithms are listed here.","category":"page"},{"location":"tutorials/regularization/regularization/#First-example-lasso","page":"Regularization","title":"First example - lasso","text":"","category":"section"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"To show how it works, let's revisit A first model:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"observed_vars = [:x1, :x2, :x3, :y1, :y2, :y3, :y4, :y5, :y6, :y7, :y8]\nlatent_vars = [:ind60, :dem60, :dem65]\n\ngraph = @StenoGraph begin\n\n    ind60 → fixed(1)*x1 + x2 + x3\n    dem60 → fixed(1)*y1 + y2 + y3 + y4\n    dem65 → fixed(1)*y5 + y6 + y7 + y8\n\n    dem60 ← ind60\n    dem65 ← dem60\n    dem65 ← ind60\n\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars) ↔ _(latent_vars)\n\n    y1 ↔ label(:cov_15)*y5\n    y2 ↔ label(:cov_24)*y4 + label(:cov_26)*y6\n    y3 ↔ label(:cov_37)*y7\n    y4 ↔ label(:cov_48)*y8\n    y6 ↔ label(:cov_68)*y8\n\nend\n\npartable = ParameterTable(\n    latent_vars = latent_vars, \n    observed_vars = observed_vars, \n    graph = graph)\n\ndata = example_data(\"political_democracy\")\n\nmodel = Sem(\n    specification = partable,\n    data = data\n)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"We labeled the covariances between the items because we want to regularize those:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"ind = get_identifier_indices([:cov_15, :cov_24, :cov_26, :cov_37, :cov_48, :cov_68], model)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"In the following, we fit the same model with lasso regularization of those covariances. The lasso penalty is defined as","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"sum lambda_i lvert theta_i rvert","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"From the previously linked documentation, we find that lasso regularization is named NormL1 in the ProximalOperators package, and that we can pass an array of hyperparameters (λ) to control the amount of regularization for each parameter. To regularize only the observed item covariances, we define λ as","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"λ = zeros(31); λ[ind] .= 0.02","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"and use SemOptimizerProximal.","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"optimizer_lasso = SemOptimizerProximal(\n    operator_g = NormL1(λ)\n    )\n\nmodel_lasso = Sem(\n    specification = partable,\n    data = data,\n    optimizer = optimizer_lasso\n)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"Let's fit the regularized model","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"\nfit_lasso = sem_fit(model_lasso)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"and compare the solution to unregularizted estimates:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"fit = sem_fit(model)\n\nupdate_estimate!(partable, fit)\n\nupdate_partable!(partable, fit_lasso, solution(fit_lasso), :estimate_lasso)\n\nsem_summary(partable)","category":"page"},{"location":"tutorials/regularization/regularization/#Second-example-mixed-l1-and-l0-regularization","page":"Regularization","title":"Second example - mixed l1 and l0 regularization","text":"","category":"section"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"You can choose to penalize different parameters with different types of regularization functions. Let's use the lasso again on the covariances, but additionally penalyze the error variances of the observed items via l0 regularization.","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"The l0 penalty is defined as","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"lambda mathrmnnz(theta)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"To define a sup of separable proximal operators (i.e. no parameter is penalized twice), we can use SlicedSeparableSum from the ProximalOperators package:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"prox_operator = SlicedSeparableSum((NormL1(0.02), NormL0(20.0), NormL0(0.0)), ([ind], [12:22], [vcat(1:11, 23:25)]))\n\nmodel_mixed = Sem(\n    specification = partable,\n    data = data,\n    optimizer = SemOptimizerProximal,\n    operator_g = prox_operator\n)\n\nfit_mixed = sem_fit(model_mixed)","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"Let's again compare the different results:","category":"page"},{"location":"tutorials/regularization/regularization/","page":"Regularization","title":"Regularization","text":"update_partable!(partable, fit_mixed, solution(fit_mixed), :estimate_mixed)\n\nsem_summary(partable)","category":"page"},{"location":"performance/mkl/#Use-MKL","page":"MKL","title":"Use MKL","text":"","category":"section"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"Depending on the machine and the specific models, we sometimes observed large performance benefits from using MKL as a backend for matrix operations.  Fortunately, this is very simple to do in julia, so you can just try it out and check if turns out to be beneficial in your use case.","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"We install the MKL.jl package:","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"using Pkg; Pkg.add(\"MKL\")","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"Whenever we execute using MKL in a julia session, from now on MKL will be used as a backend. To check the installation:","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"using LinearAlgebra\n\nBLAS.get_config()\n\nusing MKL\n\nBLAS.get_config()","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"To check the performance implications for fitting a SEM, you can use the BenchmarkTools package:","category":"page"},{"location":"performance/mkl/","page":"MKL","title":"MKL","text":"using BenchmarkTools\n\n@benchmark sem_fit($your_model)\n\nusing MKL\n\n@benchmark sem_fit($your_model)","category":"page"},{"location":"tutorials/fitting/fitting/#Model-fitting","page":"Model Fitting","title":"Model fitting","text":"","category":"section"},{"location":"tutorials/fitting/fitting/","page":"Model Fitting","title":"Model Fitting","text":"As we saw in A first model, after you have build a model, you can fit it via","category":"page"},{"location":"tutorials/fitting/fitting/","page":"Model Fitting","title":"Model Fitting","text":"model_fit = sem_fit(model)\n\n# output\n\nFitted Structural Equation Model \n=============================================== \n--------------------- Model ------------------- \n\nStructural Equation Model \n- Loss Functions \n   SemML\n- Fields \n   observed:  SemObservedData \n   imply:     RAM \n   optimizer: SemOptimizerOptim \n\n------------- Optimization result ------------- \n\n * Status: success\n\n * Candidate solution\n    Final objective value:     2.120543e+01\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 6.13e-05 ≰ 1.5e-08\n    |x - x'|/|x'|          = 8.21e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.05e-09 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 4.94e-11 ≤ 1.0e-10\n    |g(x)|                 = 2.48e-05 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    175\n    f(x) calls:    524\n    ∇f(x) calls:   524","category":"page"},{"location":"tutorials/fitting/fitting/","page":"Model Fitting","title":"Model Fitting","text":"You may optionally specify Starting values.","category":"page"},{"location":"tutorials/fitting/fitting/#API-model-fitting","page":"Model Fitting","title":"API - model fitting","text":"","category":"section"},{"location":"tutorials/fitting/fitting/","page":"Model Fitting","title":"Model Fitting","text":"sem_fit","category":"page"},{"location":"tutorials/fitting/fitting/#StructuralEquationModels.sem_fit","page":"Model Fitting","title":"StructuralEquationModels.sem_fit","text":"sem_fit(model::AbstractSem; start_val = start_val, kwargs...)\n\nReturn the fitted model.\n\nArguments\n\nmodel: AbstractSem to fit\nstart_val: vector of starting values or function to compute starting values (1)\nkwargs...: keyword arguments, passed to starting value functions\n\n(1) available options are start_fabin3, start_simple and start_partable.  For more information, we refer to the individual documentations and the online documentation on Starting values.\n\nExamples\n\nsem_fit(\n    my_model; \n    start_val = start_simple,\n    start_covariances_latent = 0.5)\n\n\n\n\n\n","category":"function"},{"location":"tutorials/concept/#Our-Concept-of-a-Structural-Equation-Model","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"In our package, every Structural Equation Model (Sem) consists of four parts:","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"(Image: SEM concept)","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"Those parts are interchangable building blocks (like 'Legos'), i.e. there are different pieces available you can choose as the 'observed' slot of the model, and stick them together with other pieces that can serve as the 'imply' part.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The 'observed' part is for observed data, the imply part is what the model implies about your data (e.g. the model implied covariance matrix), the loss part compares the observed data and implied properties (e.g. weighted least squares difference between the observed and implied covariance matrix) and the optimizer part connects to the optimization backend (e.g. the type of optimization algorithm used).","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"For example, to build a model for maximum likelihood estimation with the NLopt optimization suite as a backend you would choose SemML as a loss function and SemOptimizerNLopt as the optimizer.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"As you can see, a model can have as many loss functions as you want it to have. We always optimize over their (weighted) sum. So to build a model for ridge regularized full information maximum likelihood estimation, you would choose two loss functions, SemFIML and SemRidge.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"In julia, everything has a type. To make more precise which objects can be used as the different building blocks, we require them to have a certain type:","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"(Image: SEM concept typed)","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"So everything that can be used as the 'observed' part has to be of type SemObserved.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"Here is an overview on the available building blocks:","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemObserved SemImply SemLossFunction SemOptimizer\nSemObservedData RAM SemML SemOptimizerOptim\nSemObservedCovariance RAMSymbolic SemWLS SemOptimizerNLopt\nSemObservedMissing ImplyEmpty SemFIML \n  SemRidge \n  SemConstant ","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The rest of this page explains the building blocks for each part. First, we explain every part and give an overview on the different options that are available. After that, the API - model parts section serves as a reference for detailed explanations about the different options. (How to stick them together to a final model is explained in the section on Model Construction.)","category":"page"},{"location":"tutorials/concept/#The-observed-part-aka-[SemObserved](@ref)","page":"Our Concept of a Structural Equation Model","title":"The observed part aka SemObserved","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The 'observed' part contains all necessary information about the observed data. Currently, we have three options: SemObservedData for fully observed datasets, SemObservedCovariance for observed covariances (and means) and SemObservedMissing for data that contains missing values.","category":"page"},{"location":"tutorials/concept/#The-imply-part-aka-[SemImply](@ref)","page":"Our Concept of a Structural Equation Model","title":"The imply part aka SemImply","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The imply part is what your model implies about the data, for example, the model-implied covariance matrix.  There are two options at the moment: RAM, which uses the reticular action model to compute the model implied covariance matrix, and RAMSymbolic which does the same but symbolically pre-computes part of the model, which increases subsequent performance in model fitting (see Symbolic precomputation). There is also a third option, ImplyEmpty that can serve as a 'placeholder' for models that do not need an imply part.","category":"page"},{"location":"tutorials/concept/#The-loss-part-aka-SemLoss","page":"Our Concept of a Structural Equation Model","title":"The loss part aka SemLoss","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The loss part specifies the objective that is optimized to find the parameter estimates. If it contains more then one loss function (aka SemLossFunction)), we find the parameters by minimizing the sum of loss functions (for example in maximum likelihood estimation + ridge regularization). Available loss functions are","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemML: maximum likelihood estimation\nSemWLS: weighted least squares estimation\nSemFIML: full-information maximum likelihood estimation\nSemRidge: ridge regularization","category":"page"},{"location":"tutorials/concept/#The-optimizer-part-aka-SemOptimizer","page":"Our Concept of a Structural Equation Model","title":"The optimizer part aka SemOptimizer","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"The optimizer part of a model connects to the numerical optimization backend used to fit the model.  It can be used to control options like the optimization algorithm, linesearch, stopping criteria, etc.  There are currently two available backends, SemOptimizerOptim connecting to the Optim.jl backend, and SemOptimizerNLopt connecting to the NLopt.jl backend. For more information about the available options see also the tutorials about Using Optim.jl and Using NLopt.jl, as well as Constrained optimization.","category":"page"},{"location":"tutorials/concept/#What-to-do-next","page":"Our Concept of a Structural Equation Model","title":"What to do next","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"You now have an understanding about our representation of structural equation models.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"To learn more about how to use the package, you may visit the remaining tutorials.","category":"page"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"If you want to learn how to extend the package (e.g., add a new loss function), you may visit Extending the package.","category":"page"},{"location":"tutorials/concept/#API-model-parts","page":"Our Concept of a Structural Equation Model","title":"API - model parts","text":"","category":"section"},{"location":"tutorials/concept/#observed","page":"Our Concept of a Structural Equation Model","title":"observed","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemObserved\nSemObservedData\nSemObservedCovariance\nSemObservedMissing","category":"page"},{"location":"tutorials/concept/#StructuralEquationModels.SemObserved","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObserved","text":"Supertype of all objects that can serve as the observed field of a SEM. Pre-processes data and computes sufficient statistics for example. If you have a special kind of data, e.g. ordinal data, you should implement a subtype of SemObserved.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedData","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedData","text":"For observed data without missings.\n\nConstructor\n\nSemObservedData(;\n    specification,\n    data,\n    meanstructure = false,\n    obs_colnames = nothing,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object (1)\ndata: observed data\nmeanstructure::Bool: does the model have a meanstructure?\nobs_colnames::Vector{Symbol}: column names of the data (if the object passed as data does not have column names, i.e. is not a data frame)\n\nExtended help\n\nInterfaces\n\nn_obs(::SemObservedData) -> number of observed data points\nn_man(::SemObservedData) -> number of manifest variables\nget_data(::SemObservedData) -> observed data\nobs_cov(::SemObservedData) -> observed.obs_cov\nobs_mean(::SemObservedData) -> observed.obs_mean\ndata_rowwise(::SemObservedData) -> observed data, stored as vectors per observation\n\nImplementation\n\nSubtype of SemObserved\n\nRemarks\n\n(1) the specification argument can also be nothing, but this turns of checking whether the observed data/covariance columns are in the correct order! As a result, you should only use this if you are sure your observed data is in the right format.\n\nAdditional keyword arguments:\n\nspec_colnames::Vector{Symbol} = nothing: overwrites column names of the specification object\ncompute_covariance::Bool ) = true: should the covariance of data be computed and stored?\nrowwise::Bool = false: should the data be stored also as vectors per observation\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedCovariance","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedCovariance","text":"For observed covariance matrices and means.\n\nConstructor\n\nSemObservedCovariance(;\n    specification,\n    obs_cov,\n    obs_colnames = nothing,\n    meanstructure = false,\n    obs_mean = nothing,\n    n_obs = nothing,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object (1)\nobs_cov: observed covariance matrix\nobs_colnames::Vector{Symbol}: column names of the covariance matrix\nmeanstructure::Bool: does the model have a meanstructure?\nobs_mean: observed mean vector\nn_obs::Number: number of observed data points (necessary for fit statistics)\n\nExtended help\n\nInterfaces\n\nn_obs(::SemObservedCovariance) -> number of observed data points\nn_man(::SemObservedCovariance) -> number of manifest variables\nobs_cov(::SemObservedCovariance) -> observed covariance matrix\nobs_mean(::SemObservedCovariance) -> observed means\n\nImplementation\n\nSubtype of SemObserved\n\nRemarks\n\n(1) the specification argument can also be nothing, but this turns of checking whether the observed data/covariance columns are in the correct order! As a result, you should only use this if you are sure your covariance matrix is in the right format.\n\nAdditional keyword arguments:\n\nspec_colnames::Vector{Symbol} = nothing: overwrites column names of the specification object\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemObservedMissing","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemObservedMissing","text":"For observed data with missing values.\n\nConstructor\n\nSemObservedMissing(;\n    specification,\n    data,\n    obs_colnames = nothing,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object (1)\ndata: observed data\nobs_colnames::Vector{Symbol}: column names of the data (if the object passed as data does not have column names, i.e. is not a data frame)\n\nExtended help\n\nInterfaces\n\nn_obs(::SemObservedMissing) -> number of observed data points\nn_man(::SemObservedMissing) -> number of manifest variables\nget_data(::SemObservedMissing) -> observed data\ndata_rowwise(::SemObservedMissing) -> observed data as vector per observation, with missing values deleted\npatterns(::SemObservedMissing) -> indices of non-missing variables per missing patterns \npatterns_not(::SemObservedMissing) -> indices of missing variables per missing pattern\nrows(::SemObservedMissing) -> row indices of observed data points that belong to each pattern\npattern_n_obs(::SemObservedMissing) -> number of data points per pattern\npattern_nvar_obs(::SemObservedMissing) -> number of non-missing observed variables per pattern\nobs_mean(::SemObservedMissing) -> observed mean per pattern\nobs_cov(::SemObservedMissing) -> observed covariance per pattern\nem_model(::SemObservedMissing) -> EmMVNModel that contains the covariance matrix and mean vector found via optimization maximization\n\nImplementation\n\nSubtype of SemObserved\n\nRemarks\n\n(1) the specification argument can also be nothing, but this turns of checking whether the observed data/covariance columns are in the correct order! As a result, you should only use this if you are sure your observed data is in the right format.\n\nAdditional keyword arguments:\n\nspec_colnames::Vector{Symbol} = nothing: overwrites column names of the specification object\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#imply","page":"Our Concept of a Structural Equation Model","title":"imply","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemImply\nRAM\nRAMSymbolic\nImplyEmpty","category":"page"},{"location":"tutorials/concept/#StructuralEquationModels.SemImply","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemImply","text":"Supertype of all objects that can serve as the imply field of a SEM. Computed model-implied values that should be compared with the observed data to find parameter estimates, e. g. the model implied covariance or mean. If you would like to implement a different notation, e.g. LISREL, you should implement a subtype of SemImply.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.RAM","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.RAM","text":"Model implied covariance and means via RAM notation.\n\nConstructor\n\nRAM(;\n    specification,\n    meanstructure = false,\n    gradient = true,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\nmeanstructure::Bool: does the model have a meanstructure?\ngradient::Bool: is gradient-based optimization used\n\nExtended help\n\nImplementation\n\nSubtype of SemImply.\n\nRAM notation\n\nThe model implied covariance matrix is computed as\n\n    Sigma = F(I-A)^-1S(I-A)^-TF^T\n\nand for models with a meanstructure, the model implied means are computed as\n\n    mu = F(I-A)^-1M\n\nInterfaces\n\nidentifier(::RAM)-> Dict containing the parameter labels and their position\nn_par(::RAM) -> Number of parameters\nΣ(::RAM) -> model implied covariance matrix\nμ(::RAM) -> model implied mean vector\n\nRAM matrices for the current parameter values:\n\nA(::RAM)\nS(::RAM)\nF(::RAM)\nM(::RAM)\n\nJacobians of RAM matrices w.r.t to the parameter vector θ\n\n∇A(::RAM) -> vec(A)θᵀ\n∇S(::RAM) -> vec(S)θᵀ\n∇M(::RAM) = Mθᵀ\n\nVector of indices of each parameter in the respective RAM matrix:\n\nA_indices(::RAM)\nS_indices(::RAM)\nM_indices(::RAM)\n\nAdditional interfaces\n\nF⨉I_A⁻¹(::RAM) -> F(I-A)^-1\nF⨉I_A⁻¹S(::RAM) -> F(I-A)^-1S\nI_A(::RAM) -> I-A\nhas_meanstructure(::RAM) -> Val{Bool} does the model have a meanstructure?\n\nOnly available in gradient! calls:\n\nI_A⁻¹(::RAM) -> (I-A)^-1\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.RAMSymbolic","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.RAMSymbolic","text":"Subtype of SemImply that implements the RAM notation with symbolic precomputation.\n\nConstructor\n\nRAMSymbolic(;specification,\n    vech = false,\n    gradient = true,\n    hessian = false,\n    approximate_hessian = false,\n    meanstructure = false,\n    kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\nmeanstructure::Bool: does the model have a meanstructure?\ngradient::Bool: is gradient-based optimization used\nhessian::Bool: is hessian-based optimization used\napproximate_hessian::Bool: for hessian based optimization: should the hessian be approximated\nvech::Bool: should the half-vectorization of Σ be computed (instead of the full matrix)   (automatically set to true if any of the loss functions is SemWLS)\n\nExtended help\n\nImplementation\n\nSubtype of SemImply.\n\nInterfaces\n\nidentifier(::RAMSymbolic)-> Dict containing the parameter labels and their position\nn_par(::RAMSymbolic) -> Number of parameters\nΣ(::RAMSymbolic) -> model implied covariance matrix\nμ(::RAMSymbolic) -> model implied mean vector\n\nJacobians (only available in gradient! calls)\n\n∇Σ(::RAMSymbolic) -> vec(Σ)θᵀ\n∇μ(::RAMSymbolic) -> μθᵀ\n∇Σ_function(::RAMSymbolic) -> function to overwrite ∇Σ in place,   i.e. ∇Σ_function(∇Σ, θ). Normally, you do not want to use this but simply   query ∇Σ(::RAMSymbolic).\n\nHessians The computation of hessians is more involved, and uses the \"chain rule for hessian matrices\". Therefore, we desribe it at length in the mathematical appendix of the online documentation, and the relevant interfaces are omitted here.\n\nAdditional interfaces\n\nhas_meanstructure(::RAMSymbolic) -> Val{Bool} does the model have a meanstructure?\n\nRAM notation\n\nThe model implied covariance matrix is computed as\n\n    Sigma = F(I-A)^-1S(I-A)^-TF^T\n\nand for models with a meanstructure, the model implied means are computed as\n\n    mu = F(I-A)^-1M\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.ImplyEmpty","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.ImplyEmpty","text":"Empty placeholder for models that don't need an imply part. (For example, models that only regularize parameters.)\n\nConstructor\n\nImplyEmpty(;specification, kwargs...)\n\nArguments\n\nspecification: either a RAMMatrices or ParameterTable object\n\nExamples\n\nA multigroup model with ridge regularization could be specified as a SemEnsemble with one model per group and an additional model with ImplyEmpty and SemRidge for the regularization part.\n\nExtended help\n\nInterfaces\n\nidentifier(::RAMSymbolic)-> Dict containing the parameter labels and their position\nn_par(::RAMSymbolic) -> Number of parameters\n\nImplementation\n\nSubtype of SemImply.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#loss-functions","page":"Our Concept of a Structural Equation Model","title":"loss functions","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemLoss\nSemLossFunction\nSemML\nSemFIML\nSemWLS\nSemRidge\nSemConstant","category":"page"},{"location":"tutorials/concept/#StructuralEquationModels.SemLoss","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemLoss","text":"SemLoss(args...; loss_weights = nothing, ...)\n\nConstructs the loss field of a SEM. Can contain multiple SemLossFunctions, the model is optimized over their sum. See also SemLossFunction.\n\nArguments\n\nargs...: Multiple SemLossFunctions.\nloss_weights::Vector: Weights for each loss function. Defaults to unweighted optimization.\n\nExamples\n\nmy_ml_loss = SemML(...)\nmy_ridge_loss = SemRidge(...)\nmy_loss = SemLoss(SemML, SemRidge; loss_weights = [1.0, 2.0])\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemLossFunction","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemLossFunction","text":"Supertype for all loss functions of SEMs. If you want to implement a custom loss function, it should be a subtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemML","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemML","text":"Maximum likelihood estimation.\n\nConstructor\n\nSemML(;observed, meanstructure = false, approximate_hessian = false, kwargs...)\n\nArguments\n\nobserved::SemObserved: the observed part of the model\nmeanstructure::Bool: does the model have a meanstructure?\napproximate_hessian::Bool: if hessian-based optimization is used, should the hessian be swapped for an approximation\n\nExamples\n\nmy_ml = SemML(observed = my_observed)\n\nInterfaces\n\nAnalytic gradients are available, and for models without a meanstructure, also analytic hessians.\n\nExtended help\n\nImplementation\n\nSubtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemFIML","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemFIML","text":"Full information maximum likelihood estimation. Can handle observed data with missings.\n\nConstructor\n\nSemFIML(;observed, specification, kwargs...)\n\nArguments\n\nobserved::SemObservedMissing: the observed part of the model\nspecification: either a RAMMatrices or ParameterTable object\n\nExamples\n\nmy_fiml = SemFIML(observed = my_observed, specification = my_parameter_table)\n\nInterfaces\n\nAnalytic gradients are available.\n\nExtended help\n\nImplementation\n\nSubtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemWLS","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemWLS","text":"Weighted least squares estimation.\n\nConstructor\n\nSemWLS(;\n    observed,\n    meanstructure = false, \n    wls_weight_matrix = nothing, \n    wls_weight_matrix_mean = nothing, \n    approximate_hessian = false, \n    kwargs...)\n\nArguments\n\nobserved: the SemObserved part of the model\nmeanstructure::Bool: does the model have a meanstructure?\napproximate_hessian::Bool: should the hessian be swapped for an approximation\nwls_weight_matrix: the weight matrix for weighted least squares.    Defaults to GLS estimation (05*(D^T*kron(SS)*D) where D is the duplication matrix    and S is the inverse ob the observed covariance matrix)\nwls_weight_matrix_mean: the weight matrix for the mean part of weighted least squares.    Defaults to GLS estimation (the inverse of the observed covariance matrix)\n\nExamples\n\nmy_wls = SemWLS(observed = my_observed)\n\nInterfaces\n\nAnalytic gradients are available, and for models without a meanstructure, also analytic hessians.\n\nExtended help\n\nImplementation\n\nSubtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemRidge","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemRidge","text":"Ridge regularization.\n\nConstructor\n\nSemRidge(;α_ridge, which_ridge, n_par, parameter_type = Float64, imply = nothing, kwargs...)\n\nArguments\n\nα_ridge: hyperparameter for penalty term\nwhich_ridge::Vector: Vector of parameter labels (Symbols) or indices that indicate which parameters should be regularized.\nn_par::Int: number of parameters of the model\nimply::SemImply: imply part of the model\nparameter_type: type of the parameters\n\nExamples\n\nmy_ridge = SemRidge(;α_ridge = 0.02, which_ridge = [:λ₁, :λ₂, :ω₂₃], n_par = 30, imply = my_imply)\n\nInterfaces\n\nAnalytic gradients and hessians are available.\n\nExtended help\n\nImplementation\n\nSubtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemConstant","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemConstant","text":"Constant loss term. Can be used for comparability to other packages.\n\nConstructor\n\nSemConstant(;constant_loss, kwargs...)\n\nArguments\n\nconstant_loss::Number: constant to add to the objective\n\nExamples\n\n    my_constant = SemConstant(constant_loss = 42.0)\n\nInterfaces\n\nAnalytic gradients and hessians are available.\n\nExtended help\n\nImplementation\n\nSubtype of SemLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#optimizer","page":"Our Concept of a Structural Equation Model","title":"optimizer","text":"","category":"section"},{"location":"tutorials/concept/","page":"Our Concept of a Structural Equation Model","title":"Our Concept of a Structural Equation Model","text":"SemOptimizer\nSemOptimizerOptim\nSemOptimizerNLopt","category":"page"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizer","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizer","text":"Supertype of all objects that can serve as the optimizer field of a SEM. Connects the SEM to its optimization backend and controls options like the optimization algorithm. If you want to connect the SEM package to a new optimization backend, you should implement a subtype of SemOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizerOptim","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizerOptim","text":"Connects to Optim.jl as the optimization backend.\n\nConstructor\n\nSemOptimizerOptim(;\n    algorithm = LBFGS(), \n    options = Optim.Options(;f_tol = 1e-10, x_tol = 1.5e-8), \n    kwargs...)\n\nArguments\n\nalgorithm: optimization algorithm.\noptions::Optim.Options: options for the optimization algorithm\n\nUsage\n\nAll algorithms and options from the Optim.jl library are available, for more information see  the Optim.jl online documentation.\n\nExamples\n\nmy_optimizer = SemOptimizerOptim()\n\n# hessian based optimization with backtracking linesearch and modified initial step size\nusing Optim, LineSearches\n\nmy_newton_optimizer = SemOptimizerOptim(\n    algorithm = Newton(\n        ;linesearch = BackTracking(order=3), \n        alphaguess = InitialHagerZhang()\n    )\n)\n\nExtended help\n\nInterfaces\n\nalgorithm(::SemOptimizerOptim)\noptions(::SemOptimizerOptim)\n\nImplementation\n\nSubtype of SemOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/concept/#StructuralEquationModels.SemOptimizerNLopt","page":"Our Concept of a Structural Equation Model","title":"StructuralEquationModels.SemOptimizerNLopt","text":"Connects to NLopt.jl as the optimization backend.\n\nConstructor\n\nSemOptimizerNLopt(;\n    algorithm = :LD_LBFGS,\n    options = Dict{Symbol, Any}(),\n    local_algorithm = nothing, \n    local_options = Dict{Symbol, Any}(), \n    equality_constraints = Vector{NLoptConstraint}(), \n    inequality_constraints = Vector{NLoptConstraint}(), \n    kwargs...)\n\nArguments\n\nalgorithm: optimization algorithm.\noptions::Dict{Symbol, Any}: options for the optimization algorithm\nlocal_algorithm: local optimization algorithm\nlocal_options::Dict{Symbol, Any}: options for the local optimization algorithm\nequality_constraints::Vector{NLoptConstraint}: vector of equality constraints\ninequality_constraints::Vector{NLoptConstraint}: vector of inequality constraints\n\nExample\n\nmy_optimizer = SemOptimizerNLopt()\n\n# constrained optimization with augmented lagrangian\nmy_constrained_optimizer = SemOptimizerNLopt(;\n    algorithm = :AUGLAG,\n    local_algorithm = :LD_LBFGS,\n    local_options = Dict(:ftol_rel => 1e-6),\n    inequality_constraints = NLoptConstraint(;f = my_constraint, tol = 0.0),\n)\n\nUsage\n\nAll algorithms and options from the NLopt library are available, for more information see  the NLopt.jl package and the NLopt online documentation. For information on how to use inequality and equality constraints,  see Constrained optimization in our online documentation.\n\nExtended help\n\nInterfaces\n\nalgorithm(::SemOptimizerNLopt)\nlocal_algorithm(::SemOptimizerNLopt)\noptions(::SemOptimizerNLopt)\nlocal_options(::SemOptimizerNLopt)\nequality_constraints(::SemOptimizerNLopt)\ninequality_constraints(::SemOptimizerNLopt)\n\nImplementation\n\nSubtype of SemOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/backends/optim/#Using-Optim.jl","page":"Using Optim.jl","title":"Using Optim.jl","text":"","category":"section"},{"location":"tutorials/backends/optim/","page":"Using Optim.jl","title":"Using Optim.jl","text":"SemOptimizerOptim implements the connection to Optim.jl. It takes two arguments, algorithm and options. The defaults are LBFGS as the optimization algorithm and the standard options from Optim.jl. We can load the Optim and LineSearches packages to choose something different:","category":"page"},{"location":"tutorials/backends/optim/","page":"Using Optim.jl","title":"Using Optim.jl","text":"using Optim, LineSearches\n\nmy_optimizer = SemOptimizerOptim(\n    algorithm = BFGS(\n        linesearch = BackTracking(order=3), \n        alphaguess = InitialHagerZhang()\n        ),\n    options = Optim.Options(show_trace = true) \n    )","category":"page"},{"location":"tutorials/backends/optim/","page":"Using Optim.jl","title":"Using Optim.jl","text":"A model with this optimizer object will use BFGS (!not L-BFGS) with a back tracking linesearch and a certain initial step length guess. Also, the trace of the optimization will be printed to the console.","category":"page"},{"location":"tutorials/backends/optim/","page":"Using Optim.jl","title":"Using Optim.jl","text":"For a list of all available algorithms and options, we refer to this page of the Optim.jl manual.","category":"page"},{"location":"developer/sem/#Custom-model-types","page":"Custom model types","title":"Custom model types","text":"","category":"section"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"The abstract supertype for all models is AbstractSem, which has two subtypes, AbstractSemSingle{O, I, L, D} and AbstractSemCollection. Currently, there are 2 subtypes of AbstractSemSingle: Sem, SemFiniteDiff. All subtypes of AbstractSemSingle should have at least observed, imply, loss and optimizer fields, and share their types ({O, I, L, D}) with the parametric abstract supertype. For example, the SemFiniteDiff type is implemented as","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"struct SemFiniteDiff{\n        O <: SemObserved, \n        I <: SemImply, \n        L <: SemLoss, \n        D <: SemOptimizer} <: AbstractSemSingle{O, I, L, D}\n    observed::O\n    imply::I\n    loss::L\n    optimizer::Dend","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"Additionally, we need to define a method to compute at least the objective value, and if you want to use gradient based optimizers (which you most probably will), we need also to define a method to compute the gradient. For example, the respective fallback methods for all AbstractSemSingle models are defined as","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"function objective!(model::AbstractSemSingle, parameters)\n    objective!(imply(model), parameters, model)\n    return objective!(loss(model), parameters, model)\nend\n\nfunction gradient!(gradient, model::AbstractSemSingle, parameters)\n    fill!(gradient, zero(eltype(gradient)))\n    gradient!(imply(model), parameters, model)\n    gradient!(gradient, loss(model), parameters, model)\nend","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"Note that the gradient! method takes a pre-allocated array that should be filled with the gradient values.","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"Additionally, we can define constructors like the one in \"src/frontend/specification/Sem.jl\".","category":"page"},{"location":"developer/sem/","page":"Custom model types","title":"Custom model types","text":"It is also possible to add new subtypes for AbstractSemCollection.","category":"page"},{"location":"developer/optimizer/#Custom-optimizer-types","page":"Custom optimizer types","title":"Custom optimizer types","text":"","category":"section"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"The optimizer part of a model connects it to the optimization backend.  The first part of the implementation is very similar to loss functions, so we just show the implementation of SemOptimizerOptim here as a reference:","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"############################################################################\n### Types and Constructor\n############################################################################\n\nmutable struct SemOptimizerOptim{A, B} <: SemOptimizer\n    algorithm::A\n    options::B\nend\n\nfunction SemOptimizerOptim(;\n        algorithm = LBFGS(), \n        options = Optim.Options(;f_tol = 1e-10, x_tol = 1.5e-8), \n        kwargs...)\n    return SemOptimizerOptim(algorithm, options)\nend\n\n############################################################################\n### Recommended methods\n############################################################################\n\nupdate_observed(optimizer::SemOptimizerOptim, observed::SemObserved; kwargs...) = optimizer\n\n############################################################################\n### additional methods\n############################################################################\n\nalgorithm(optimizer::SemOptimizerOptim) = optimizer.algorithm\noptions(optimizer::SemOptimizerOptim) = optimizer.options","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"Now comes a part that is a little bit more complicated: We need to write methods for sem_fit:","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"function sem_fit(\n        model::AbstractSemSingle{O, I, L, D}; \n        start_val = start_val, \n        kwargs...) where {O, I, L, D <: SemOptimizerOptim}\n    \n    if !isa(start_val, Vector)\n        start_val = start_val(model; kwargs...)\n    end\n\n    optimization_result = ...\n\n    ...\n\n    return SemFit(minimum, minimizer, start_val, model, optimization_result)\nend","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"The method has to return a SemFit object that consists of the minimum of the objective at the solution, the minimizer (aka parameter estimates), the starting values, the model and the optimization result (which may be anything you desire for your specific backend).","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"If we want our type to also work with SemEnsemble models, we also have to provide a method for that:","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"function sem_fit(\n        model::SemEnsemble{N, T , V, D, S}; \n        start_val = start_val, \n        kwargs...) where {N, T, V, D <: SemOptimizerOptim, S}\n\n    if !isa(start_val, Vector)\n        start_val = start_val(model; kwargs...)\n    end\n\n\n    optimization_result = ...\n\n    ...\n\n    return SemFit(minimum, minimizer, start_val, model, optimization_result)\n\nend","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"In addition, you might want to provide methods to access properties of your optimization result:","category":"page"},{"location":"developer/optimizer/","page":"Custom optimizer types","title":"Custom optimizer types","text":"optimizer(res::MyOptimizationResult) = ...\nn_iterations(res::MyOptimizationResult) = ...\nconvergence(res::MyOptimizationResult) = ...","category":"page"},{"location":"tutorials/collection/multigroup/#Multigroup-models","page":"Multigroup models","title":"Multigroup models","text":"","category":"section"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"using StructuralEquationModels","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"As an example, we will fit the model from the lavaan tutorial with loadings constrained to equality across groups.","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"We first load the example data and split it between groups:","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"dat = example_data(\"holzinger_swineford\")\n\ndat_g1 = dat[dat.school .== \"Pasteur\", :]\ndat_g2 = dat[dat.school .== \"Grant-White\", :]","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"dat = example_data(\"holzinger_swineford\")\n\ndat_g1 = dat[dat.school .== \"Pasteur\", :]\ndat_g2 = dat[dat.school .== \"Grant-White\", :]","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"We then specify our model via the graph interface:","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"latent_vars = [:visual, :textual, :speed]\nobserved_vars = Symbol.(:x, 1:9)\n\ngraph = @StenoGraph begin\n    # measurement model\n    visual  → fixed(1.0, 1.0)*x1 + label(:λ₂, :λ₂)*x2 + label(:λ₃, :λ₃)*x3\n    textual → fixed(1.0, 1.0)*x4 + label(:λ₅, :λ₅)*x5 + label(:λ₆, :λ₆)*x6\n    speed   → fixed(1.0, 1.0)*x7 + label(:λ₈, :λ₈)*x8 + label(:λ₉, :λ₉)*x9\n    # variances and covariances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars)   ⇔ _(latent_vars)\nend","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"latent_vars = [:visual, :textual, :speed]\nobserved_vars = Symbol.(:x, 1:9)\n\ngraph = @StenoGraph begin\n    # measurement model\n    visual  → fixed(1, 1)*x1 + label(:λ₂, :λ₂)*x2 + label(:λ₃, :λ₃)*x3\n    textual → fixed(1, 1)*x4 + label(:λ₅, :λ₅)*x5 + label(:λ₆, :λ₆)*x6\n    speed   → fixed(1, 1)*x7 + label(:λ₈, :λ₈)*x8 + label(:λ₉, :λ₉)*x9\n    # variances and covariances\n    _(observed_vars) ↔ _(observed_vars)\n    _(latent_vars)   ⇔ _(latent_vars)\nend","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"You can pass multiple arguments to fix() and label() for each group. Parameters with the same label (within and across groups) are constrained to be equal. To fix a parameter in one group, but estimate it freely in the other, you may write fix(NaN, 4.3).","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"You can then use the resulting graph to specify an EnsembleParameterTable","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"groups = [:Pasteur, :Grant_White]\n\npartable = EnsembleParameterTable(;\n    graph = graph, \n    observed_vars = observed_vars,\n    latent_vars = latent_vars,\n    groups = groups)","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"The parameter table can be used to create a Dict of RAMMatrices with keys equal to the group names and parameter tables as values:","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"specification = RAMMatrices(partable)","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"That is, you can asses the group-specific RAMMatrices as specification[:group_name].","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"note: A different way to specify\nInstead of choosing the workflow \"Graph -> EnsembleParameterTable -> RAMMatrices\", you may also directly specify RAMMatrices for each group (for an example see this test).","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"The next step is to construct the model:","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"model_g1 = Sem(\n    specification = specification[:Pasteur],\n    data = dat_g1\n)\n\nmodel_g2 = Sem(\n    specification = specification[:Grant_White],\n    data = dat_g2\n)\n\nmodel_ml_multigroup = SemEnsemble(model_g1, model_g2)","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"We now fit the model and inspect the parameter estimates:","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"solution = sem_fit(model_ml_multigroup)\nupdate_estimate!(partable, solution)\nsem_summary(partable)","category":"page"},{"location":"tutorials/collection/multigroup/","page":"Multigroup models","title":"Multigroup models","text":"Other things you can query about your fitted model (fit measures, standard errors, etc.) are described in the section Model inspection and work the same way for multigroup models.","category":"page"},{"location":"#A-fast-and-flexible-SEM-framework","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"StructuralEquationModels.jl is a package for Structural Equation Modeling (SEM) still under active development. It is written for one purpose: Facilitating methodological innovations for SEM. This purpose implies two subgoals for the package: Easy extensibility and speed. You can easily define custom objective functions and other parts of the model. At the same time, it is (very) fast. These properties enable SEM researchers (such as you!) to play around with ideas (extensibility) and run extensive simulations (speed) to evaluate these ideas and users to profit from the resulting innovation.","category":"page"},{"location":"#Get-Started","page":"A fast and flexible SEM framework","title":"Get Started","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"To get started, we recommend the following order:","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"install the package (Installation),\nread A first model, and\nget familiar with Our Concept of a Structural Equation Model.","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"After that, if you are interested in specifying your own loss function (or other parts), you can proceed with Extending the package.","category":"page"},{"location":"#Target-Group","page":"A fast and flexible SEM framework","title":"Target Group","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"You may consider using this package if you need extensibility and/or speed, e.g.","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"you want to extend SEM (e.g. add a new objective function)\nyou want to extend SEM, and your implementation needs to be fast\nyou want to fit the same model(s) to many datasets (bootstrapping, simulation studies)\nyou are planning a study and would like to do power simulations","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"For examples of how to use the package, see the Tutorials.","category":"page"},{"location":"#Batteries-Included","page":"A fast and flexible SEM framework","title":"Batteries Included","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"Models you can fit out of the box include","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"Linear SEM that can be specified in RAM notation\nML, GLS and FIML estimation\nRidge Regularization\nMultigroup SEM\nSums of arbitrary loss functions (everything the optimizer can handle)","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"We provide fast objective functions, gradients, and for some cases hessians as well as approximations thereof. As a user, you can easily define custom loss functions. For those, you can decide to provide analytical gradients or use finite difference approximation / automatic differentiation. You can choose to mix loss functions natively found in this package and those you provide. In such cases, you optimize over a sum of different objectives (e.g. ML + Ridge). This strategy also applies to gradients, where you may supply analytic gradients or opt for automatic differentiation or mixed analytical and automatic differentiation.","category":"page"},{"location":"#Installation","page":"A fast and flexible SEM framework","title":"Installation","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"You must have julia installed (and we strongly recommend using an IDE of your choice; we like VS Code with the Julia extension).","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"To install the latest version of our package, use the following commands:","category":"page"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"julia> ]\npkg> add StructuralEquationModels","category":"page"},{"location":"#Citing-the-package","page":"A fast and flexible SEM framework","title":"Citing the package","text":"","category":"section"},{"location":"","page":"A fast and flexible SEM framework","title":"A fast and flexible SEM framework","text":"To cite our package, go to the GitHub repostory and click on \"Cite this repostiory\" on the right side or see the CSL file.","category":"page"},{"location":"internals/types/#Type-hierarchy","page":"types","title":"Type hierarchy","text":"","category":"section"},{"location":"internals/types/","page":"types","title":"types","text":"The type hierarchy is implemented in \"src/types.jl\".","category":"page"},{"location":"internals/types/","page":"types","title":"types","text":"AbstractSem: the most abstract type in our package","category":"page"},{"location":"internals/types/","page":"types","title":"types","text":"AbstractSemSingle{O, I, L, D} <: AbstractSem is an abstract parametric type that is a supertype of all single models\nSem: models that do not need automatic differentiation or finite difference approximation\nSemFiniteDiff: models whose gradients and/or hessians should be computed via finite difference approximation\nAbstractSemCollection <: AbstractSem is an abstract supertype of all models that contain multiple AbstractSem submodels","category":"page"},{"location":"internals/types/","page":"types","title":"types","text":"Every AbstractSemSingle has to have SemObserved, SemImply, SemLoss and SemOptimizer fields (and can have additional fields).","category":"page"},{"location":"internals/types/","page":"types","title":"types","text":"SemLoss is a container for multiple SemLossFunctions.","category":"page"},{"location":"complementary/maths/","page":"Mathematical appendix","title":"Mathematical appendix","text":"This page is still empty.","category":"page"}]
}
